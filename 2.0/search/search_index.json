{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Rubix ML","text":"<p>A high-level machine learning and deep learning library for the PHP language.</p> <ul> <li>Developer-friendly API is delightful to use</li> <li>40+ supervised and unsupervised learning algorithms</li> <li>Support for ETL, preprocessing, and cross-validation</li> <li>Open Source and free to use commercially</li> </ul>"},{"location":"index.html#what-is-rubix-ml","title":"What is Rubix ML?","text":"<p>Rubix ML is a free open-source machine learning (ML) library that allows you to build programs that learn from your data using the PHP language. We provide tools for the entire machine learning life cycle from ETL to training, cross-validation, and production with over 40 supervised and unsupervised learning algorithms. In addition, we provide tutorials and other educational content to help you get started using ML in your projects.</p>"},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>If you are new to machine learning, we recommend taking a look at the What is Machine Learning? section to get started. If you are already familiar with basic ML concepts, you can browse the basic introduction for a brief look at a typical Rubix ML project. From there, you can browse the official tutorials below which range from beginner to advanced skill level.</p>"},{"location":"index.html#tutorials-example-projects","title":"Tutorials &amp; Example Projects","text":"<p>Check out these example projects using the Rubix ML library. Many come with instructions and a pre-cleaned dataset.</p> <ul> <li>CIFAR-10 Image Recognizer</li> <li>Color Clusterer</li> <li>Credit Default Risk Predictor</li> <li>Customer Churn Predictor</li> <li>Divorce Predictor</li> <li>DNA Taxonomer</li> <li>Dota 2 Game Outcome Predictor</li> <li>Human Activity Recognizer</li> <li>Housing Price Predictor</li> <li>Iris Flower Classifier</li> <li>MNIST Handwritten Digit Recognizer</li> <li>Text Sentiment Analyzer</li> <li>Titanic Survival Predictor</li> </ul>"},{"location":"index.html#interact-with-the-community","title":"Interact With The Community","text":"<ul> <li>Join Our Telegram Channel</li> </ul>"},{"location":"index.html#contributing","title":"Contributing","text":"<p>See CONTRIBUTING.md for guidelines.</p>"},{"location":"index.html#license","title":"License","text":"<p>The code is licensed MIT and the documentation is licensed CC BY-NC 4.0.</p>"},{"location":"basic-introduction.html","title":"Basic Introduction","text":"<p>In this basic introduction to machine learning in Rubix ML, you'll learn how to structure a project, train a learner to predict successful marriages, and then test the model for accuracy. We assume that you already have a basic understanding of the different types of machine learning such as classification and regression. If not, we recommend the section on What is Machine Learning? to start with.</p>"},{"location":"basic-introduction.html#obtaining-data","title":"Obtaining Data","text":"<p>Machine learning (ML) projects typically begin with a question. For example, you might want to answer the question of \"who of my friends are most likely to stay married to their partner?\" One way to go about answering this question with machine learning would be to go out and ask a bunch of happily married and divorced couples the same set of questions about their partner and then use the answers they gave you to build a model to predict successful relationships. In machine learning terms, the answers you collect are the values of the features that constitute measurements of the phenomena being observed - in this case, the response to a question. The number of features in a sample is called the dimensionality of the sample. For example, a sample with 10 features is said to be 10-dimensional.</p> <p>Suppose that you went out and asked 4 couples (2 married and 2 divorced) to respond to 3 features - their partner's communication skills (between 1 and 5), attractiveness (between 1 and 5), and time spent together per week (hours per week). You would structure the data in PHP like in the example below. You'll notice that the samples are represented in a 2-d array (or matrix) and the labels are represented as a 1-d array.</p> <pre><code>$samples = [\n    [3, 4, 50.5],\n    [1, 5, 24.7],\n    [4, 4, 62.0],\n    [3, 2, 31.1],\n];\n\n$labels = ['married', 'divorced', 'married', 'divorced'];\n</code></pre> <p>Note</p> <p>See the Representing your Data section for an in-depth description of how the library treats various forms of data.</p>"},{"location":"basic-introduction.html#the-dataset-object","title":"The Dataset Object","text":"<p>In Rubix ML, data are passed in specialized containers called Dataset objects. Dataset objects handle selecting, subsampling, splitting, randomizing, and sorting of the samples and labels contained within. In general, there are two types of datasets, Labeled and Unlabeled. Labeled datasets are used for supervised learning and for providing the ground-truth during testing. Unlabeled datasets are used for unsupervised learning and for making predictions.</p> <p>You could construct a Labeled dataset from the data we collected earlier by passing the samples and their labels into the constructor like in the example below.</p> <pre><code>use Rubix\\ML\\Datasets\\Labeled;\n\n$dataset = new Labeled($samples, $labels);\n</code></pre> <p>Note</p> <p>See the Extracting Data section to learn more about extracting data from different formats and storage mediums.</p>"},{"location":"basic-introduction.html#choosing-an-estimator","title":"Choosing an Estimator","text":"<p>Estimators make up the core of the Rubix ML library. They provide the <code>predict()</code> API and are responsible for making predictions on unknown samples. Estimators that can be trained with data are called Learners and must be trained before making predictions.</p> <p>In practice, one will experiment with a number of estimators to find the one that works best for their dataset. For our example, we'll focus on an intuitable distance-based supervised learner called K Nearest Neighbors. KNN is a classifier because it takes unknown samples and assigns them a class label. In our example, the output of KNN will either be <code>married</code> or <code>divorced</code> since those are the class labels that we'll train it with.</p> <p>Like most estimators in Rubix, the K Nearest Neighbors classifier requires a set of parameters (called hyper-parameters) to be chosen up-front by the user. These parameters are defined in the class's constructor and control how the learner behaves during training and inference. Hyper-parameters can be selected based on some prior knowledge or completely at random. The defaults provided are a good place to start for most problems.</p> <p>K Nearest Neighbors works by locating the closest training samples to an unknown sample and choosing the class label that is most common. The hyper-parameter k is the number of nearest points from the training set to compare an unknown sample to in order to infer its class label. For example, if the 3 closest neighbors to a given unknown sample have 2 married and 1 divorced label, then the algorithm will output a prediction of married since its the most common. To instantiate the learner, pass a set of hyper-parameters to the class's constructor. For this example, let's set k to 3 and leave the rest of the hyper-parameters as their default.</p> <pre><code>use Rubix\\ML\\Classifiers\\KNearestNeighbors;\n\n$estimator = new KNearestNeighbors(3);\n</code></pre> <p>Note</p> <p>See the Choosing an Estimator section for an in-depth look at the estimators available to you in the library.</p>"},{"location":"basic-introduction.html#training","title":"Training","text":"<p>Training is the process of feeding the learning algorithm data so that it can build an internal representation (or model) of the task its trying to learn. This representation consists of all of the parameters (except hyper-parameters) that are required to make a prediction.</p> <p>To start training, pass the training dataset as a argument to the <code>train()</code> method on the learner instance.</p> <pre><code>$estimator-&gt;train($dataset);\n</code></pre> <p>We can verify that the learner has been trained by calling the <code>trained()</code> method.</p> <pre><code>var_dump($estimator-&gt;trained());\n</code></pre> <pre><code>bool(true)\n</code></pre> <p>For our small training set, the training process should only take a matter of microseconds, but larger datasets with more features can take longer. Now that the learner is trained, in the next section we'll show how we can feed in unknown samples to generate predictions.</p> <p>Note</p> <p>See the Training section of the docs for a closer look at training a learner.</p>"},{"location":"basic-introduction.html#making-predictions","title":"Making Predictions","text":"<p>Suppose that we went out and collected 4 new data points from different friends using the same questions we asked the couples we interviewed for our training set. We could predict whether or not they will stay married to their spouse by taking their answers and passing them in an Unlabeled dataset to the <code>predict()</code> method on our newly trained estimator. This process of making predictions is called inference because the estimator uses the model constructed during training to infer the label of the unknown samples.</p> <p>Note</p> <p>If you attempt to make predictions using an untrained learner, it will throw an exception.</p> <pre><code>use Rubix\\ML\\Datasets\\Unlabeled;\n\n$samples = [\n    [4, 3, 44.2],\n    [2, 2, 16.7],\n    [2, 4, 19.5],\n    [3, 3, 55.0],\n];\n\n$dataset = new Unlabeled($samples);\n\n$predictions = $estimator-&gt;predict($dataset);\n\nprint_r($predictions);\n</code></pre> <pre><code>Array\n(\n    [0] =&gt; married\n    [1] =&gt; divorced\n    [2] =&gt; divorced\n    [3] =&gt; married\n)\n</code></pre> <p>The output of the estimator are the predicted class labels of the unknown samples. We could either trust these predictions as-is or we could proceed to further evaluate the model. In the next section, we'll learn how to test its accuracy using a process called cross validation.</p> <p>Note</p> <p>Check out the section on Inference for more info on making predictions.</p>"},{"location":"basic-introduction.html#model-evaluation","title":"Model Evaluation","text":"<p>Let's imagine we went out and collected enough data from our married and divorced friends to build a  dataset consisting of 50 samples with their corresponding labels. We could use the entire dataset to train the learner or we could set some of the data aside to use for testing. By setting some data aside we are able to test the model on data it has never seen before. This technique is referred to as cross validation and its goal is to test an estimator's ability to generalize its training.</p> <p>For the purposes of the introduction, we'll use a simple Hold Out validator which takes a portion of the dataset for testing and leaves the rest for training. The Hold Out validator requires the user to set the ratio of testing to training samples as a constructor parameter. Let's choose to use a factor of 0.2 (20%) of the dataset for testing leaving the rest (80%) for training.</p> <p>Note</p> <p>20% is a good default choice however your mileage may vary. The important thing to note here is the trade off between more data for training and more data to produce better testing results.</p> <pre><code>use Rubix\\ML\\CrossValidation\\HoldOut;\n\n$validator = new HoldOut(0.2);\n</code></pre> <p>The <code>test()</code> method on the validator requires a compatible validation Metric to be chosen as the scoring function. One classification metric we could use is the Accuracy metric which is defined as the number of true positives over the total number of predictions. For example, if the estimator returned 10 out of 10 correct predictions then its accuracy would be 1.</p> <p>To return a score from the Hold Out validator using the Accuracy metric, pass an estimator instance along with the samples and their ground-truth labels in a dataset object to the validator like in the example below.</p> <pre><code>use Rubix\\ML\\Datasets\\Labeled;\nuse Rubix\\ML\\CrossValidation\\Metrics\\Accuracy;\n\n$dataset = new Labeled($samples, $labels);\n\n$score = $validator-&gt;test($estimator, $dataset, new Accuracy());\n\necho $score;\n</code></pre> <pre><code>0.88\n</code></pre> <p>The return value is the accuracy score which can be interpreted as the degree to which the learner is able to correctly generalize its training to unseen data. According to the example above, our model is 88% accurate. Nice work!</p> <p>Note</p> <p>More info can be found in the Cross Validation section of the docs.</p>"},{"location":"basic-introduction.html#next-steps","title":"Next Steps","text":"<p>Congratulations! You've completed the basic introduction to machine learning in PHP with Rubix ML. For a more in-depth tutorial using the K Nearest Neighbors classifier and a real dataset, check out the Divorce Predictor tutorial and example project. Have fun!</p>"},{"location":"bootstrap-aggregator.html","title":"Bootstrap Aggregator","text":"<p>[source]</p>"},{"location":"bootstrap-aggregator.html#bootstrap-aggregator","title":"Bootstrap Aggregator","text":"<p>Bootstrap Aggregating (or bagging for short) is a model averaging technique designed to improve the stability and performance of a user-specified base estimator by training a number of them on a unique bootstrapped training set sampled at random with replacement. Bagging works especially well with estimators that tend to have high prediction variance by reducing the variance through averaging.</p> <p>Interfaces: Estimator, Learner, Parallel, Persistable</p> <p>Data Type Compatibility: Depends on base learner</p>"},{"location":"bootstrap-aggregator.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 base Learner The base learner. 2 estimators 10 int The number of base learners to train in the ensemble. 3 ratio 0.5 float The ratio of samples from the training set to randomly subsample to train each base learner."},{"location":"bootstrap-aggregator.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\BootstrapAggregator;\nuse Rubix\\ML\\Regressors\\RegressionTree;\n\n$estimator = new BootstrapAggregator(new RegressionTree(10), 300, 0.2);\n</code></pre>"},{"location":"bootstrap-aggregator.html#additional-methods","title":"Additional Methods","text":"<p>This meta estimator does not have any additional methods.</p>"},{"location":"bootstrap-aggregator.html#references","title":"References","text":"<ol> <li> <p>L. Breiman. (1996). Bagging Predictors.\u00a0\u21a9</p> </li> </ol>"},{"location":"choosing-an-estimator.html","title":"Choosing an Estimator","text":"<p>Estimators make up the core of the Rubix ML library and include classifiers, regressors, clusterers, anomaly detectors, and meta-estimators organized into their own namespaces. They are responsible for making predictions and are usually trained with data. Most estimators allow tuning by adjusting their user-defined hyper-parameters. Hyper-parameters are arguments to the learning algorithm that effect its behavior during training and inference. The values for the hyper-parameters can be chosen by intuition, tuning, or completely at random. The defaults provided by the library are a good place to start for most problems. To instantiate a new estimator, pass the desired values of the hyper-parameters to the estimator's constructor like in the example below.</p> <pre><code>use Rubix\\ML\\Classifiers\\KNearestNeighbors;\nuse Rubix\\ML\\Kernels\\Distance\\Minkowski;\n\n$estimator = new KNearestNeighbors(10, false, new Minkowski(2.5));\n</code></pre>"},{"location":"choosing-an-estimator.html#classifiers","title":"Classifiers","text":"<p>Classifiers are supervised learners that predict a categorical class label. They can be used to recognize (<code>cat</code>, <code>dog</code>, <code>turtle</code>), differentiate (<code>spam</code>, <code>not spam</code>), or describe (<code>running</code>, <code>walking</code>) the samples in a dataset based on the labels they were trained on. In addition, classifiers that implement the Probabilistic interface can infer the joint probability distribution of each possible class given an unclassified sample.</p> Name Flexibility Proba Online Ranks Features Verbose Data Compatibility AdaBoost High \u25cf \u25cf Depends on base learner Classification Tree Medium \u25cf \u25cf Categorical, Continuous Extra Tree Classifier Medium \u25cf \u25cf Categorical, Continuous Gaussian Naive Bayes Medium \u25cf \u25cf Continuous K-d Neighbors Medium \u25cf Depends on distance kernel K Nearest Neighbors Medium \u25cf \u25cf Depends on distance kernel Logistic Regression Low \u25cf \u25cf \u25cf \u25cf Continuous Logit Boost High \u25cf \u25cf \u25cf Categorical, Continuous Multilayer Perceptron High \u25cf \u25cf \u25cf Continuous Naive Bayes Medium \u25cf \u25cf Categorical One Vs Rest Medium \u25cf Depends on base learner Radius Neighbors Medium \u25cf Depends on distance kernel Random Forest High \u25cf \u25cf Categorical, Continuous Softmax Classifier Low \u25cf \u25cf \u25cf Continuous SVC High Continuous"},{"location":"choosing-an-estimator.html#regressors","title":"Regressors","text":"<p>Regressors are a type of supervised learner that predict a continuous-valued outcome such as <code>1.275</code> or <code>655</code>. They can be used to quantify a sample such as its credit score, age, or steering wheel position in units of degrees. Unlike classifiers whose range of predictions is bounded by the number of possible classes in the training set, a regressor's range is unbounded - meaning, the number of possible values a regressor could predict is infinite.</p> Name Flexibility Online Ranks Features Verbose Persistable Data Compatibility Adaline Low \u25cf \u25cf \u25cf \u25cf Continuous Extra Tree Regressor Medium \u25cf \u25cf Categorical, Continuous Gradient Boost High \u25cf \u25cf \u25cf Categorical, Continuous K-d Neighbors Regressor Medium \u25cf Depends on distance kernel KNN Regressor Medium \u25cf \u25cf Depends on distance kernel MLP Regressor High \u25cf \u25cf \u25cf Continuous Radius Neighbors Regressor Medium \u25cf Depends on distance kernerl Regression Tree Medium \u25cf \u25cf Categorical, Continuous Ridge Low \u25cf \u25cf Continuous SVR High Continuous"},{"location":"choosing-an-estimator.html#clusterers","title":"Clusterers","text":"<p>Clusterers are unsupervised learners that predict an integer-valued cluster number such as <code>0</code>, <code>1</code>, <code>...</code>, <code>n</code>. They are similar to classifiers, however since they lack a supervised training signal, they cannot be used to recognize or describe samples. Instead, clusterers differentiate and group samples using only the information found within the structure of the samples without their labels.</p> Name Flexibility Proba Online Verbose Persistable Data Compatibility DBSCAN High Depends on distance kernel Fuzzy C Means Low \u25cf \u25cf \u25cf Continuous Gaussian Mixture Medium \u25cf \u25cf \u25cf Continuous K Means Low \u25cf \u25cf \u25cf \u25cf Continuous Mean Shift Medium \u25cf \u25cf \u25cf Continuous"},{"location":"choosing-an-estimator.html#anomaly-detectors","title":"Anomaly Detectors","text":"<p>Anomaly Detectors are unsupervised learners that predict whether a sample should be classified as an anomaly or not. We use the value <code>1</code> to indicate an outlier and <code>0</code> for a regular sample and the predictions can be cast to their boolean equivalent if needed. Anomaly detectors that implement the Scoring interface can output an anomaly score that can be used to sort the samples by their degree of anomalousness.</p> Name Scope Scoring Online Verbose Persistable Data Compatibility Gaussian MLE Global \u25cf \u25cf \u25cf Continuous Isolation Forest Local \u25cf \u25cf Categorical, Continuous Local Outlier Factor Local \u25cf \u25cf Depends on distance kernel Loda Local \u25cf \u25cf \u25cf Continuous One Class SVM Global \u25cf Continuous Robust Z-Score Global \u25cf \u25cf Continuous"},{"location":"choosing-an-estimator.html#model-flexibility-tradeoff","title":"Model Flexibility Tradeoff","text":"<p>A characteristic of most estimator types is the notion of flexibility. Flexibility can be expressed in different ways but greater flexibility usually comes with the capacity to handle more complex tasks. The tradeoff for flexibility is increased computational complexity, reduced model interpretability, and greater susceptibility to overfitting. In contrast, low flexibility models tend to be easier to interpret and quicker to train but are more prone to underfitting. In general, we recommend choosing the simplest model that does not underfit the training data for your project.</p>"},{"location":"choosing-an-estimator.html#no-free-lunch-theorem","title":"No Free Lunch Theorem","text":"<p>At some point you may ask yourself \"Why do we need so many different learning algorithms?\" The answer to that question can be understood by the No Free Lunch Theorem which states that, when averaged over the space of all possible problems, no algorithm performs any better than the next. Perhaps a more useful way of stating NFL is that certain learners perform better at certain tasks and worse in others. This is explained by the fact that all learning algorithms have some prior knowledge inherent in them whether it be via the choice of hyper-parameters or the design of the algorithm itself. Another consequence of No Free Lunch is that there exists no single estimator that performs better for all problems.</p>"},{"location":"committee-machine.html","title":"Committee Machine","text":"<p>[source]</p>"},{"location":"committee-machine.html#committee-machine","title":"Committee Machine","text":"<p>A voting ensemble that aggregates the predictions of a committee of heterogeneous learners (referred to as experts). The committee employs a user-specified influence scheme to weight the final predictions.</p> <p>Note</p> <p>Influence values can be on any arbitrary scale as they are automatically normalized upon instantiation.</p> <p>Interfaces: Estimator, Learner, Parallel, Persistable</p> <p>Data Type Compatibility: Depends on the base learners</p>"},{"location":"committee-machine.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 experts array An array of learner instances that will comprise the committee. 2 influences null array The influence values for each expert in the committee. If null, each expert will be weighted equally."},{"location":"committee-machine.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CommitteeMachine;\nuse Rubix\\ML\\Classifiers\\GaussianNB;\nuse Rubix\\ML\\Classifiers\\RandomForest;\nuse Rubix\\ML\\Classifiers\\ClassificationTree;\nuse Rubix\\ML\\Classifiers\\KDNeighbors;\nuse Rubix\\ML\\Classifiers\\SoftmaxClassifier;\n\n$estimator = new CommitteeMachine([\n    new GaussianNB(),\n    new RandomForest(new ClassificationTree(4), 100, 0.3),\n    new KDNeighbors(3),\n    new SoftmaxClassifier(100),\n], [\n    0.2, 0.4, 0.3, 0.1,\n]);\n</code></pre>"},{"location":"committee-machine.html#additional-methods","title":"Additional Methods","text":"<p>Return the learner instances of the committee: <pre><code>public experts() : array\n</code></pre></p> <p>Return the normalized influence scores of each expert in the committee: <pre><code>public influences() : array\n</code></pre></p>"},{"location":"committee-machine.html#references","title":"References","text":"<ol> <li> <p>H. Drucker. (1997). Fast Committee Machines for Regression and Classification.\u00a0\u21a9</p> </li> </ol>"},{"location":"cross-validation.html","title":"Cross Validation","text":"<p>Cross Validation (CV) is a technique for assessing the generalization performance of a model using data it has never seen before. The validation score gives us a sense for how well the model will perform in the real world. In addition, it allows the user to identify problems such as underfitting, overfitting, and selection bias which are discussed in the last section.</p>"},{"location":"cross-validation.html#creating-a-testing-set","title":"Creating a Testing Set","text":"<p>For some projects we'll create a dedicated testing set, but in others we can separate some of the samples from our master dataset to be used for testing on the fly. To ensure that both the training and testing sets contain samples that accurately represent the master set we have a number of methods on the Dataset object we can employ.</p>"},{"location":"cross-validation.html#randomized-split","title":"Randomized Split","text":"<p>The first method of creating a training and testing set that works for all datasets is to randomize and then split the dataset into two subsets of varying proportions. In the example below we'll create a training set with 80% of the samples and a testing set with the remaining 20% using the <code>randomize()</code> and <code>split()</code> methods on the Dataset object.</p> <pre><code>[$training, $testing] = $dataset-&gt;randomize()-&gt;split(0.8);\n</code></pre> <p>You can also use the <code>take()</code> method to extract a testing set while leaving the remaining samples in the training set.</p> <pre><code>$testing = $training-&gt;randomize()-&gt;take(1000);\n</code></pre>"},{"location":"cross-validation.html#stratified-split","title":"Stratified Split","text":"<p>If we have a Labeled dataset containing class labels, we can split the dataset in such a way that samples belonging to each class are represented fairly in both sets. This stratified method helps to reduce selection bias by ensuring that each subset remains balanced.</p> <pre><code>[$training, $testing] = $dataset-&gt;stratifiedSplit(0.8);\n</code></pre>"},{"location":"cross-validation.html#metrics","title":"Metrics","text":"<p>Cross validation Metrics are used to score the predictions made by an Estimator with respect to their known ground-truth labels. There are different metrics for different types of problems. To return a validation score from a Metric pass the predictions and labels to the <code>score()</code> method like in the example below.</p> <pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\Accuracy;\n\n$predictions = $estimator-&gt;predict($testing);\n\n$metric = new Accuracy();\n\n$score = $metric-&gt;score($predictions, $testing-&gt;labels());\n\necho $score;\n</code></pre> <pre><code>0.85\n</code></pre> <p>Note</p> <p>All metrics follow the schema that higher scores are better - thus, common loss functions such as Mean Squared Error and RMSE are given as their negative to conform to this schema.</p>"},{"location":"cross-validation.html#classification-and-anomaly-detection","title":"Classification and Anomaly Detection","text":"<p>Metrics for classification and anomaly detection (a special case of binary classification) compare class predictions to other categorical labels. Their scores are calculated from the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) counts derived from the confusion matrix between the set of predictions and their ground-truth labels.</p> Name Range Formula Notes Accuracy [0, 1] \\(\\frac{TP}{TP + FP}\\) Not suited for imbalanced datasets F Beta [0, 1] \\((1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}}\\) Informedness [-1, 1] \\({\\frac {\\text{TP}}{{\\text{TP}}+{\\text{FN}}}}+{\\frac {\\text{TP}}{{\\text{TN}}+{\\text{FP}}}}-1\\) MCC [-1, 1] \\({\\frac {\\mathrm {TP} \\times \\mathrm {TN} -\\mathrm {FP} \\times \\mathrm {FN} }{\\sqrt {(\\mathrm {TP} +\\mathrm {FP} )(\\mathrm {TP} +\\mathrm {FN} )(\\mathrm {TN} +\\mathrm {FP} )(\\mathrm {TN} +\\mathrm {FN} )}}}\\)"},{"location":"cross-validation.html#regression","title":"Regression","text":"<p>Regression metrics output a score based on the error achieved by comparing continuous-valued predictions and their ground-truth labels.</p> Name Range Formula Notes Mean Absolute Error [-\u221e, 0] \\({\\frac {1}{n}}{\\sum _{i=1}^{n}\\left | Y_{i}-\\hat {Y_{i}}\\right | }\\) Output in same units as predictions Mean Squared Error [-\u221e, 0] \\({\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}\\) Sensitive to outliers Median Absolute Error [-\u221e, 0] \\({\\operatorname {median} (| Y_{i}-{\\tilde {Y}} |)}\\) Robust to outliers R Squared [-\u221e, 1] \\(1-{SS_{\\rm {res}} \\over SS_{\\rm {tot}}}\\) RMSE [-\u221e, 0] \\({\\sqrt{ \\frac {1}{n} \\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}}}\\) Output in same units as predictions SMAPE [-100, 0] \\({\\frac {100\\%}{n}}\\sum _{t=1}^{n}{\\frac {\\left|F_{t}-A_{t}\\right|}{(|A_{t}|+|F_{t}|)/2}}\\)"},{"location":"cross-validation.html#clustering","title":"Clustering","text":"<p>Clustering metrics derive their scores from a contingency table which can be thought of as a confusion matrix where the class names of the predictions are unknown.</p> Name Range Formula Notes Completeness [0, 1] \\(1-\\frac{H(K, C)}{H(K)}\\) Not suited for hyper-parameter tuning Homogeneity [0, 1] \\(1-\\frac{H(C, K)}{H(C)}\\) Not suited for hyper-parameter tuning Rand Index [-1, 1] \\({\\frac {\\left.\\sum _{ij}{\\binom {n_{ij}}{2}}-\\left[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}\\right]\\right/{\\binom {n}{2}}}{\\left.{\\frac {1}{2}}\\left[\\sum _{i}{\\binom {a_{i}}{2}}+\\sum _{j}{\\binom {b_{j}}{2}}\\right]-\\left[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}\\right]\\right/{\\binom {n}{2}}}}\\) V Measure [0, 1] \\(\\frac{(1+\\beta)hc}{\\beta h + c}\\)"},{"location":"cross-validation.html#probabilistic-metrics","title":"Probabilistic Metrics","text":"<p>These metrics calculate validation scores from the estimated probabilities of a Probabilistic classifier instead of their class predictions.</p> <pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\TopKAccuracy;\n\n$probabilities = $estimator-&gt;proba($testing);\n\n$metric = new TopKAccuracy(5);\n\n$score = $metric-&gt;score($probabilities, $testing-&gt;labels());\n\necho $score;\n</code></pre> <pre><code>0.95\n</code></pre> Name Range Formula Notes Brier Score [-2, 0] \\(\\frac{1}{n}\\sum\\limits _{i=1}^{n}\\sum\\limits _{j=1}^{c}(P_{ij}-{\\hat {P_{ij}}})^2\\) Probabilistic Accuracy [0, 1] \\(\\frac{1}{n}\\sum\\limits _{i=1}^{n} P_{label}\\) Top K Accuracy [0, 1]"},{"location":"cross-validation.html#reports","title":"Reports","text":"<p>Cross validation reports give you a deeper sense for how well a particular model performs with fine-grained information. The <code>generate()</code> method on the Report Generator interface takes a set of predictions and their corresponding ground-truth labels and returns a Report object filled with useful statistics that can be printed directly to the terminal or saved to a file.</p> Report Usage Confusion Matrix Classification or Anomaly Detection Contingency Table Clustering Error Analysis Regression Multiclass Breakdown Classification or Anomaly Detection"},{"location":"cross-validation.html#generating-a-report","title":"Generating a Report","text":"<p>To generate the report, pass the predictions made by an estimator and their ground-truth labels to the <code>generate()</code> method on the report generator instance.</p> <pre><code>use Rubix\\ML\\CrossValidation\\Reports\\ErrorAnalysis;\n\n$report = new ErrorAnalysis();\n\n$results = $report-&gt;generate($predictions, $labels);\n</code></pre>"},{"location":"cross-validation.html#printing-a-report","title":"Printing a Report","text":"<p>The results of the report are returned in a Report object. Report objects implement the Stringable interface which means they can be cast to strings to output the human-readable form of the report.</p> <pre><code>echo $results;\n</code></pre> <pre><code>{\n    \"mean absolute error\": 0.8,\n    \"median absolute error\": 1,\n    \"mean squared error\": 1,\n    \"mean absolute percentage error\": 14.02077497665733,\n    \"rms error\": 1,\n    \"mean squared log error\": 0.019107097505647368,\n    \"r squared\": 0.9958930551562692,\n    \"error mean\": -0.2,\n    \"error standard deviation\": 0.9898464007663,\n    \"error skewness\": -0.22963966338592326,\n    \"error kurtosis\": -1.0520833333333324,\n    \"error min\": -2,\n    \"error 25%\": -1.0,\n    \"error median\": 0.0,\n    \"error 75%\": 0.75,\n    \"error max\": 1,\n    \"cardinality\": 10\n}\n</code></pre>"},{"location":"cross-validation.html#accessing-report-attributes","title":"Accessing Report Attributes","text":"<p>You can access individual report attributes by treating the report object as an associative array.</p> <pre><code>$mae = $results['mean absolute error'];\n</code></pre>"},{"location":"cross-validation.html#saving-a-report","title":"Saving a Report","text":"<p>Report objects can be cast to JSON encodings which are persistable using a Persister object. To save a report, call the <code>toJSON()</code> method on the report to return an encoding object and then pass a persister to its <code>saveTo()</code> method like in the example below.</p> <pre><code>use Rubix\\ML\\Persisters\\Filesystem;\n\n$results-&gt;toJSON()-&gt;saveTo(new Filesystem('error.report'));\n</code></pre>"},{"location":"cross-validation.html#validators","title":"Validators","text":"<p>Metrics can be used stand-alone or they can be used within a Validator object as the scoring function. Validators automate the cross validation process by training and testing a learner on different subsets of a master dataset. The way in which subsets are chosen depends on the algorithm employed under the hood. Most validators implement the Parallel interface which allows multiple tests to be run at the same time in parallel.</p> Validator Test Coverage Parallel Hold Out Partial K Fold Full \u25cf Leave P Out Full \u25cf Monte Carlo Asymptotically Full \u25cf <p>For example, the K Fold validator automatically selects one of k subsets referred to as a fold as a validation set and then uses the rest of the folds to train the learner. It does this until the learner is trained and tested on every sample in the dataset at least once. The final score is then an average of the k validation scores returned by each test. To begin, pass an untrained Learner, a Labeled dataset, and your chosen validation metric to the validator's <code>test()</code> method.</p> <pre><code>use Rubix\\ML\\CrossValidation\\KFold;\nuse Rubix\\ML\\CrossValidation\\Metrics\\FBeta;\n\n$validator = new KFold(5);\n\n$score = $validator-&gt;test($estimator, $dataset, new FBeta());\n\necho $score;\n</code></pre> <pre><code>0.9175\n</code></pre>"},{"location":"cross-validation.html#common-problems","title":"Common Problems","text":"<p>Poor generalization performance can be explained by one or more of these common problems.</p>"},{"location":"cross-validation.html#underfitting","title":"Underfitting","text":"<p>A poorly performing model can sometimes be explained as underfitting the training data - a condition in which the learner is unable to capture the underlying pattern or trend given the model constraints. The result of an underfit model is an estimator with high bias error. Underfitting usually occurs when a simple model is chosen to represent data that is complex and non-linear. Adding more features to the dataset can help, however if the problem is too severe, a more flexible learning algorithm can be chosen for the task instead.</p>"},{"location":"cross-validation.html#overfitting","title":"Overfitting","text":"<p>When a model performs well on training data but poorly during cross-validation it could be that the model has overfit the training data. Overfitting occurs when the model conforms too closely to the training data and therefore fails to generalize well to new data or make predictions reliably. Flexible models are more prone to overfitting due to their ability to memorize individual samples. Most learners employ strategies such as regularization, early stopping, and/or tree pruning to control overfitting, however if overfitting is still a problem, adding more unique samples to the dataset can also help.</p>"},{"location":"cross-validation.html#selection-bias","title":"Selection Bias","text":"<p>When a model performs well on certain samples but poorly on others it could be that the learner was trained with a dataset that exhibits selection bias. Selection bias is the bias introduced when a population is disproportionally represented in a dataset. For example, if a learner is trained to classify pictures of cats and dogs but mostly (say 90%) cats are represented in the dataset, the model will likely have difficulty making real-world predictions where cats and dogs are more equally represented. To correct selection bias, either obtain more unique training samples or up-sample the class of the underrepresented type.</p>"},{"location":"estimator.html","title":"Estimator","text":"<p>The Estimator interface is implemented by all learners in Rubix ML. It provides basic inference functionality through the <code>predict()</code> method which returns a set of predictions from a dataset. Additionally, it provides methods for returning estimator type and data type compatibility declarations.</p>"},{"location":"estimator.html#make-predictions","title":"Make Predictions","text":"<p>Return the predictions from a dataset containing unknown samples in an array: <pre><code>public predict(Dataset $dataset) : array\n</code></pre></p> <pre><code>$predictions = $estimator-&gt;predict($dataset);\n\nprint_r($predictions);\n</code></pre> <pre><code>Array\n(\n    [0] =&gt; married\n    [1] =&gt; divorced\n    [2] =&gt; divorced\n    [3] =&gt; married\n)\n</code></pre> <p>Note</p> <p>The return value of <code>predict()</code> is an array containing the predictions in the same order that they were indexed in the dataset.</p>"},{"location":"exploring-data.html","title":"Exploring Data","text":"<p>Exploratory Data Analysis (EDA) is an approach to modeling data that produces insights into the characteristics of a dataset. EDA is useful for feature engineering as well as model selection and can save time and lead to better modes when included in your machine learning lifecycle. In general, there are two types of Exploratory Data Analysis - quantitative and graphical. Quantitative data analysis summarizes the data using statistical or probabilistic methods. Graphical analysis uses techniques such as scatterplots and histograms to glean information from the structure and shape of the data and can incorporate Manifold Learning to reduce the dimensionality of the samples.</p>"},{"location":"exploring-data.html#describe-a-dataset","title":"Describe a Dataset","text":"<p>The Dataset API has a handy method named <code>describe()</code> that computes statistics for each continuous feature of the dataset such as the column median, standard deviation, and skewness. In addition, it provides the probabilities of each category for categorical feature columns. In the example below, we'll echo the Report object returned by the <code>describe()</code> method to get a better understanding for how the values of our features are distributed.</p> <pre><code>$report = $dataset-&gt;describe();\n\necho $report;\n</code></pre> <pre><code>[\n    {\n        \"offset\": 0,\n        \"type\": \"categorical\",\n        \"num categories\": 2,\n        \"probabilities\": {\n            \"friendly\": 0.6666666666666666,\n            \"loner\": 0.3333333333333333\n        }\n    },\n    {\n        \"offset\": 1,\n        \"type\": \"continuous\",\n        \"mean\": 0.3333333333333333,\n        \"standard deviation\": 3.129252661934191,\n        \"skewness\": -0.4481030843690633,\n        \"kurtosis\": -1.1330702741786107,\n        \"range\": 9.0,\n        \"min\": -5,\n        \"25%\": -1.375,\n        \"median\": 0.8,\n        \"75%\": 2.825,\n        \"max\": 4\n    }\n]\n</code></pre> <p>We can also save the report by passing a Persister to the <code>saveTo()</code> method on the Encoding object returned by calling <code>toJSON()</code> on the Report object.</p> <pre><code>use Rubix\\ML\\Persisters\\Filesystem;\n\n$report-&gt;toJSON()-&gt;saveTo(new Filesystem('report.json'));\n</code></pre>"},{"location":"exploring-data.html#describe-by-label","title":"Describe by Label","text":"<p>You can also describe the dataset in terms of the classes each sample belongs to by calling the <code>describeByLabel()</code> method on a Labeled dataset object with categorical labels.</p> <pre><code>$report = $dataset-&gt;describeByLabel();\n</code></pre>"},{"location":"exploring-data.html#visualization","title":"Visualization","text":"<p>Another technique used in data analysis is plotting one or more of its dimensions in a chart such as a scatterplot or histogram. Visualizing the data gives us an understanding as to the shape of the data and can aid in discovering outliers or for choosing features to train our model with. Since the library works with common data formats, you are free to use your favorite 3rd party plotting software to visualize the data copied from Rubix ML. If you are looking for a place to start, the free Plotly online Chart Studio or a modern spreadsheet application should work well for most visualization tasks.</p>"},{"location":"exploring-data.html#exporting-data","title":"Exporting Data","text":"<p>Before importing a dataset into your plotting software, you may need to export it in a format that can be recognized. For this, the library provides the Writable Extractor API to handle exporting dataset objects to various formats including CSV and NDJSON. For example, to export a dataset in CSV format pass the CSV extractor to the <code>exportTo()</code> method on the dataset object.</p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$dataset-&gt;exportTo(new CSV('dataset.csv'));\n</code></pre>"},{"location":"exploring-data.html#converting-formats","title":"Converting Formats","text":"<p>You may want to convert a dataset stored in one format to another format. To convert formats, pass an extractor object to the <code>export()</code> method on a target extractor that implements the Writable interface. In the example below, we'll convert a data table from CSV format to NDJSON, saving it to a new file.</p> <pre><code>use Rubix\\ML\\Extractors\\NDJSON;\nuse Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new NDJSON('dataset.ndjson');\n\n$extractor-&gt;export(new CSV('dataset.csv'));\n</code></pre>"},{"location":"exploring-data.html#1d-histogram","title":"1D Histogram","text":"<p>One way to visualize the categorical features of a dataset is to put each sample into a bin corresponding to the particular category it belongs to. We can then count the number of samples and display them in a histogram so they can be visually compared. In the following example, we'll bin the samples of the Housing dataset according to building type.</p> <p></p>"},{"location":"exploring-data.html#2d-scatterplot","title":"2D Scatterplot","text":"<p>A common way to visualize the continuous features of a dataset is to plot two features as X and Y axis of a scatterplot. In the example below, we'll plot the <code>petal width</code> and <code>petal length</code> features of the Iris dataset. Notice that we can distinguish 3 clusters corresponding to each class label - therefore, these features will do a pretty good job of informing the learner at training time.</p> <p></p>"},{"location":"exploring-data.html#manifold-learning","title":"Manifold Learning","text":"<p>Manifold Learning is a type of dimensionality reduction that aims to produce a faithful low-dimensional (1 - 3) representation of a whole dataset for visualization. Unlike the example above in which we isolated a fixed number of features, Manifold Learning allows us to plot a representation of all the features. This representation is referred to as an embedding because the high-dimensional features are embedded into a lower-dimensional manifold.</p> <p>In the first example, we'll use a dimensionality reduction method called Truncated SVD to project the Iris dataset down into 2 dimensions and then export the data to a CSV file using the <code>exportTo()</code> method so we can import it into our plotting software.</p> <pre><code>use Rubix\\ML\\Transformers\\TruncatedSVD;\nuse Rubix\\ML\\Extractors\\CSV;\n\n$dataset-&gt;apply(new TruncatedSVD(2))\n    -&gt;exportTo(new CSV('embedding.csv'));\n</code></pre> <p>When we visualize the embedding, again we see the formation of clusters, however, notice that the X and Y axis no longer correspond to individual features but rather to arbitrary axis of the 2-dimensional embedding of all the features.</p> <p></p> <p>Another algorithm often used for manifold learning is T-distributed Stochastic Neighbor Embedding or t-SNE. Unlike Truncated SVD which is a linear dimensionality reducer, t-SNE is able to find non-linear manifolds of the dataset and therefore can sometimes produce more faithful representations of the data in low dimensions. In the example below, we'll use the t-SNE transformer to embed the 4-dimensional Iris dataset into 2 dimensions so we can visualize it.</p> <pre><code>use Rubix\\ML\\Transformers\\TSNE;\nuse Rubix\\ML\\Extractors\\CSV;\n\n$dataset-&gt;apply(new TSNE(2, 100.0, 10.0))\n    -&gt;exportTo(new CSV('embedding.csv'));\n</code></pre> <p>Here is what a t-SNE embedding looks like when it is plotted. Notice that although the clusters are sparser and more gaussian-like, the structure and distances between samples is roughly preserved.</p> <p></p>"},{"location":"extracting-data.html","title":"Extracting Data","text":"<p>There are a number of ways to instantiate a new Dataset object, but all of them require the data to be loaded into memory first. Some common formats you'll find data in are structured plain-text such as CSV or NDJSON, or in a queryable database such as MySQL or MongoDB. No matter how your data are stored, you have the freedom and flexibility to implement the data source to fit your current infrastructure. To help make extraction simple for more common use cases, the library provides a number of Extractor objects. Extractors are iterators that let you loop over the records of a dataset in storage and can be used to instantiate a dataset object using the <code>fromIterator()</code> method.</p>"},{"location":"extracting-data.html#csv","title":"CSV","text":"<p>A common plain-text format for small to medium-sized datasets is comma-separated values or CSV for short. A CSV file contains a table with individual samples indicated by rows and the values of the features stored in each column. Columns are separated by a delimiter such as the <code>,</code> or <code>;</code> character and may be enclosed on both ends with an optional enclosure such as <code>\"</code>. The file can sometimes contain a header as the first row. CSV files have the advantage of being able to be processed line by line, however, their disadvantage is that type information cannot be inferred from the format. Thus, all CSV data are imported as categorical type (strings) by default.</p> <p>Example</p> <pre><code>attitude,texture,sociability,rating,class\nnice,furry,friendly,4,not monster\nmean,furry,loner,-1.5,monster\n</code></pre> <p>The library provides the CSV Extractor to help import data from the CSV format. We'll use it in conjunction with the <code>fromIterator()</code> method to instantiate a new dataset object. In the example below, In addition, we'll apply the Numeric String Converter to the newly instantiated dataset object to convert the numeric data to the proper format immediately after instantiation.</p> <pre><code>use Rubix\\ML\\Datasets\\Labeled;\nuse Rubix\\ML\\Extractors\\CSV;\nuse Rubix\\ML\\Transformers\\NumericStringConverter;\n\n$dataset = Labeled::fromIterator(new CSV('example.csv', true))\n    -&gt;apply(new NumericStringConverter());\n</code></pre> <p>We can check the number of records that were imported by calling the <code>numSamples()</code> method on the dataset object.</p> <pre><code>echo $dataset-&gt;numSamples();\n</code></pre> <pre><code>5000\n</code></pre>"},{"location":"extracting-data.html#ndjson","title":"NDJSON","text":"<p>Another plain-text format called NDJSON or Newline Delimited Javascript Object Notation (JSON) can be considered a hybrid of both CSV and JSON. It contains rows of JSON arrays or objects delineated by a newline character (<code>\\n</code> or <code>\\r\\n</code>). It has the advantage of retaining type information like JSON and can also be read into memory efficiently like CSV.</p> <p>Example</p> <pre><code>{\"attitude\":\"nice\",\"texture\":\"furry\",\"sociability\":\"friendly\",\"rating\":4,\"class\":\"not monster\"}\n{\"attitude\":\"mean\",\"texture\":\"furry\",\"sociability\":\"loner\",\"rating\":-1.5,\"class\":\"monster\"}\n{\"attitude\":\"nice\",\"texture\":\"rough\",\"sociability\":\"friendly\",\"rating\":2.6,\"class\":\"not monster\"}\n</code></pre> <p>The NDJSON extractor can be used to instantiate a new dataset object from a NDJSON file. Optionally, it can be combined with the standard PHP library's Limit Iterator to only load a portion of the data into memory. In the example below, we load the first 1,000 rows of data from an NDJSON file into an Unlabeled dataset.</p> <pre><code>use Rubix\\ML\\Extractors\\NDJSON;\nuse Rubix\\ML\\Datasets\\Unlabeled;\nuse LimitIterator;\n\n$extractor = new NDJSON('example.ndjson');\n\n$iterator = new LimitIterator($extractor-&gt;getIterator(), 0, 1000);\n\n$dataset = Unlabeled::fromIterator($iterator);\n</code></pre>"},{"location":"extracting-data.html#sql","title":"SQL","text":"<p>Medium to large datasets will often be stored in an RDBMS (relational database management system) such as MySQL, PostgreSQLor Sqlite. Relational databases allow you to query large amounts of data on-the-fly and can be very flexible. PHP comes with robust relational database support through its PDO interface. To iterate over the rows of an SQL table we provide an SQL Table extractor uses the PDO interface under the hood. In the example below we'll wrap our SQL Table extractor in a Column Picker to instantiate a new Unlabeled dataset object from a particular set of columns of the table.</p> <pre><code>use Rubix\\ML\\Extractors\\SQLTable;\nuse Rubix\\ML\\Extractors\\ColumnPicker;\nuse Rubix\\ML\\Datasets\\Unlabeled;\nuse PDO;\n\n$connection = new PDO('sqlite:/example.sqlite');\n\n$extractor = new ColumnPicker(new SQLTable($connection, 'patients'), [\n    'age', 'gender', 'height', 'diagnosis',\n]);\n\n$dataset = Unlabeled::fromIterator($extractor);\n</code></pre> <p>If you need more control over your data pipeline then we recommend writing your own custom queries. The following example uses the PDO interface to execute a user-defined SQL query and instantiate a dataset object containing the same data as the example above. However, this method may be more efficient because it avoids querying for more data than you need.</p> <pre><code>use Rubix\\ML\\Datasets\\Unlabeled;\nuse PDO;\n\n$pdo = new PDO('sqlite:/example.sqlite');\n\n$query = $pdo-&gt;prepare('SELECT age, gender, height, diagnosis FROM patients');\n\n$query-&gt;execute();\n\n$samples = $query-&gt;fetchAll(PDO::FETCH_NUM);\n\n$dataset = new Unlabeled($samples);\n</code></pre>"},{"location":"extracting-data.html#images","title":"Images","text":"<p>PHP offers a number of functions to import images as PHP resources such as <code>imagecreatefromjpeg()</code> and <code>imagecreatefrompng()</code> that come with the GD extension. The example below imports the .png images in the <code>train</code> folder and labels them using part of their filename. The samples and labels are then put into a Labeled dataset using the <code>build()</code> factory method and then converted into raw color channel data by applying the Image Vectorizer.</p> <pre><code>use Rubix\\ML\\Datasets\\Labeled;\nuse Rubix\\ML\\Transformers\\ImageVectorizer;\n\n$samples = $labels = [];\n\nforeach (glob('train/*.png') as $file) {\n    $samples[] = [imagecreatefrompng($file)];\n    $labels[] = preg_replace('/[0-9]+_(.*).png/', '$1', basename($file));\n}\n\n$dataset = Labeled::build($samples, $labels)\n    -&gt;apply(new ImageVectorizer());\n</code></pre>"},{"location":"extracting-data.html#synthetic-datasets","title":"Synthetic Datasets","text":"<p>Synthetic datasets are those that can be generated by one or more predefined formulas. In Rubix ML, we can generate synthetic datasets using Generator objects. Generators are useful in educational settings and for supplementing a small dataset with more samples. To generate a labeled dataset using the Half Moon generator pass the number of records you wish to generate to the <code>generate()</code> method.</p> <pre><code>use Rubix\\ML\\Datasets\\Generators\\HalfMoon;\n\n$generator = new HalfMoon();\n\n$dataset = $generator-&gt;generate(1000);\n</code></pre> <p>Now we can write the dataset to a CSV file and import it into our favorite plotting software.</p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$dataset-&gt;exportTo(new CSV('half-moon.csv'));\n</code></pre> <p></p>"},{"location":"faq.html","title":"FAQ","text":"<p>Here you will find answers to the most frequently asked questions.</p>"},{"location":"faq.html#is-machine-learning-the-same-thing-as-ai","title":"Is Machine Learning the same thing as AI?","text":"<p>Machine Learning is a subset of Artificial Intelligence (AI) that focuses on using data to train a computer to perform tasks. While machine learning (ML) has contributed substantially to the field of AI, other non-learning techniques such as rule-based or symbolic systems are also forms of artificial intelligence.</p>"},{"location":"faq.html#what-types-of-problems-is-ml-good-for","title":"What types of problems is ML good for?","text":"<p>Machine Learning is a good fit for problems in which it would be infeasible for software developers and domain experts to design a system that could encode all the necessary rulesets to obtain accurate predictions. In other words, if your problem can be solved with a few if/then statements, it is probably not a good fit for machine learning due to unnecessary complexity.</p>"},{"location":"faq.html#what-environment-sapi-should-i-run-rubix-ml-in","title":"What environment (SAPI) should I run Rubix ML in?","text":"<p>All Rubix ML projects are designed to run from the PHP command line interface (CLI). The reason almost always boils down to performance and memory consumption.</p> <p>If you would like to serve your models in production, the preferred method is to use the Server library to spin up a high-performance standalone model server from the command line. If you plan to implement your own model server, we recommend using an asynchronous event loop such as React PHP or Swoole to prevent the model from having to be loaded into memory on each request.</p> <p>To run a PHP script using the command line interface (CLI), open a terminal window and enter: <pre><code>$ php example.php\n</code></pre></p> <p>Note</p> <p>The PHP interpreter must be installed and in your default PATH for the above syntax to work correctly.</p>"},{"location":"faq.html#im-getting-out-of-memory-errors","title":"I'm getting out of memory errors.","text":"<p>Try adjusting the <code>memory_limit</code> option in your php.ini file to something more reasonable. We recommend setting this to <code>-1</code> (no limit) or slightly below your device's memory supply for best results.</p> <p>You can temporarily set the <code>memory_limit</code> in your script by using the <code>ini_set()</code> function.</p> <pre><code>ini_set('memory_limit', '-1');\n</code></pre> <p>Note</p> <p>Training can require a lot of memory. The amount necessary will depend on the amount of training data and the size of your model. If you have more data than you can hold in memory, some learners allow you to train in batches. See the Online Learning section of the Training docs for more information.</p>"},{"location":"faq.html#training-is-slower-than-usual","title":"Training is slower than usual.","text":"<p>Training time depends on a number of factors including size of the dataset and complexity of the model. If you believe that training is taking unusually long then check the following factors.</p> <ul> <li>Xdebug or other debuggers are not enabled.</li> <li>You have enough RAM to hold the dataset and model in memory without swapping to disk.</li> </ul>"},{"location":"faq.html#does-rubix-ml-support-multiprocessingmultithreading","title":"Does Rubix ML support multiprocessing/multithreading?","text":"<p>Yes, learners that support parallel processing (multiprocessing or multithreading) do so by utilizing a pluggable parallel computing backend such as Amp or extension such as Tensor under the hood.</p>"},{"location":"faq.html#does-rubix-ml-support-deep-learning","title":"Does Rubix ML support Deep Learning?","text":"<p>Yes, a number of learners in the library support Deep Learning including the Multilayer Perceptron classifier and MLP Regressor.</p>"},{"location":"faq.html#does-rubix-ml-support-reinforcement-learning","title":"Does Rubix ML support Reinforcement Learning?","text":"<p>Not currently, but we may in the future.</p>"},{"location":"faq.html#does-rubix-ml-support-time-series-data","title":"Does Rubix ML support time series data?","text":"<p>Yes and no. Currently, the library treats time series data like any other continuous feature. In the future, we may add algorithms that work specifically with a separate time component.</p>"},{"location":"faq.html#how-can-i-contribute-to-the-project","title":"How can I contribute to the project?","text":"<p>Anyone is welcome to contribute to Rubix ML. See the CONTRIBUTING guide in the project root for more info.</p>"},{"location":"faq.html#how-can-i-become-a-sponsor","title":"How can I become a sponsor?","text":"<p>Check out our funding sources here and consider donating to the cause.</p>"},{"location":"grid-search.html","title":"Grid Search","text":"<p>[source]</p>"},{"location":"grid-search.html#grid-search","title":"Grid Search","text":"<p>Grid Search is an algorithm that optimizes hyper-parameter selection. From the user's perspective, the process of training and predicting is the same, however, under the hood Grid Search trains a model for each combination of possible parameters and the best model is selected as the base estimator.</p> <p>Interfaces: Estimator, Learner, Parallel, Persistable, Verbose</p> <p>Data Type Compatibility: Depends on base learner</p>"},{"location":"grid-search.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 base string The class name of the base learner. 2 params array An array of lists containing the possible values for each of the base learner's constructor parameters. 3 metric auto Metric The validation metric used to score each set of hyper-parameters. 4 validator KFold Validator The validator used to test and score the model."},{"location":"grid-search.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\GridSearch;\nuse Rubix\\ML\\Classifiers\\KNearestNeighbors;\nuse Rubix\\ML\\Kernels\\Distance\\Euclidean;\nuse Rubix\\ML\\Kernels\\Distance\\Manhattan;\nuse Rubix\\ML\\CrossValidation\\Metrics\\FBeta;\nuse Rubix\\ML\\CrossValidation\\KFold;\n\n$params = [\n    [1, 3, 5, 10], [true, false], [new Euclidean(), new Manhattan()],\n];\n\n$estimator = new GridSearch(KNearestNeighbors::class, $params, new FBeta(), new KFold(5));\n</code></pre>"},{"location":"grid-search.html#additional-methods","title":"Additional Methods","text":"<p>Return the base learner instance: <pre><code>public base() : ?\\Rubix\\ML\\Learner\n</code></pre></p>"},{"location":"hyper-parameter-tuning.html","title":"Hyper-parameter Tuning","text":"<p>Hyper-parameter tuning is an experimental process that incorporates cross-validation to guide hyper-parameter selection. When choosing an estimator for your project it often helps to fine-tune its hyper-parameters in order to get the best accuracy and performance from the model.</p>"},{"location":"hyper-parameter-tuning.html#manual-tuning","title":"Manual Tuning","text":"<p>When actively tuning a model, we will train an estimator with one set of hyper-parameters, obtain a validation score, and then use that as a baseline to make future adjustments. The goal at each iteration is to determine whether the adjustments improve accuracy or cause it to decrease. We can consider a model to be fully tuned when adjustments to the hyper-parameters can no longer make improvements to the validation score. With practice, we'll develop an intuition for which parameters need adjusting. Refer to the API documentation for each learner for a description of each hyper-parameter. In the example below, we'll tune the radius parameter of Radius Neighbors Regressor by iterating over the following block of code with a different setting each time. At first, we can start by choosing radius from a set of values and then honing in on the best value once we have obtained the settings with the highest SMAPE score.</p> <pre><code>use Rubix\\ML\\Regressors\\RadiusNeighborsRegressor;\nuse Rubix\\ML\\CrossValidation\\Metrics\\SMAPE;\n\n[$training, $testing] = $dataset-&gt;randomize()-&gt;split(0.8);\n\n$estimator = new RadiusNeighborsRegressor(0.5); // 0.1, 0.5, 1.0, 2.0, 5.0\n\n$estimator-&gt;train($training);\n\n$predictions = $estimator-&gt;predict($testing);\n\n$metric = new SMAPE();\n\n$score = $metric-&gt;score($predictions, $testing-&gt;labels());\n\necho $score;\n</code></pre> <pre><code>-4.75\n</code></pre>"},{"location":"hyper-parameter-tuning.html#deterministic-training","title":"Deterministic Training","text":"<p>When the algorithm that trains a Learner is stochastic or randomized, it may be desirable for the sake of hyper-parameter tuning to isolate the effect of randomness on training. Fortunately, PHP makes it easy to seed the pseudo-random number generator (PRNG) with a known constant so your training sessions are repeatable. To seed the random number generator call the <code>srand()</code> function at the start of your training script passing any integer constant. After that point the PRNG will generate the same series of random numbers each time the training script is run.</p> <pre><code>srand(42)\n</code></pre>"},{"location":"hyper-parameter-tuning.html#hyper-parameter-optimization","title":"Hyper-parameter Optimization","text":"<p>In distinction to manual tuning, Hyper-parameter optimization is an AutoML technique that employs search and meta-learning strategies to explore various algorithm configurations. In Rubix ML, hyper-parameter optimizers are implemented as meta-estimators that wrap a base learner whose hyper-parameters we wish to optimize.</p>"},{"location":"hyper-parameter-tuning.html#grid-search","title":"Grid Search","text":"<p>Grid Search is a meta-estimator that aims to find the combination of hyper-parameters that maximizes a particular cross-validation Metric. It works by training and testing a unique model for each combination of possible hyper-parameters and then picking the combination that returns the highest validation score. Since Grid Search implements the Parallel interface, we can greatly reduce the search time by training many models in parallel.</p> <p>As an example, we could attempt to find the best setting for the hyper-parameter k in K Nearest Neighbors from a list of possible values <code>1</code>, <code>3</code>, <code>5</code>, and <code>10</code>. In addition, we could try each value of k with distance weighting turned on or off. We might also want to know if the data is sensitive to the underlying distance kernel so we'll try the standard Euclidean as well as the Manhattan distances. The order in which the sets of possible parameters are given to Grid Search is the same order they are given in the constructor of the learner.</p> <pre><code>use Rubix\\ML\\GridSearch;\nuse Rubix\\ML\\Classifiers\\KNearestNeighbors;\nuse Rubix\\ML\\Kernels\\Distance\\Euclidean;\nuse Rubix\\ML\\Kernels\\Distance\\Manhattan;\n\n$params = [\n    [1, 3, 5, 10], [true, false], [new Euclidean(), new Manhattan()]\n];\n\n$estimator = new GridSearch(KNearestNeighbors::class, $params);\n\n$estimator-&gt;train($dataset);\n</code></pre> <p>Once training is complete, Grid Search automatically trains the base learner with the best hyper-parameters on the full dataset and can perform inference like a normal estimator.</p> <pre><code>$predictions = $estimator-&gt;predict($dataset);\n</code></pre> <p>We can also dump the selected hyper-parameters by calling the <code>params()</code> method on the base learner. To return the base learner trained by Grid Search, call the <code>base()</code> method like in the example below.</p> <pre><code>print_r($estimator-&gt;base()-&gt;params());\n</code></pre> <pre><code>Array\n(\n    [k] =&gt; 3\n    [weighted] =&gt; true\n    [kernel] =&gt; Rubix\\ML\\Kernels\\Distance\\Euclidean Object ()\n)\n</code></pre>"},{"location":"hyper-parameter-tuning.html#grid-search-vs-random-search","title":"Grid Search vs. Random Search","text":"<p>When the possible values of the continuous hyper-parameters are selected such that they are evenly spaced out in a grid, we call that grid search. You can use the static <code>grid()</code> method on the Params helper to generate an array of evenly-spaced values automatically.</p> <pre><code>use Rubix\\ML\\Helpers\\Params;\n\n$params = [\n    Params::grid(1, 10, 4), [true, false], // ...\n];\n</code></pre> <p>When the list of possible continuous-valued hyper-parameters is randomly chosen from a distribution, we call that random search. In the absence of a good manual strategy, random search has the advantage of being able to search the hyper-parameter space more effectively by testing combinations of parameters that might not have been considered otherwise. To generate a list of random values from a uniform distribution you can use either the <code>ints()</code> or <code>floats()</code> method on the Params helper.</p> <pre><code>use Rubix\\ML\\Helpers\\Params;\n\n$params = [\n    Params::ints(1, 10, 4), [true, false], // ...\n];\n</code></pre>"},{"location":"inference.html","title":"Inference","text":"<p>Inference is the process of making predictions using an Estimator. You can think of an estimator inferring the outcome of a sample given the input features and the estimator's hidden state obtained during training. Once a learner has been trained it can perform inference on any number of samples.</p>"},{"location":"inference.html#estimator-types","title":"Estimator Types","text":"<p>There are 4 base estimator types to consider in Rubix ML and each type outputs a prediction specific to its type. Meta-estimators are polymorphic in the sense that they take on the type of the base estimator they wrap.</p> Estimator Type Prediction Data Type Example Classifier Class label String 'cat' Regressor Number Integer or Float 1.348957 Clusterer Cluster number Integer 6 Anomaly Detector 1 for an anomaly or 0 otherwise Integer 0"},{"location":"inference.html#making-predictions","title":"Making Predictions","text":"<p>All estimators implement the Estimator interface which provides the <code>predict()</code> method. The <code>predict()</code> method takes a dataset of unknown samples and returns their predictions from the model in an array.</p> <p>Note</p> <p>The inference samples must contain the same number and order of feature columns as the samples used to train the learner.</p> <pre><code>$predictions = $estimator-&gt;predict($dataset);\n\nprint_r($predictions);\n</code></pre> <pre><code>Array\n(\n    [0] =&gt; cat\n    [1] =&gt; dog\n    [2] =&gt; frog\n)\n</code></pre>"},{"location":"inference.html#estimation-of-probabilities","title":"Estimation of Probabilities","text":"<p>Sometimes, you may want to know how certain the model is about a particular outcome. Classifiers and clusterers that implement the Probabilistic interface have the <code>proba()</code> method that computes the joint probability estimates for each class or cluster number as shown in the example below.</p> <pre><code>$probabilities = $estimator-&gt;proba($dataset);  \n\nprint_r($probabilities);\n</code></pre> <pre><code>Array\n(\n    [0] =&gt; Array\n        (\n            [cat] =&gt; 0.6\n            [dog] =&gt; 0.4\n            [frog] =&gt; 0.0\n        )\n    [1] =&gt; Array\n        (\n            [cat] =&gt; 0.3\n            [dog] =&gt; 0.6\n            [frog] =&gt; 0.1\n        )\n    [2] =&gt; Array\n        (\n            [cat] =&gt; 0.0\n            [dog] =&gt; 0.0\n            [frog] =&gt; 1.0\n        )\n)\n</code></pre>"},{"location":"inference.html#anomaly-scores","title":"Anomaly Scores","text":"<p>Anomaly detectors that implement the Scoring interface can output the anomaly scores assigned to the samples in a dataset. Anomaly scores are useful for attaining the degree of anomalousness for a sample relative to other samples. Higher anomaly scores equate to greater abnormality whereas low scores are typical of normal samples.</p> <pre><code>$scores = $estimator-&gt;score($dataset);\n\nprint_r($scores);\n</code></pre> <pre><code>Array\n(\n    [0] =&gt; 0.35033\n    [1] =&gt; 0.40992\n    [2] =&gt; 1.68153\n)\n</code></pre>"},{"location":"installation.html","title":"Installation","text":"<p>Install Rubix ML into your project using Composer:</p> <pre><code>$ composer require rubix/ml\n</code></pre>"},{"location":"installation.html#requirements","title":"Requirements","text":"<ul> <li>PHP 7.4 or above</li> </ul> <p>Recommended</p> <ul> <li>Tensor extension for fast Matrix/Vector computing</li> </ul> <p>Optional</p> <ul> <li>GD extension for image support</li> <li>Mbstring extension for fast multibyte string manipulation</li> <li>SVM extension for Support Vector Machine engine (libsvm)</li> <li>PDO extension for relational database support</li> <li>GraphViz for graph visualization</li> </ul>"},{"location":"learner.html","title":"Learner","text":"<p>Most estimators have the ability to be trained with data. These estimators are called Learners and require training before they are can make predictions. Training is the process of feeding data to the learner so that it can form a generalized representation or model of the dataset.</p>"},{"location":"learner.html#train-a-learner","title":"Train a Learner","text":"<p>To train a learner pass a training dataset as argument to the <code>train()</code> method: <pre><code>public train(Dataset $training) : void\n</code></pre></p> <pre><code>$estimator-&gt;train($dataset);\n</code></pre> <p>Note</p> <p>Calling the <code>train()</code> method on an already trained learner will erase its previous training. If you would like to train a model incrementally, you can do so with learners implementing the Online interface.</p>"},{"location":"learner.html#is-the-learner-trained","title":"Is the Learner Trained?","text":"<p>Return whether or not the learner has been trained: <pre><code>public trained() : bool\n</code></pre></p> <pre><code>var_dump($estimator-&gt;trained());\n</code></pre> <pre><code>bool(true)\n</code></pre>"},{"location":"model-ensembles.html","title":"Model Ensembles","text":"<p>Ensemble learning is when multiple estimators are used together to form the prediction of a sample. Model ensembles can consist of multiple variations of the same estimator, a heterogeneous mix of estimators of the same type, or even a mix of different estimator types.</p>"},{"location":"model-ensembles.html#bootstrap-aggregator","title":"Bootstrap Aggregator","text":"<p>Bootstrap Aggregation or bagging is a technique that trains multiple clones of the same estimator that each specialize on a subset of the training set known as a bootstrap set. The final prediction is the averaged prediction returned by the ensemble. By averaging, we can often achieve greater accuracy than a single estimator at the cost of training more models. In the example below, we'll wrap a Regression Tree in the Bootstrap Aggregator meta-estimator to form a forest of 1,000 trees. Calling the <code>train()</code> method will train the ensemble and afterward the meta-estimator can be used to make predictions like a regular estimator.</p> <pre><code>use Rubix\\ML\\BootstrapAggregator;\nuse Rubix\\ML\\Regressors\\RegressionTree;\n\n$estimator = new BootstrapAggregator(new RegressionTree(5), 1000);\n\n$estimator-&gt;train($dataset);\n</code></pre>"},{"location":"model-ensembles.html#committee-machine","title":"Committee Machine","text":"<p>Committee Machine is another ensemble learner that works by the principal of averaging. It is a meta-estimator consisting of a heterogeneous mix of estimators (referred to as experts) with user-programmable influences. Each expert is trained on the same dataset and the final prediction is based on the contribution of each expert weighted by their influence in the committee. By varying the influences of the experts, we can control which estimators contribute more or less to the final prediction.</p> <pre><code>use Rubix\\ML\\CommitteeMachine;\nuse Rubix\\ML\\RandomForest;\nnew Rubix\\ML\\SoftmaxClassifier;\nuse Rubix\\ML\\AdaBoost;\n\n$estimator = new CommitteeMachine([\n    new RandomForest(),\n    new SoftmaxClassifier(128),\n    new AdaBoost(),\n], [\n    3.0, 1.7, 2.5,\n]);\n</code></pre>"},{"location":"model-ensembles.html#model-chaining","title":"Model Chaining","text":"<p>Model chaining is a form of ensemble learning that uses the predictions of one or more estimators as the input features to other downstream estimators. In this simple example, let's say we want to predict if we should give a customer a loan or not. One thing we could do is we could first predict the customer's credit score and then add it to the dataset with the original features for the loan classifier to train and infer on. We'll write a custom callback function to add the new feature to the training set after their values have been predicted using the Lambda Function transformer. The callback accepts three arguments - the current sample passed by reference, the current row offset, and a context variable which we'll use to store the predicted credit scores.</p> <pre><code>use Rubix\\ML\\Regressors\\KDNeighborsRegressor;\nuse Rubix\\ML\\Transformers\\LambdaFunction;\nuse Rubix\\ML\\Classifiers\\RandomForest;\nuse Rubix\\ML\\Classifiers\\ExtraTreeClassifier;\n\n$creditScoreEstimator = new KDNeighborsRegressor(10);\n\n$creditScoreEstimator-&gt;train($dataset);\n\n$creditScores = $creditScoreEstimator-&gt;predict($dataset);\n\n$addFeature = function (&amp;$sample, $offset, $context) {\n    $sample[] = $context[$offset];\n}\n\n$dataset-&gt;apply(new LambdaFunction($addFeature, $creditScores));\n\n$loanApprovalEstimator = new RandomForest(new ExtraTreeClassifier(8), 300);\n\n$loanApprovalEstimator-&gt;train($dataset);\n</code></pre>"},{"location":"model-ensembles.html#model-orchestra","title":"Model Orchestra","text":"<p>When you combine chaining with model averaging you get a technique referred to as stacking. Unlike Committee Machine, which relies on a priori knowledge of the estimator influences, stacking aims to learn the influence scores automatically by using another model. We introduce the orchestra pattern for implementing a stacked model ensemble. The complete model consists of three Probabilistic classifiers referred to as the orchestra and a conductor that makes the final prediction by training on the class probabilities outputted by the orchestra. A key step to this process is to separate the training set into two sets so that we can do a second optimization to determine the model influences. We can vary the amount of data used to train each layer of the model by changing the proportion argument to the <code>stratifiedSplit()</code> method. For this example, we'll choose to use half of the data to train the orchestra and half to train the conductor.</p> <pre><code>use Rubix\\ML\\Classifiers\\KDNeighbors;\nuse Rubix\\ML\\Classifiers\\RadiusNeighbors;\nuse Rubix\\ML\\Classifiers\\AdaBoost;\nuse Rubix\\ML\\Classifiers\\MultilayerPerceptron;\nuse Rubix\\ML\\NeuralNet\\Layers\\Dense;\nuse Rubix\\ML\\NeuralNet\\Layers\\Activation;\nuse Rubix\\ML\\NeuralNet\\ActivationFunctions\\ReLU;\nuse Rubix\\ML\\Datasets\\Labeled;\n\n[$dataset1, $dataset2] = $training-&gt;stratifiedSplit(0.5);\n\n$orchestra = [\n    new KDNeighbors(10),\n    new RadiusNeighbors(2.0),\n    new AdaBoost(),\n];\n\n$samples = [];\n\nforeach ($orchestra as $estimator) {\n    $estimator-&gt;train($dataset1);\n\n    $probabilities = $estimator-&gt;proba($dataset2);\n\n    foreach ($probabilities as $offset =&gt; $dist) {\n        $sample = &amp;$sample[$offset];\n\n        $sample = array_merge($sample, array_values($dist));\n    }\n}\n\n$dataset = new Labeled($samples, $dataset2-&gt;labels());\n\n$conductor = new MultilayerPerceptron([\n    new Dense(100),\n    new Activation(new ReLU()),\n]);\n\n$conductor-&gt;train($dataset);\n</code></pre>"},{"location":"model-persistence.html","title":"Model Persistence","text":"<p>Model persistence is the ability to save and subsequently load a learner's state in another process. Trained estimators can be used for real-time inference by loading the model onto a server or they can be saved to make predictions in batches offline at a later time. Estimators that implement the Persistable interface are able to have their internal state captured between processes. In addition, the library provides the Persistent Model meta-estimator that acts as a wrapper for persistable estimators.</p>"},{"location":"model-persistence.html#serialization","title":"Serialization","text":"<p>Serialization occurs in between saving and loading a model and can be thought of as packaging the model's parameters. The data can be in a lightweight format such as with PHP's Native serializer or in a robust format such as RBX. In the this example, we'll demonstrate how to encode a Persistable learner using the compressed RBX format, save the encoding with a Persister, and then how to deserialize the encoding.</p> <pre><code>use Rubix\\ML\\Classifiers\\RandomForest;\nuse Rubix\\ML\\Serializers\\RBX;\nuse Rubix\\ML\\Persisters\\Filesystem;\n\n$estimator = new RandomForest(100);\n\n$serializer = new RBX();\n\n$encoding = $serializer-&gt;serialize($estimator);\n\n$estimator = $serializer-&gt;deserialize($encoding);\n</code></pre> <p>Note</p> <p>Due to a limitation in PHP, anonymous classes and functions (closures) are not able to be deserialized. Therefore, avoid anonymous classes or functions if you intend to persist the model.</p>"},{"location":"model-persistence.html#persistent-model-meta-estimator","title":"Persistent Model Meta-estimator","text":"<p>The persistence subsystem can be interfaced at a low level with Serializer and Persister objects or it can be interacted with at a higher level using the Persistent Model meta-estimator. It is a decorator that provides <code>save()</code> and <code>load()</code> methods giving the estimator the ability to save and load itself.</p> <pre><code>use Rubix\\ML\\PersistentModel;\nuse Rubix\\ML\\Clusterers\\KMeans;\nuse Rubix\\ML\\Persisters\\Filesystem;\n\n$estimator = new PersistentModel(new KMeans(5), new Filesystem('example.rbx'));\n\n$estimator-&gt;train($dataset);\n\n$estimator-&gt;save();\n</code></pre> <pre><code>use Rubix\\ML\\PersistentModel;\nuse Rubix\\ML\\Persisters\\Filesystem;\n\n$estimator = PersistentModel::load(new Filesystem('example.rbx'));\n</code></pre>"},{"location":"model-persistence.html#persisting-transformers","title":"Persisting Transformers","text":"<p>In addition to Learners, the persistence subsystem can be used to individually save and load any Stateful transformer that implements the Persistable interface. In the example below we'll fit a transformer to a dataset and then save it to the Filesystem.</p> <pre><code>use Rubix\\ML\\Transformers\\OneHotEncoder;\nuse Rubix\\ML\\Serializers\\RBX;\nuse Rubix\\ML\\Persisters\\Filesystem;\n\n$transformer = new OneHotEncoder();\n\n$serializer = new RBX();\n\n$transformer-&gt;fit($dataset);\n\n$serializer-&gt;serialize($transformer)-&gt;saveTo(new Filesystem('example.rbx'));\n</code></pre> <p>Then, to load the transformer in another process call the <code>deserialize()</code> method on the encoding returned by the persister's <code>load()</code> method.</p> <pre><code>use Rubix\\ML\\Serializers\\RBX;\nuse Rubix\\ML\\Persisters\\Filesystem;\n\n$transformer = $persister-&gt;load()-&gt;deserializeWith(new RBX());\n</code></pre>"},{"location":"model-persistence.html#caveats","title":"Caveats","text":"<p>Since model data are exported with the learner's current class definition in mind, problems may occur when loading a model using a different version of the library than the one it was trained and saved on. For example, when upgrading to a new version, there is a small chance that a previously saved learner may not be able to be deserialized if the model is not compatible with the learner's new class definition. For maximum interoperability, ensure that each system is running the same version of the library.</p>"},{"location":"online.html","title":"Online","text":"<p>Learners that implement the Online interface can be trained in batches. Learners of this type are great for when you either have a continuous stream of data or a dataset that is too large to fit into memory. In addition, partial training allows the model to evolve over time.</p>"},{"location":"online.html#partially-train","title":"Partially Train","text":"<p>To partially train an Online learner pass it a training set to its <code>partial()</code> method: <pre><code>public partial(Dataset $dataset) : void\n</code></pre></p> <pre><code>$folds = $dataset-&gt;fold(3);\n\n$estimator-&gt;train($folds[0]);\n\n$estimator-&gt;partial($folds[1]);\n\n$estimator-&gt;partial($folds[2]);\n</code></pre> <p>Note</p> <p>Learner will continue to train as long as you are using the <code>partial()</code> method, however, calling <code>train()</code> on a trained or partially trained learner will reset it back to baseline first.</p>"},{"location":"parallel.html","title":"Parallel","text":"<p>Multiprocessing is the use of two or more processes that execute in parallel. Objects that implement the Parallel interface can take advantage of multicore processors by executing parts or all of the algorithm in parallel. Choose a number of processes equal to the number of CPU cores in order to take advantage of a system's full processing capability.</p> <p>Note</p> <p>Most parallel learners are configured to use the Serial backend by default.</p>"},{"location":"parallel.html#set-a-backend","title":"Set a Backend","text":"<p>Parallelizable objects can utilize a parallel processing Backend by passing it to the <code>setBackend()</code> method.</p> <p>To set the backend processing engine: <pre><code>public setBackend(Backend $backend) : void\n</code></pre></p> <pre><code>use Rubix\\ML\\Classifiers\\RandomForest;\nuse Rubix\\ML\\Backends\\Amp;\n\n$estimator = new RandomForest();\n\n$estimator-&gt;setBackend(new Amp(16));\n</code></pre>"},{"location":"persistable.html","title":"Persistable","text":"<p>An estimator that implements the Persistable interface can be serialized by a Serializer or save and loaded using the Persistent Model meta-estimator.</p> <p>To return the current class revision hash: <pre><code>public revision() : string\n</code></pre></p> <pre><code>echo $persistable-&gt;revision();\n</code></pre> <pre><code>e7eeec9a\n</code></pre>"},{"location":"persistent-model.html","title":"Persistent Model","text":"<p>[source]</p>"},{"location":"persistent-model.html#persistent-model","title":"Persistent Model","text":"<p>The Persistent Model meta-estimator wraps a Persistable learner with additional functionality for saving and loading the model. It uses Persister objects to interface with various storage backends such as the Filesystem.</p> <p>Interfaces: Estimator, Learner, Probabilistic, Scoring</p> <p>Data Type Compatibility: Depends on base learner</p>"},{"location":"persistent-model.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 base Persistable The persistable base learner. 2 persister Persister The persister used to interface with the storage system. 3 serializer RBX Serializer The object serializer."},{"location":"persistent-model.html#examples","title":"Examples","text":"<pre><code>use Rubix\\ML\\PersistentModel;\nuse Rubix\\ML\\Clusterers\\KMeans;\nuse Rubix\\ML\\Persisters\\Filesystem;\nuse Rubix\\ML\\Serializers\\RBX;\n\n$estimator = new PersistentModel(new KMeans(10), new Filesystem('example.model'), new RBX());\n</code></pre>"},{"location":"persistent-model.html#additional-methods","title":"Additional Methods","text":"<p>Load the model from storage: <pre><code>public static load(Persister $persister, ?Serializer $serializer = null) : self\n</code></pre></p> <pre><code>use Rubix\\ML\\PersistentModel;\nuse Rubix\\ML\\Persisters\\Filesystem;\nuse Rubix\\ML\\Serializers\\RBX;\n\n$estimator = PersistentModel::load(new Filesystem('example.model'), new RBX());\n</code></pre> <p>Save the model to storage: <pre><code>public save() : void\n</code></pre></p> <pre><code>$estimator-&gt;save();\n</code></pre>"},{"location":"pipeline.html","title":"Pipeline","text":"<p>[source]</p>"},{"location":"pipeline.html#pipeline","title":"Pipeline","text":"<p>Pipeline is a meta-estimator capable of transforming an input dataset by applying a series of Transformer middleware. Under the hood, Pipeline will automatically fit the training set and transform any Dataset object supplied as an argument to one of the base estimator's methods before reaching the method context. With elastic mode enabled, Pipeline will update the fitting of Elastic transformers during partial training.</p> <p>Note</p> <p>Pipeline modifies the input dataset during fitting. If you need to keep a clean dataset in memory, you can clone the dataset object before calling the method that consumes it.</p> <p>Interfaces: Estimator, Learner, Online, Probabilistic, Scoring, Persistable</p> <p>Data Type Compatibility: Depends on base learner and transformers</p>"},{"location":"pipeline.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 transformers array A list of transformers to be applied in order. 2 estimator Estimator An instance of a base estimator to receive the transformed data. 3 elastic true bool Should we update the elastic transformers during partial training?"},{"location":"pipeline.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Pipeline;\nuse Rubix\\ML\\Transformers\\MissingDataImputer;\nuse Rubix\\ML\\Transformers\\OneHotEncoder;\nuse Rubix\\ML\\Transformers\\PrincipalComponentAnalysis;\nuse Rubix\\ML\\Classifiers\\SoftmaxClassifier;\n\n$estimator = new Pipeline([\n    new MissingDataImputer(),\n    new OneHotEncoder(), \n    new PrincipalComponentAnalysis(20),\n], new SoftmaxClassifier(128), true);\n</code></pre>"},{"location":"pipeline.html#additional-methods","title":"Additional Methods","text":"<p>This meta-estimator does not have any additional methods.</p>"},{"location":"preprocessing.html","title":"Preprocessing","text":"<p>Sometimes, one or more preprocessing steps may need to be taken before handing a dataset off to a Learner. In some cases, data may not be in the correct format and in others you may want to process the data to aid in training.</p>"},{"location":"preprocessing.html#transformers","title":"Transformers","text":"<p>Transformers are objects that perform various preprocessing steps to the samples in a dataset. They take a dataset object as input and transform it in place. Stateful transformers are a type of transformer that must be fitted to a dataset. Fitting a dataset to a transformer is much like training a learner but in the context of preprocessing rather than inference. After fitting a stateful transformer, it will expect the features to be present in the same order when transforming subsequent datasets. A few transformers are supervised meaning they must be fitted with a Labeled dataset. Elastic transformers can have their fittings updated with new data after an initial fitting.</p>"},{"location":"preprocessing.html#transform-a-dataset","title":"Transform a Dataset","text":"<p>An example of a transformation is one that converts the categorical features of a dataset to continuous ones using a one hot encoding. To accomplish this with the library, pass a One Hot Encoder instance as an argument to the Dataset object's <code>apply()</code> method. Note that the <code>apply()</code> method also handles fitting a Stateful transformer automatically.</p> <pre><code>use Rubix\\ML\\Transformers\\OneHotEncoder;\n\n$dataset-&gt;apply(new OneHotEncoder());\n</code></pre> <p>Transformations can be chained by calling the <code>apply()</code> method fluently.</p> <pre><code>use Rubix\\ML\\Transformers\\HotDeckImputer;\nuse Rubix\\ML\\Transformers\\OneHotEncoder;\nuse Rubix\\ML\\Transformers\\MinMaxNormalizer;\n\n$dataset-&gt;apply(new HotDeckImputer(5))\n    -&gt;apply(new OneHotEncoder())\n    -&gt;apply(new MinMaxNormalizer());\n</code></pre>"},{"location":"preprocessing.html#transforming-the-labels","title":"Transforming the Labels","text":"<p>Transformers do not alter the labels in a dataset. For that we can pass a callback function to the <code>transformLabels()</code> method on a Labeled dataset instance. The callback accepts a single argument that is the value of the label to be transformed. In this example, we'll convert the categorical labels of a dataset to integer ordinals.</p> <pre><code>$dataset-&gt;transformLabels('intval');\n</code></pre>"},{"location":"preprocessing.html#manually-fitting","title":"Manually Fitting","text":"<p>If you need to fit a Stateful transformer to a dataset other than the one it was meant to transform, you can fit the transformer manually by calling the <code>fit()</code> method before applying the transformation.</p> <pre><code>use Rubix\\ML\\Transformers\\WordCountVectorizer;\n\n$transformer = new WordCountVectorizer(5000);\n\n$transformer-&gt;fit($dataset1);\n\n$dataset2-&gt;apply($transformer);\n</code></pre>"},{"location":"preprocessing.html#update-fitting","title":"Update Fitting","text":"<p>To update the fitting of an Elastic transformer call the <code>update()</code> method with a new dataset.</p> <pre><code>$transformer-&gt;update($dataset);\n</code></pre>"},{"location":"preprocessing.html#types-of-preprocessing","title":"Types of Preprocessing","text":"<p>Here we dive into the different types of data preprocessing that Transformers are capable of.</p>"},{"location":"preprocessing.html#standardization-and-normalization","title":"Standardization and Normalization","text":"<p>Oftentimes, the continuous features of a dataset will be on different scales because they were measured by different methods. For example, age (0 - 100) and income (0 - 9,999,999) are on two widely different scales. Standardization is the processes of transforming a dataset such that the features are all on one common scale. Normalization is the special case where the transformed features have a range between 0 and 1. Depending on the transformer, it may operate on the columns or the rows of the dataset.</p> Transformer Operates Output Range Stateful Elastic L1 Normalizer Row-wise [0, 1] L2 Normalizer Row-wise [0, 1] Max Absolute Scaler Column-wise [-1, 1] \u25cf \u25cf Min Max Normalizer Column-wise [min, max] \u25cf \u25cf Robust Standardizer Column-wise [-\u221e, \u221e] \u25cf Z Scale Standardizer Column-wise [-\u221e, \u221e] \u25cf \u25cf"},{"location":"preprocessing.html#feature-conversion","title":"Feature Conversion","text":"<p>Feature converters are transformers that convert feature columns of one data type to another by changing their representation.</p> Transformer From To Stateful Elastic Interval Discretizer Continuous Categorical \u25cf One Hot Encoder Categorical Continuous \u25cf Numeric String Converter Categorical Continuous Boolean Converter Other Categorical or Continuous"},{"location":"preprocessing.html#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction is a preprocessing technique for projecting a dataset onto a lower dimensional vector space. It allows a learner to train and infer quicker by producing a training set with fewer but more informative features. Dimensionality reducers can also be used to visualize datasets by outputting low (1 - 3) dimensionality embeddings for use in plotting software.</p> Transformer Supervised Stateful Elastic Gaussian Random Projector \u25cf Linear Discriminant Analysis \u25cf \u25cf Principal Component Analysis \u25cf Sparse Random Projector \u25cf Truncated SVD \u25cf t-SNE"},{"location":"preprocessing.html#feature-expansion","title":"Feature Expansion","text":"<p>Feature expansion aims to add flexibility to a model by deriving additional features from a dataset. It can be thought of as the opposite of dimensionality reduction.</p> Transformer Supervised Stateful Elastic Polynomial Expander"},{"location":"preprocessing.html#imputation","title":"Imputation","text":"<p>Imputation is a technique for handling missing values in a dataset by replacing them with a pretty good guess.</p> Transformer Compatibility Supervised Stateful Elastic KNN Imputer Depends on distance kernel \u25cf Missing Data Imputer Categorical, Continuous \u25cf Hot Deck Imputer Depends on distance kernel \u25cf"},{"location":"preprocessing.html#natural-language","title":"Natural Language","text":"<p>The library provides a number of transformers for Natural Language Processing (NLP) and Information Retrieval (IR) tasks such as text cleaning, feature extraction, and term weighting.</p> Transformer Supervised Stateful Elastic BM25 Transformer \u25cf \u25cf Regex Filter Text Normalizer Multibyte Text Normalizer Stop Word Filter TF-IDF Transformer \u25cf \u25cf Token Hashing Vectorizer Word Count Vectorizer \u25cf"},{"location":"preprocessing.html#images","title":"Images","text":"<p>These transformers operate on the high-level image data type.</p> Transformer Supervised Stateful Elastic Image Resizer Image Rotator Image Vectorizer \u25cf"},{"location":"preprocessing.html#custom-transformations","title":"Custom Transformations","text":"<p>In additional to providing specialized Transformers for common preprocessing tasks, the library includes a Lambda Function transformer that allows you to apply custom data transformations using a callback. The callback function accepts a sample passed by reference so that the transformation occurs in-place. In the following example, let's write a callback to binarize the continuous features just at column offset 3 of the dataset.</p> <pre><code>use Rubix\\ML\\Transformers\\LambdaFunction;\n\n$binarize = function (&amp;$sample) {\n    $sample[3] = $sample[3] &gt; 182 ? 'tall' : 'not tall';\n}\n\n$dataset-&gt;apply(new LambdaFunction($binarize));\n</code></pre> <p>Another technique we can employ using the Lambda Function transformer is to perform a categorical feature cross between two feature columns of a dataset. A cross feature is a higher-order feature that represents the presence of two or more features simultaneously. For example, we may want to represent the combination of someone's gender and education level as it's own feature. We'll choose to represent the new feature as a CRC32 hash to save on memory and storage but you could just concatenate both categories to represent the new feature as well.</p> <pre><code>use Rubix\\ML\\Transformers\\LambdaFunction;\nuse function hash;\n\n$crossFeatures = function (&amp;$sample) {\n    $sample[] = hash('crc32b', \"{$sample[6]} and {$sample[7]}\");\n};\n\n$dataset-&gt;apply(new LambdaFunction($crossFeatures));\n</code></pre>"},{"location":"preprocessing.html#advanced-preprocessing","title":"Advanced Preprocessing","text":"<p>In some cases, certain features of a dataset may require a different set of preprocessing steps than the others. In such a case, we can extract a certain set of features, preprocess them, and then join them with the rest of the dataset later. In the example below, we'll extract just the text reviews and their sentiment labels into a dataset object and put the sample's category, number of clicks, and ratings into another one using two Column Pickers. Then, we can apply a separate set of transformations to each set of features and use the <code>join()</code> method to combine them into a single dataset. We can even apply another set of transformations to the joined dataset after that.</p> <pre><code>use Rubix\\ML\\Dataset\\Labeled;\nuse Rubix\\ML\\Extractors\\ColumnPicker;\nuse Rubix\\ML\\Extractors\\NDJSON;\nuse Rubix\\ML\\Dataset\\Unlabeled;\nuse Rubix\\ML\\Transformers\\TextNormalizer;\nuse Rubix\\ML\\Transformers\\WordCountVectorizer;\nuse Rubix\\ML\\Transformers\\TfIdfTransformer;\nuse Rubix\\ML\\Transformers\\OneHotEncoder;\nuse Rubix\\ML\\Transformers\\ZScaleStandardizer;\n\n$extractor1 = new ColumnPicker(new NDJSON('example.ndjson'), [\n    'review', 'sentiment',\n]);\n\n$extractor2 = new ColumnPicker(new NDJSON('example.ndjson'), [\n    'category', 'clicks', 'rating',\n]);\n\n$dataset1 = Labeled::fromIterator($extractor1)\n    -&gt;apply(new TextNormalizer())\n    -&gt;apply(new WordCountVectorizer(5000))\n    -&gt;apply(new TfIdfTransformer());\n\n$dataset2 = Unlabeled::fromIterator($extractor2)\n    -&gt;apply(new OneHotEncoder());\n\n$dataset = $dataset1-&gt;join($dataset2)\n    -&gt;apply(new ZScaleStandardizer());\n</code></pre>"},{"location":"preprocessing.html#transformer-pipelines","title":"Transformer Pipelines","text":"<p>The Pipeline meta-estimator helps you automate a series of transformations applied to the input dataset to an estimator. With a Pipeline, any dataset object passed to will automatically be fitted and/or transformed before it arrives in the estimator's context. In addition, transformer fittings can be saved alongside the model data when the Pipeline is persisted.</p> <pre><code>use Rubix\\ML\\Pipeline;\nuse Rubix\\ML\\Transformers\\HotDeckImputer;\nuse Rubix\\ML\\Transformers\\OneHotEncoder;\nuse Rubix\\ML\\Transformers\\ZScaleStandardizer;\nuse Rubix\\ML\\Clusterers\\KMeans;\n\n$estimator = new Pipeline([\n    new HotDeckImputer(5),\n    new OneHotEncoder(),\n    new ZScaleStandardizer(),\n], new KMeans(10));\n</code></pre> <p>Calling <code>train()</code> or <code>partial()</code> will result in the transformers being fitted or updated before being passed to the Softmax Classifier.</p> <pre><code>$estimator-&gt;train($dataset); // Transformers fitted and applied\n\n$estimator-&gt;partial($dataset); // Transformers updated and applied\n</code></pre> <p>Any time a dataset is passed to the Pipeline it will automatically be transformed before being handed to the underlying estimator.</p> <pre><code>$predictions = $estimator-&gt;predict($dataset); // Dataset transformed automatically\n</code></pre>"},{"location":"preprocessing.html#filtering-records","title":"Filtering Records","text":"<p>In some cases, you may want to remove entire rows from the dataset. For example, you may want to remove records that contain features with abnormally low/high values as these samples can be interpreted as noise. The <code>filter()</code> method on the dataset object uses a callback function to determine if a row should be included in the return dataset. In this example, we'll filter all the samples whose value for feature at offset 3 is greater than some amount.</p> <pre><code>$tallPeople = function ($record) {\n    return $record[3] &gt; 178.5;\n};\n\n$dataset = $dataset-&gt;filter($tallPeople);\n</code></pre> <p>Let's say we wanted to train a classifier with our Labeled dataset but only on a subset of the possible class outcomes. We could filter the samples that correspond to undesirable outcomes by targetting the label with our callback.</p> <pre><code>use function in_array;\n\n$dogsAndCats = function ($record) {\n    return in_array(end($record), ['dog', 'cat']);\n}\n\n$training = $dataset-&gt;filter($dogsAndCats);\n</code></pre> <p>Note</p> <p>For Labeled datasets the label column is always the last column of the record.</p> <p>In the next example, we'll filter all the records that have missing feature values. We can detect missing continuous variables by calling the custom library function <code>iterator_contains_nan()</code> on each record. Additionally, we can filter records with missing categorical values by looking for a special placeholder category, in this case we'll use the value <code>'?'</code>, to denote missing categorical variables.</p> <pre><code>use function Rubix\\ML\\iterator_contains_nan;\nuse function in_array;\n\n$noMissingValues = function ($record) {\n    return !iterator_contains_nan($record) and !in_array('?', $record);\n};\n\n$complete = $dataset-&gt;filter($noMissingValues);\n</code></pre> <p>Note</p> <p>The standard PHP library function <code>in_array()</code> does not handle <code>NAN</code> comparisons.</p>"},{"location":"preprocessing.html#de-duplication","title":"De-duplication","text":"<p>When it is undesirable for a dataset to contain duplicate records, you can remove all duplicates by calling the <code>deduplicate()</code> method on the dataset object.</p> <pre><code>$dataset-&gt;deduplicate();\n</code></pre> <p>Note</p> <p>The O(N^2) time complexity of de-duplication may be prohibitive for large datasets.</p>"},{"location":"probabilistic.html","title":"Probabilistic","text":"<p>Estimators that implement the Probabilistic interface have the <code>proba()</code> method that returns an array of joint probability estimates for every possible class or cluster number. Probabilities are useful for ascertaining the degree to which the estimator is certain about a particular outcome. A value of 1 indicates that the estimator is 100% certain about a particular class or cluster number. Conversely, a value of 0 means that the estimator is 100% certain that it's not that class or cluster number. When the probabilities are considered together they are called a joint distribution and always sum to 1.</p>"},{"location":"probabilistic.html#predict-probabilities","title":"Predict Probabilities","text":"<p>Return the joint probability estimates from a dataset: <pre><code>public proba(Dataset $dataset) : array\n</code></pre></p> <pre><code>$probabilities = $estimator-&gt;proba($dataset);  \n\nprint_r($probabilities);\n</code></pre> <pre><code>Array\n(\n    [0] =&gt; Array\n        (\n            [monster] =&gt; 0.6\n            [not monster] =&gt; 0.4\n        )\n    [1] =&gt; Array\n        (\n            [monster] =&gt; 0.5\n            [not monster] =&gt; 0.5\n        )\n    [2] =&gt; Array\n        (\n            [monster] =&gt; 0.2\n            [not monster] =&gt; 0.8\n        )\n)\n</code></pre>"},{"location":"ranks-features.html","title":"Ranks Features","text":"<p>The Ranks Features interface is for learners that can determine the importances of the features used to train them. Low importance is given to feature columns that do not contribute significantly in the model whereas high importance indicates that the feature is more influential. Feature importances can help explain the predictions derived from a model and can also be used to identify informative features for feature selection.</p>"},{"location":"ranks-features.html#feature-importances","title":"Feature Importances","text":"<p>Return the importance scores of each feature column of the training set: <pre><code>public featureImportances() : array\n</code></pre></p> <pre><code>$estimator-&gt;train($dataset);\n\n$importances = $estimator-&gt;featureImportances();\n\nprint_r($importances);\n</code></pre> <pre><code>Array\n(\n    [0] =&gt; 0.04757\n    [1] =&gt; 0.37948\n    [2] =&gt; 0.53170\n    [3] =&gt; 0.04123\n)\n</code></pre>"},{"location":"representing-your-data.html","title":"Representing Your Data","text":"<p>The library makes it easy to work with your data via the Dataset object, which is a specialized data container that every learner can recognize. A dataset is made up of a matrix of samples comprised of features which are usually scalar values. Each sample is a sequentially-ordered array with exactly the same number of elements as every other sample. The columns of the matrix contain the values for a particular feature represented by that column. The dimensionality of a sample is equal to the number of features it has. For example, the samples below are said to be 3-dimensional because they contain 3 feature columns. You'll notice that samples can be made up of a heterogeneous mix of data types.</p> <pre><code>$samples = [\n    [0.1, 21.5, 'furry'],\n    [2.0, -5, 'rough'],\n    [0.001, -10, 'rough'],\n];\n</code></pre>"},{"location":"representing-your-data.html#high-level-data-types","title":"High-level Data Types","text":"<p>The library comes with a higher-order type system that distinguishes types that are continuous, categorical (discrete), or some other data type. The distinction between types is important for determining the operations that can be performed on a particular feature.</p> Library Type PHP Type Continuous Integer or floating point number Categorical String Image GD Image object or resource"},{"location":"representing-your-data.html#continuous-features","title":"Continuous Features","text":"<p>Continuous features represent some quantitative property of the sample and are represented as natural, integer, or real (floating point) numbers. They can be broken down into intervals, ratios, and counts each with their own properties and constraints. One property they all share, however, is that the distances between adjacent values are equal and consistent.</p>"},{"location":"representing-your-data.html#intervals","title":"Intervals","text":"<p>Intervals are the most general form of continuous measurement and can take on any value within the set of real numbers. Some examples of interval data include temperature in Celsius or Fahrenheit, income, and scores on a personality test.</p>"},{"location":"representing-your-data.html#ratios","title":"Ratios","text":"<p>Ratios are lower bounded at a fixed zero point. Due to this extra constraint, ratio variables are able to say something about the relative differences between samples by comparing numbers on the scale to absolute zero. Examples of ratio data include height, distance, and temperature in Kelvin.</p>"},{"location":"representing-your-data.html#counts","title":"Counts","text":"<p>Count variables are limited to the set of natural (or counting) numbers and therefore are always non-negative.</p>"},{"location":"representing-your-data.html#categorical-features","title":"Categorical Features","text":"<p>Categories are discrete values that describe a qualitative property of a sample such as texture, genre, or political party. They are represented as strings and, unlike continuous features, have no numerical relationship between the values.</p>"},{"location":"representing-your-data.html#categories","title":"Categories","text":"<p>Categorical or nominal variables specify which category a sample belongs to among a finite set of choices. For example, a texture feature might include categories such as <code>rough</code>, <code>furry</code>, or <code>smooth</code>.</p>"},{"location":"representing-your-data.html#ordinals","title":"Ordinals","text":"<p>Numeric strings such as <code>'1'</code> and <code>'2'</code> are considered categorical variables in our high-level type system. This conveniently allows you to represent ordinals as ordered categories in which the distances between the levels could be arbitrary.</p>"},{"location":"representing-your-data.html#booleans","title":"Booleans","text":"<p>A boolean (or binary) variable is a special case of a categorical variable in which the number of possible categories is strictly two. For example, to denote if a subject is tall or not you can use the <code>tall</code> and <code>not tall</code> categories respectively.</p>"},{"location":"representing-your-data.html#text","title":"Text","text":"<p>Text data are a product of language communication and can be viewed as a dense encoding of many sparse features. Initially, text blobs are imported as categorical features, however, they have little meaning as a category because the features are still encoded. Thus, import text blobs and use a preprocessing step to extract features such as word counts, weighted term frequencies, or word embeddings.</p>"},{"location":"representing-your-data.html#images","title":"Images","text":"<p>Images are represented as either the GD resource type or a <code>GdImage</code> object. An image type is a special type that holds a reference to the data stored within the image file. For this reason, images must eventually be converted to a scalar type, such as the RGB color intensity values of each pixel, before they can be the input to a learning algorithm.</p>"},{"location":"representing-your-data.html#datetime","title":"Date/Time","text":"<p>There are a number of ways that date/time features can be represented in a dataset. One way is to discretize the value into days, months, and years using categories like <code>june</code>, <code>july</code>, <code>august</code>, and <code>2020</code>, <code>2021</code>. Date/times can also be represented as continuous features by converting them to a numerical timestamp.</p>"},{"location":"representing-your-data.html#what-about-null","title":"What about NULL?","text":"<p>Null values are often used to indicate the absence of a value, however since they do not give any information as to the type of variable that is missing, they cannot be used in a dataset. Instead, represent missing values as either the standard PHP math constant <code>NAN</code> for continuous features or use a special category (such as <code>?</code>) to denote missing categorical values.</p>"},{"location":"scoring.html","title":"Scoring","text":"<p>A Scoring anomaly detector is one that assigns anomaly scores to unknown samples in a dataset. The interface provides the <code>score()</code> method which returns a set of scores from the model. Higher scores indicate a greater degree of anomalousness. In addition, samples can be sorted by their anomaly score to identify the top outliers.</p>"},{"location":"scoring.html#score-a-dataset","title":"Score a Dataset","text":"<p>Return the anomaly scores assigned to the samples in a dataset: <pre><code>public score(Dataset $dataset) : array\n</code></pre></p> <pre><code>$scores = $estimator-&gt;score($dataset);\n\nprint_r($scores);\n</code></pre> <pre><code>Array\n(\n    [0] =&gt; 0.35033\n    [1] =&gt; 0.40992\n    [2] =&gt; 1.68153\n)\n</code></pre>"},{"location":"training.html","title":"Training","text":"<p>Most estimators have the ability to be trained with data. Estimators that require training are called Learners and implement the <code>train()</code> method among others. Training is the process of feeding data to the learner so that it can build an internal representation (or model). Supervised learners require a Labeled training set. Unsupervised learners can be trained with either a Labeled or Unlabeled dataset but only the information contained within the features are used to build the model. Depending on the size of your dataset and choice of learning algorithm, training can a long time or just a few seconds. We recommend assessing your time (compute) and memory requirements before training large models.</p> <p>To begin training a learner, pass a training Dataset object to the <code>train()</code> method on the learner instance like in the example below.</p> <pre><code>$estimator-&gt;train($dataset);\n</code></pre> <p>We can verify that a learner has been trained by calling the <code>trained()</code> method which returns true if the estimator is ready to make predictions.</p> <pre><code>var_dump($estimator-&gt;trained());\n</code></pre> <pre><code>bool(true)\n</code></pre>"},{"location":"training.html#batch-vs-online-learning","title":"Batch vs Online Learning","text":"<p>Batch learning is when a learner is trained in full using only one dataset in a single session. Calling the <code>train()</code> method on a learner instance is an example of batch learning. In contrast, online learning occurs when a learner is trained over multiple sessions with multiple datasets as small as a single sample each. Learners that are capable of being partially trained like this implement the Online interface which includes the <code>partial()</code> method for training with a dataset in an online scheme. Subsequent calls to the <code>partial()</code> method will continue training where the learner left off. Online learning is especially useful for when you have a dataset that is too large to fit into memory all at once or when your dataset is in the form of a stream.</p> <pre><code>$estimator-&gt;train($dataset1);\n\n$estimator-&gt;partial($dataset2);\n\n$estimator-&gt;partial($dataset3);\n</code></pre> <p>Note</p> <p>After the initial training, the learner will expect subsequent training sets to contain the same number and order of features.</p>"},{"location":"training.html#monitoring-progress","title":"Monitoring Progress","text":"<p>Since training is often an iterative process, it is useful to obtain feedback as to how the learner is progressing in real-time. For example, you may want to monitor the training loss to make sure that it isn't increasing instead of decreasing with training. Such early feedback saves you time by allowing you to abort training early if things aren't going well. Learners that implement the Verbose interface accept a PSR-3 logger instance that can be used to output training information at each time step (or epoch). The library comes built-in with the Screen logger that does the job for most cases.</p> <pre><code>use Rubix\\ML\\Classifiers\\LogisticRegression;\nuse Rubix\\ML\\NeuralNet\\Optimizers\\Adam;\nuse Rubix\\ML\\Loggers\\Screen;\n\n$estimator = new LogisticRegression(128, new Adam(0.01));\n\n$estimator-&gt;setLogger(new Screen());\n\n$estimator-&gt;train($dataset);\n</code></pre> <pre><code>[2020-09-04 08:39:04] INFO: Logistic Regression (batch_size: 128, optimizer: Adam (rate: 0.01, momentum_decay: 0.1, norm_decay: 0.001), alpha: 0.0001, epochs: 1000, min_change: 0.0001, window: 5, cost_fn: Cross Entropy) initialized\n[2020-09-04 08:39:04] INFO: Epoch 1 - Cross Entropy: 0.16895133388673\n[2020-09-04 08:39:04] INFO: Epoch 2 - Cross Entropy: 0.16559247705179\n[2020-09-04 08:39:04] INFO: Epoch 3 - Cross Entropy: 0.16294448401323\n[2020-09-04 08:39:04] INFO: Epoch 4 - Cross Entropy: 0.16040135038265\n[2020-09-04 08:39:04] INFO: Epoch 5 - Cross Entropy: 0.15786801071483\n[2020-09-04 08:39:04] INFO: Epoch 6 - Cross Entropy: 0.1553151426337\n[2020-09-04 08:39:04] INFO: Epoch 7 - Cross Entropy: 0.15273253982757\n[2020-09-04 08:39:04] INFO: Epoch 8 - Cross Entropy: 0.15011771931339\n[2020-09-04 08:39:04] INFO: Epoch 9 - Cross Entropy: 0.14747194148672\n[2020-09-04 08:39:04] INFO: Epoch 10 - Cross Entropy: 0.14479847759871\n...\n[2020-09-04 08:39:04] INFO: Epoch 77 - Cross Entropy: 0.0082096137827592\n[2020-09-04 08:39:04] INFO: Epoch 78 - Cross Entropy: 0.0081004235278088\n[2020-09-04 08:39:04] INFO: Epoch 79 - Cross Entropy: 0.0079956096838174\n[2020-09-04 08:39:04] INFO: Epoch 80 - Cross Entropy: 0.0078948616067878\n[2020-09-04 08:39:04] INFO: Epoch 81 - Cross Entropy: 0.0077978960869396\n[2020-09-04 08:39:04] INFO: Training complete\n</code></pre>"},{"location":"training.html#parallel-training","title":"Parallel Training","text":"<p>Learners that implement the Parallel interface can utilize a parallel processing (multiprocessing) backend for training. Parallel computing can greatly reduce training time on multicore systems at the cost of some overhead to synchronize the data. For small datasets, the overhead may actually cause the runtime to increase. Most parallel learners do not use parallel processing by default, so to enable it you must set a parallel backend using the <code>setBackend()</code> method. In the example below, we'll train a Random Forest classifier with 500 trees in parallel using the Amp backend under the hood. By settings the <code>$workers</code> argument to 4 we tell the backend to use up to 4 cores at a time to execute the computation.</p> <pre><code>use Rubix\\ML\\Classifiers\\RandomForest;\nuse Rubix\\ML\\Classifiers\\ClassificationTree;\nuse Rubix\\ML\\Backends\\Amp;\n\n$estimator = new RandomForest(new ClassificationTree(20), 500);\n\n$estimator-&gt;setBackend(new Amp(4));\n\n$estimator-&gt;train($dataset);\n</code></pre>"},{"location":"training.html#feature-importances","title":"Feature Importances","text":"<p>Learners that implement the Ranks Features interface can evaluate the importance of each feature in the training set. Feature importances are defined as the degree to which an individual feature influences the model. Feature importances are useful for feature selection and for helping to explain predictions derived from a model. To output the importance scores, call the <code>featureImportances()</code> method of a trained learner that implements the Ranks Features interface.</p> <pre><code>use Rubix\\ML\\Regressors\\Ridge;\n\n$estimator = new Ridge();\n\n$estimator-&gt;train($dataset);\n\n$importances = $estimator-&gt;featureImportances();\n\nprint_r($importances);\n</code></pre> <pre><code>Array\n(\n    [0] =&gt; 0.04757\n    [1] =&gt; 0.37948\n    [2] =&gt; 0.53170\n    [3] =&gt; 0.04123\n)\n</code></pre>"},{"location":"verbose.html","title":"Verbose","text":"<p>Verbose objects are capable of logging important events to any PSR-3 compatible logger such as Monolog, Analog, or the included Screen Logger. Logging is especially useful for monitoring the progress of the underlying learning algorithm in real-time.</p>"},{"location":"verbose.html#set-the-logger","title":"Set the Logger","text":"<p>To set the logger pass in any PSR-3 compatible logger instance: <pre><code>public setLogger(LoggerInterface $logger) : void\n</code></pre></p>"},{"location":"verbose.html#return-the-logger","title":"Return the Logger","text":"<p>Return the logger or null if not set: <pre><code>public logger() : ?LoggerInterface\n</code></pre></p> <pre><code>use Rubix\\ML\\Regressors\\Adaline;\nuse Rubix\\ML\\Loggers\\Screen;\n\n$estimator = new Adaline();\n\n$estimator-&gt;setLogger(new Screen('example'));\n\n$estimator-&gt;train($dataset);\n</code></pre> <pre><code>[2020-08-05 04:26:11] INFO: Learner init Adaline {batch_size: 128, optimizer: Adam {rate: 0.01, momentum_decay: 0.1, norm_decay: 0.001}, alpha: 0.0001, epochs: 100, min_change: 0.001, window: 5, cost_fn: Huber Loss {alpha: 1}}\n[2020-08-05 04:26:11] INFO: Training started\n[2020-08-05 04:26:11] example.INFO: Epoch 1 - Huber Loss {alpha: 1}: 0.36839299586132\n[2020-08-05 04:26:11] example.INFO: Epoch 2 - Huber Loss {alpha: 1}: 0.0018235958273629\n[2020-08-05 04:26:11] example.INFO: Epoch 3 - Huber Loss {alpha: 1}: 0.0017358090553563\n[2020-08-05 04:26:11] example.INFO: Training complete\n</code></pre>"},{"location":"what-is-machine-learning.html","title":"What is Machine Learning?","text":"<p>Machine Learning (ML) is a form of Artificial Intelligence (AI) that uses data to train a computer to perform tasks. Unlike traditional programming, in which rules are programmed explicitly, machine learning uses algorithms to build rulesets automatically. At a high level, machine learning is a collection of techniques borrowed from many disciplines including statistics, probability theory, and neuroscience combined with novel ideas for the purpose of gaining insight through data and computation. Machine Learning is further broken down into subcategories based on how the learners are trained and the tasks they handle.</p>"},{"location":"what-is-machine-learning.html#supervised-learning","title":"Supervised Learning","text":"<p>Supervised learning is a type of machine learning that incorporates a training signal in the form of labels which are often determined by a human expert. Labels are the desired output of a learner given the sample we are showing it. For this reason, you can think of supervised leaning as learning by example. There are two types of supervised learning to consider in Rubix ML.</p>"},{"location":"what-is-machine-learning.html#classification","title":"Classification","text":"<p>For classification problems, a learner is trained to differentiate samples among a set of k possible discrete classes. In this type of problem, the training labels are the classes that each sample belongs to. Examples of class labels include <code>cat</code>, <code>dog</code>, <code>human</code>, or any other categorical label. Classification problems include image recognition, text sentiment analysis, and Iris flower classification.</p>"},{"location":"what-is-machine-learning.html#regression","title":"Regression","text":"<p>Regression is a learning problem that aims to predict a continuous-valued outcome. In this case, the training labels are continuous data types such as integers and floating point numbers. Regression problems include estimating house prices, credit scoring, and the steering angle of an autonomous vehicle.</p>"},{"location":"what-is-machine-learning.html#unsupervised-learning","title":"Unsupervised Learning","text":"<p>A form of learning that does not require labeled data is called Unsupervised learning. Unsupervised learners focus on digesting patterns within just the samples. There are three types of unsupervised learning offered in Rubix ML.</p>"},{"location":"what-is-machine-learning.html#clustering","title":"Clustering","text":"<p>Clustering takes a dataset and assigns each of the samples a discrete cluster number based on its similarity to other samples from the training set. It can be viewed as a weaker form of classification where the class names are unknown. Clustering is used to group colors, segment customer databases, and to discover communities within social networks for example.</p>"},{"location":"what-is-machine-learning.html#anomaly-detection","title":"Anomaly Detection","text":"<p>Anomalies are defined as samples that have been generated by a different process than normal. Samples can either be flagged or ranked based on their anomaly score. Anomaly detection is used in information security for intrusion and denial of service detection, and in the financial industry to detect fraud.</p>"},{"location":"what-is-machine-learning.html#manifold-learning","title":"Manifold Learning","text":"<p>Manifold learning is a type of unsupervised non-linear dimensionality reduction used for embedding datasets into dense feature representations. Embedders can be used for visualizing high dimensional (4 or more) datasets in low (1 to 3) dimensions, or for compressing the information within the samples before input to a learning algorithm.</p>"},{"location":"what-is-machine-learning.html#deep-learning","title":"Deep Learning","text":"<p>Deep Learning is a subset of machine learning that incorporates layers of computation that form feature representations of greater and greater complexity. It is a paradigm shift from feature engineering to letting the learner construct its own features from the raw data. Deep Learning is used in image recognition, natural language processing (NLP), and for other tasks involving very high-dimensional raw inputs.</p>"},{"location":"what-is-machine-learning.html#automl","title":"AutoML","text":"<p>Automated Machine Learning (AutoML) is the application of automated tools when designing machine learning models. The goal of AutoML is to simplify the machine learning lifecycle for non-experts and to facilitate rapid prototyping. In addition, AutoML can aid in the discovery of simpler and more accurate solutions than could otherwise be discovered by human intuition alone. Rubix provides a number of tools to help automate the machine learning process including hyper-parameter optimizers and feature selectors.</p>"},{"location":"what-is-machine-learning.html#other-forms-of-ml","title":"Other Forms of ML","text":"<p>Although the supervised and unsupervised learning framework covers a substantial number of problems, there are other types of machine learning that the library does not support out of the box.</p>"},{"location":"what-is-machine-learning.html#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Reinforcement Learning (RL) is a type of machine learning that aims to learn the optimal control of an agent within an environment through cumulative reward. The data used to train an RL learner are the states obtained by performing some action and then observing the response. If supervised learning is learning by example then reinforcement learning is learning from mistakes. Reinforcement learning is used to train AIs to play games such as Go, Chess, and Starcraft 2, and in robotics for movement planning.</p>"},{"location":"what-is-machine-learning.html#sequence-learning","title":"Sequence Learning","text":"<p>Sequence Learning is a type of ML that aims to predict the next value in a sequence such as the next word in a sentence or a future stock price. It differs from learning from sets of data in that the order of the samples matter. Time-series analysis is a special case of sequence learning where the sequences are ordered by time. Sequence-to-sequence Learning is used to denote when the output is not just the next value but the next sequence of values.</p>"},{"location":"what-is-machine-learning.html#self-supervised-learning","title":"Self-supervised Learning","text":"<p>A hybrid approach to learning is Self-supervised learning in which a learner is trained to predict the parts of a sample that were partially omitted during training. As such, supervised methods can be employed on unlabeled data to learn representations. Self-supervised learning is used in language models such as GPT-3 to generate sequences of text or in autonomous robots to learn from ancillary sensor feedback.</p>"},{"location":"anomaly-detectors/gaussian-mle.html","title":"Gaussian MLE","text":"<p>[source]</p>"},{"location":"anomaly-detectors/gaussian-mle.html#gaussian-mle","title":"Gaussian MLE","text":"<p>The Gaussian Maximum Likelihood Estimator (MLE) is able to spot outliers by computing a probability density function (PDF) over the features assuming they are independently and normally (Gaussian) distributed. Samples that are assigned low probability density are more likely to be outliers.</p> <p>Interfaces: Estimator, Learner, Online, Scoring, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"anomaly-detectors/gaussian-mle.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 contamination 0.1 float The proportion of outliers that are assumed to be present in the training set. 2 smoothing 1e-9 float The amount of epsilon smoothing added to the variance of each feature."},{"location":"anomaly-detectors/gaussian-mle.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\AnomalyDetectors\\GaussianMLE;\n\n$estimator = new GaussianMLE(0.03, 1e-8);\n</code></pre>"},{"location":"anomaly-detectors/gaussian-mle.html#additional-methods","title":"Additional Methods","text":"<p>Return the column means computed from the training set: <pre><code>public means() : float[]\n</code></pre></p> <p>Return the column variances computed from the training set: <pre><code>public variances() : float[]\n</code></pre></p>"},{"location":"anomaly-detectors/gaussian-mle.html#references","title":"References","text":"<ol> <li> <p>T. F. Chan et al. (1979). Updating Formulae and a Pairwise Algorithm for Computing Sample Variances.\u00a0\u21a9</p> </li> </ol>"},{"location":"anomaly-detectors/isolation-forest.html","title":"Isolation Forest","text":"<p>[source]</p>"},{"location":"anomaly-detectors/isolation-forest.html#isolation-forest","title":"Isolation Forest","text":"<p>An ensemble of Isolation Trees that each specialize on a unique subset of the training set. Isolation Trees are a type of randomized decision tree that assign anomaly scores based on the depth a sample reaches when traversing the tree. Based on the premise that anomalies are isolated into their own nodes sooner, samples that receive high anomaly scores achieve the shallowest depth during traversal.</p> <p>Interfaces: Estimator, Learner, Scoring, Persistable</p> <p>Data Type Compatibility: Categorical, Continuous</p>"},{"location":"anomaly-detectors/isolation-forest.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 estimators 100 int The number of isolation trees to train in the ensemble. 2 ratio null float The ratio of samples to train each estimator with. If null, the subsample size will be set to 256. 3 contamination null float The proportion of outliers that are assumed to be present in the training set. If null, the threshold anomaly score will be set to 0.5."},{"location":"anomaly-detectors/isolation-forest.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\AnomalyDetectors\\IsolationForest;\n\n$estimator = new IsolationForest(100, 0.2, 0.05);\n</code></pre>"},{"location":"anomaly-detectors/isolation-forest.html#additional-methods","title":"Additional Methods","text":"<p>This estimator does not have any additional methods.</p>"},{"location":"anomaly-detectors/isolation-forest.html#references","title":"References","text":"<ol> <li> <p>F. T. Liu et al. (2008). Isolation Forest.\u00a0\u21a9</p> </li> <li> <p>F. T. Liu et al. (2011). Isolation-based Anomaly Detection.\u00a0\u21a9</p> </li> <li> <p>M. Garchery et al. (2018). On the influence of categorical features in ranking anomalies using mixed data.\u00a0\u21a9</p> </li> </ol>"},{"location":"anomaly-detectors/local-outlier-factor.html","title":"Local Outlier Factor","text":"<p>[source]</p>"},{"location":"anomaly-detectors/local-outlier-factor.html#local-outlier-factor","title":"Local Outlier Factor","text":"<p>Local Outlier Factor (LOF) measures the local deviation of density of an unknown sample with respect to its k nearest neighbors from the training set. As such, LOF only considers the neighborhood of an unknown sample which enables it to detect anomalies within individual clusters of data.</p> <p>Interfaces: Estimator, Learner, Scoring, Persistable</p> <p>Data Type Compatibility: Depends on distance kernel</p>"},{"location":"anomaly-detectors/local-outlier-factor.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 k 20 int The k nearest neighbors that form a local region. 2 contamination null float The proportion of outliers that are assumed to be present in the training set. 3 tree KDTree Spatial The spatial tree used to run nearest neighbor searches."},{"location":"anomaly-detectors/local-outlier-factor.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\AnomalyDetectors\\LocalOutlierFactor;\nuse Rubix\\ML\\Graph\\Trees\\BallTree;\nuse Rubix\\ML\\Kernels\\Distance\\Euclidean;\n\n$estimator = new LocalOutlierFactor(20, 0.1, new BallTree(30, new Euclidean));\n</code></pre>"},{"location":"anomaly-detectors/local-outlier-factor.html#additional-methods","title":"Additional Methods","text":"<p>Return the base spatial tree instance: <pre><code>public tree() : Spatial\n</code></pre></p>"},{"location":"anomaly-detectors/local-outlier-factor.html#references","title":"References","text":"<ol> <li> <p>M. M. Breunig et al. (2000). LOF: Identifying Density-Based Local Outliers.\u00a0\u21a9</p> </li> </ol>"},{"location":"anomaly-detectors/loda.html","title":"Loda","text":"<p>[source]</p>"},{"location":"anomaly-detectors/loda.html#loda","title":"Loda","text":"<p>Lightweight Online Detector of Anomalies uses a collection of sparse random projection vectors to provide scalar inputs to an ensemble of unique one-dimensional equi-width histograms. Each histogram then estimates the probability density of the unknown sample using a limited feature set. The final predictions are derived from the averaged densities over the entire ensemble.</p> <p>Interfaces: Estimator, Learner, Online, Scoring, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"anomaly-detectors/loda.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 estimators 100 int The number of projection/histogram pairs in the ensemble. 2 bins null int The number of equi-width bins for each histogram. If null then will estimate bin count. 3 contamination 0.1 float The proportion of outliers that are assumed to be present in the training set."},{"location":"anomaly-detectors/loda.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\AnomalyDetectors\\Loda;\n\n$estimator = new Loda(250, 8, 0.01);\n</code></pre>"},{"location":"anomaly-detectors/loda.html#additional-methods","title":"Additional Methods","text":"<p>This estimator does not have any additional methods.</p>"},{"location":"anomaly-detectors/loda.html#references","title":"References","text":"<ol> <li> <p>T. Pevn\u00fd. (2015). Loda: Lightweight on-line detector of anomalies.\u00a0\u21a9</p> </li> <li> <p>L. Birg\u00b4e et al. (2005). How Many Bins Should Be Put In A Regular Histogram.\u00a0\u21a9</p> </li> </ol>"},{"location":"anomaly-detectors/one-class-svm.html","title":"One Class SVM","text":"<p>[source]</p>"},{"location":"anomaly-detectors/one-class-svm.html#one-class-svm","title":"One Class SVM","text":"<p>An unsupervised Support Vector Machine (SVM) used for anomaly detection. The One Class SVM aims to find a maximum margin between a set of data points and the origin, rather than between classes such as with SVC.</p> <p>Note</p> <p>This estimator requires the SVM extension which uses the libsvm engine under the hood.</p> <p>Interfaces: Estimator, Learner</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"anomaly-detectors/one-class-svm.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 nu 0.1 float An upper bound on the percentage of margin errors and a lower bound on the percentage of support vectors. 2 kernel RBF Kernel The kernel function used to express non-linear data in higher dimensions. 3 shrinking true bool Should we use the shrinking heuristic? 4 tolerance 1e-3 float The minimum change in the cost function necessary to continue training. 5 cacheSize 100.0 float The size of the kernel cache in MB."},{"location":"anomaly-detectors/one-class-svm.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\AnomalyDetectors\\OneClassSVM;\nuse Rubix\\ML\\Kernels\\SVM\\Polynomial;\n\n$estimator = new OneClassSVM(0.1, new Polynomial(4), true, 1e-3, 100.0);\n</code></pre>"},{"location":"anomaly-detectors/one-class-svm.html#additional-methods","title":"Additional Methods","text":"<p>Save the model data to the filesystem: <pre><code>public save(string $path) : void\n</code></pre></p> <p>Load the model data from the filesystem: <pre><code>public load(string $path) : void\n</code></pre></p>"},{"location":"anomaly-detectors/one-class-svm.html#references","title":"References","text":"<ol> <li> <p>C. Chang et al. (2011). LIBSVM: A library for support vector machines.\u00a0\u21a9</p> </li> </ol>"},{"location":"anomaly-detectors/robust-z-score.html","title":"Robust Z-Score","text":"<p>[source]</p>"},{"location":"anomaly-detectors/robust-z-score.html#robust-z-score","title":"Robust Z-Score","text":"<p>A statistical anomaly detector that uses modified Z-Scores that are robust to preexisting outliers in the training set. The modified Z-Score is defined as the feature value minus the median over the median absolute deviation (MAD). Anomalies are flagged if their final weighted Z-Score exceeds a user-defined threshold.</p> <p>Note</p> <p>A beta value of 1 means the estimator only considers the maximum absolute Z-Score, whereas a setting of 0 indicates that only the average Z-Score factors into the final score.</p> <p>Interfaces: Estimator, Learner, Scoring, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"anomaly-detectors/robust-z-score.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 threshold 3.5 float The minimum Z-Score to be flagged as an anomaly. 2 beta 0.5 float The weight of the maximum Z-Score in the overall anomaly score. 3 smoothing 1e-9 float The amount of epsilon smoothing added to the MAD of each feature."},{"location":"anomaly-detectors/robust-z-score.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\AnomalyDetectors\\RobustZScore;\n\n$estimator = new RobustZScore(3.5, 0.25, 1e-6);\n</code></pre>"},{"location":"anomaly-detectors/robust-z-score.html#additional-methods","title":"Additional Methods","text":"<p>Return the median of each feature column in the training set: <pre><code>public medians() : float[]|null\n</code></pre></p> <p>Return the median absolute deviation (MAD) of each feature column in the training set: <pre><code>public mads() : float[]|null\n</code></pre></p>"},{"location":"anomaly-detectors/robust-z-score.html#references","title":"References","text":"<ol> <li> <p>B. Iglewicz et al. (1993). How to Detect and Handle Outliers.\u00a0\u21a9</p> </li> </ol>"},{"location":"backends/amp.html","title":"Amp","text":"<p>[source]</p>"},{"location":"backends/amp.html#amp","title":"Amp","text":"<p>Amp Parallel is a multiprocessing subsystem that requires no extensions. It uses a non-blocking concurrency framework that implements coroutines using PHP generator functions under the hood.</p> <p>Note</p> <p>The optimal number of workers will depend on the system specifications of the computer. Fewer workers than CPU cores may not achieve full processing potential but more workers than cores can cause excess overhead.</p>"},{"location":"backends/amp.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 workers Auto int The maximum number of workers in the worker pool. If null then tries to autodetect CPU core count."},{"location":"backends/amp.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Backends\\Amp;\n\n$backend = new Amp(16);\n</code></pre>"},{"location":"backends/amp.html#additional-methods","title":"Additional Methods","text":"<p>Return the maximum number of workers in the worker pool: <pre><code>public workers() : int\n</code></pre></p>"},{"location":"backends/serial.html","title":"Serial","text":"<p>[source]</p>"},{"location":"backends/serial.html#serial","title":"Serial","text":"<p>The Serial backend executes tasks sequentially inside of a single process. The advantage of the Serial backend is that it has zero overhead, thus it may be faster than a parallel backend for small datasets.</p> <p>Note</p> <p>The Serial backend is the default for most objects that are capable of parallel processing.</p>"},{"location":"backends/serial.html#parameters","title":"Parameters","text":"<p>This backend does not have any additional parameters.</p>"},{"location":"backends/serial.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Backends\\Serial;\n\n$backend = new Serial();\n</code></pre>"},{"location":"backends/serial.html#additional-methods","title":"Additional Methods","text":"<p>This backend does not have any additional methods.</p>"},{"location":"classifiers/adaboost.html","title":"AdaBoost","text":"<p>[source]</p>"},{"location":"classifiers/adaboost.html#adaboost","title":"AdaBoost","text":"<p>Short for Adaptive Boosting, this ensemble classifier can improve the performance of an otherwise weak classifier by focusing more attention on samples that are harder to classify. It builds an additive model where, at each stage, a new learner is trained and given an influence score inversely proportional to the loss it incurs at that epoch.</p> <p>Note</p> <p>The default base learner is a Classification Tree with a max height of 1 i.e a Decision Stump.</p> <p>Interfaces: Estimator, Learner, Probabilistic, Verbose, Persistable</p> <p>Data Type Compatibility: Depends on base learner</p>"},{"location":"classifiers/adaboost.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 base ClassificationTree Learner The base weak classifier to be boosted. 2 rate 1.0 float The learning rate of the ensemble i.e. the shrinkage applied to each step. 3 ratio 0.8 float The ratio of samples to subsample from the training set to train each weak learner. 4 epochs 100 int The maximum number of training epochs. i.e. the number of times to iterate before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the training loss to wait before considering an early stop."},{"location":"classifiers/adaboost.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\AdaBoost;\nuse Rubix\\ML\\Classifiers\\ExtraTreeClassifier;\n\n$estimator = new AdaBoost(new ExtraTreeClassifier(3), 0.1, 0.5, 200, 1e-3, 10);\n</code></pre>"},{"location":"classifiers/adaboost.html#additional-methods","title":"Additional Methods","text":"<p>Return an iterable progress table with the steps from the last training session: <pre><code>public steps() : iterable\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('progress.csv', true);\n\n$extractor-&gt;export($estimator-&gt;steps());\n</code></pre> <p>Return the loss for each epoch from the last training session: <pre><code>public losses() : float[]|null\n</code></pre></p>"},{"location":"classifiers/adaboost.html#references","title":"References","text":"<ol> <li> <p>Y. Freund et al. (1996). A Decision-theoretic Generalization of On-line Learning and an Application to Boosting.\u00a0\u21a9</p> </li> <li> <p>J. Zhu et al. (2006). Multi-class AdaBoost.\u00a0\u21a9</p> </li> </ol>"},{"location":"classifiers/classification-tree.html","title":"Classification Tree","text":"<p>[source]</p>"},{"location":"classifiers/classification-tree.html#classification-tree","title":"Classification Tree","text":"<p>A binary tree-based learner that greedily constructs a decision map for classification that minimizes the Gini impurity among the training labels within the leaf nodes. The height and bushiness of the tree can be determined by the user-defined <code>max height</code> and <code>max leaf size</code> hyper-parameters. Classification Trees also serve as the base learner of ensemble methods such as Random Forest and AdaBoost.</p> <p>Interfaces: Estimator, Learner, Probabilistic, Ranks Features, Persistable</p> <p>Data Type Compatibility: Categorical, Continuous</p>"},{"location":"classifiers/classification-tree.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 maxHeight PHP_INT_MAX int The maximum height of the tree. 2 maxLeafSize 3 int The max number of samples that a leaf node can contain. 3 minPurityIncrease 1e-7 float The minimum increase in purity necessary to continue splitting a subtree. 4 maxFeatures Auto int The max number of feature columns to consider when determining a best split. 5 maxBins Auto int The maximum number of bins to consider when determining a split with a continuous feature as the split point."},{"location":"classifiers/classification-tree.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\ClassificationTree;\n\n$estimator = new ClassificationTree(10, 5, 0.001, null, null);\n</code></pre>"},{"location":"classifiers/classification-tree.html#additional-methods","title":"Additional Methods","text":"<p>Export a Graphviz \"dot\" encoding of the decision tree structure. <pre><code>public exportGraphviz() : Encoding\n</code></pre></p> <pre><code>use Rubix\\ML\\Helpers\\Graphviz;\nuse Rubix\\ML\\Persisters\\Filesystem;\n\n$dot = $estimator-&gt;exportGraphviz();\n\nGraphviz::dotToImage($dot)-&gt;saveTo(new Filesystem('tree.png'));\n</code></pre> <p>Return the number of levels in the tree. <pre><code>public height() : ?int\n</code></pre></p> <p>Return a factor that quantifies the skewness of the distribution of nodes in the tree. <pre><code>public balance() : ?int\n</code></pre></p>"},{"location":"classifiers/classification-tree.html#references","title":"References:","text":"<ol> <li> <p>W. Y. Loh. (2011). Classification and Regression Trees.\u00a0\u21a9</p> </li> <li> <p>K. Alsabti. et al. (1998). CLOUDS: A Decision Tree Classifier for Large Datasets.\u00a0\u21a9</p> </li> </ol>"},{"location":"classifiers/extra-tree-classifier.html","title":"Extra Tree Classifier","text":"<p>[source]</p>"},{"location":"classifiers/extra-tree-classifier.html#extra-tree-classifier","title":"Extra Tree Classifier","text":"<p>An Extremely Randomized Classification Tree that recursively chooses node splits with the least entropy among a set of k (given by max features) random split points. Extra Trees are useful in ensembles such as Random Forest or AdaBoost as the weak learner or they can be used on their own. The strength of Extra Trees as compared to standard decision trees are their computational efficiency and lower prediction variance.</p> <p>Interfaces: Estimator, Learner, Probabilistic, Ranks Features, Persistable</p> <p>Data Type Compatibility: Categorical, Continuous</p>"},{"location":"classifiers/extra-tree-classifier.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 maxHeight PHP_INT_MAX int The maximum height of the tree. 2 maxLeafSize 3 int The max number of samples that a leaf node can contain. 3 minPurityIncrease 1e-7 float The minimum increase in purity necessary to continue splitting a subtree. 4 maxFeatures Auto int The max number of feature columns to consider when determining a best split."},{"location":"classifiers/extra-tree-classifier.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\ExtraTreeClassifier;\n\n$estimator = new ExtraTreeClassifier(50, 3, 1e-7, 10);\n</code></pre>"},{"location":"classifiers/extra-tree-classifier.html#additional-methods","title":"Additional Methods","text":"<p>Export a Graphviz \"dot\" encoding of the decision tree structure. <pre><code>public exportGraphviz() : Encoding\n</code></pre></p> <p>Return the number of levels in the tree. <pre><code>public height() : ?int\n</code></pre></p> <p>Return a factor that quantifies the skewness of the distribution of nodes in the tree. <pre><code>public balance() : ?int\n</code></pre></p>"},{"location":"classifiers/extra-tree-classifier.html#references","title":"References","text":"<ol> <li> <p>P. Geurts et al. (2005). Extremely Randomized Trees.\u00a0\u21a9</p> </li> </ol>"},{"location":"classifiers/gaussian-naive-bayes.html","title":"Gaussian Naive Bayes","text":"<p>[source]</p>"},{"location":"classifiers/gaussian-naive-bayes.html#gaussian-naive-bayes","title":"Gaussian Naive Bayes","text":"<p>Gaussian Naive Bayes is a version of the Naive Bayes classifier for continuous features. It places a probability density function (PDF) over the features conditioned on a class basis and uses Bayes' Theorem to derive the final probabilities. In addition to the naive feature independence assumption, Gaussian Naive Bayes also assumes that all features are normally (Gaussian) distributed.</p> <p>Interfaces: Estimator, Learner, Online, Probabilistic, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"classifiers/gaussian-naive-bayes.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 priors null array The class prior probabilities as an associative array with class labels as keys and their prior probabilities as values totalling 1. If null, then priors will automatically be computed from the training data. 2 smoothing 1e-9 float The amount of epsilon smoothing added to the variance of each feature."},{"location":"classifiers/gaussian-naive-bayes.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\GaussianNB;\n\n$estimator = new GaussianNB([\n    'benign' =&gt; 0.9,\n    'malignant' =&gt; 0.1,\n], 1e-9);\n</code></pre>"},{"location":"classifiers/gaussian-naive-bayes.html#additional-methods","title":"Additional Methods","text":"<p>Return the class prior probabilities: <pre><code>public priors() : float[]|null\n</code></pre></p> <p>Return the mean of each feature column for each class: <pre><code>public means() : array[]|null\n</code></pre></p> <p>Return the variance of each feature column for each class: <pre><code>public variances() : array[]|null\n</code></pre></p>"},{"location":"classifiers/gaussian-naive-bayes.html#references","title":"References","text":"<ol> <li> <p>T. F. Chan et al. (1979). Updating Formulae and a Pairwise Algorithm for Computing Sample Variances.\u00a0\u21a9</p> </li> </ol>"},{"location":"classifiers/k-nearest-neighbors.html","title":"K Nearest Neighbors","text":"<p>[source]</p>"},{"location":"classifiers/k-nearest-neighbors.html#k-nearest-neighbors","title":"K Nearest Neighbors","text":"<p>A brute-force distance-based learning algorithm that locates the k nearest samples from the training set and predicts the class label that is most common. K Nearest Neighbors (KNN) is considered a lazy learner because it performs most of its computation at inference time.</p> <p>Note</p> <p>For a faster spatial tree-accelerated version of KNN, see KD Neighbors.</p> <p>Interfaces: Estimator, Learner, Online, Probabilistic, Persistable</p> <p>Data Type Compatibility: Depends on distance kernel</p>"},{"location":"classifiers/k-nearest-neighbors.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 k 5 int The number of nearest neighbors to consider when making a prediction. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 kernel Euclidean Distance The distance kernel used to compute the distance between sample points."},{"location":"classifiers/k-nearest-neighbors.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\KNearestNeighbors;\nuse Rubix\\ML\\Kernels\\Distance\\Manhattan;\n\n$estimator = new KNearestNeighbors(3, false, new Manhattan());\n</code></pre>"},{"location":"classifiers/k-nearest-neighbors.html#additional-methods","title":"Additional Methods","text":"<p>This estimator does not have any additional methods.</p>"},{"location":"classifiers/kd-neighbors.html","title":"K-d Neighbors","text":"<p>[source]</p>"},{"location":"classifiers/kd-neighbors.html#k-d-neighbors","title":"K-d Neighbors","text":"<p>A fast K Nearest Neighbors algorithm that uses a binary search tree (BST) to divide the training set into neighborhoods that contain samples that are close together spatially. K-d Neighbors then does a binary search to locate the nearest neighborhood of an unknown sample and prunes all neighborhoods whose bounding box is further than the k'th nearest neighbor found so far. The main advantage of K-d Neighbors over brute force KNN is that it is much more efficient, however it cannot be partially trained.</p> <p>Interfaces: Estimator, Learner, Probabilistic, Persistable</p> <p>Data Type Compatibility: Depends on distance kernel</p>"},{"location":"classifiers/kd-neighbors.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 k 5 int The number of nearest neighbors to consider when making a prediction. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 tree KDTree Spatial The spatial tree used to run nearest neighbor searches."},{"location":"classifiers/kd-neighbors.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\KDNeighbors;\nuse Rubix\\ML\\Graph\\Trees\\BallTree;\nuse Rubix\\ML\\Kernels\\Distance\\Minkowski;\n\n$estimator = new KDNeighbors(10, false, new BallTree(40, new Minkowski()));\n</code></pre>"},{"location":"classifiers/kd-neighbors.html#additional-methods","title":"Additional Methods","text":"<p>Return the base spatial tree instance: <pre><code>public tree() : Spatial\n</code></pre></p>"},{"location":"classifiers/logistic-regression.html","title":"Logistic Regression","text":"<p>[source]</p>"},{"location":"classifiers/logistic-regression.html#logistic-regression","title":"Logistic Regression","text":"<p>A linear classifier that uses the logistic (sigmoid) function to estimate the probabilities of exactly two class outcomes. The model parameters (weights and bias) are solved using Mini Batch Gradient Descent with pluggable optimizers and cost functions that run on the neural network subsystem.</p> <p>Interfaces: Estimator, Learner, Online, Probabilistic, Ranks Features, Verbose, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"classifiers/logistic-regression.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 batchSize 128 int The number of training samples to process at a time. 2 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 3 l2Penalty 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the training loss to wait before considering an early stop. 7 costFn CrossEntropy ClassificationLoss The function that computes the loss associated with an erroneous activation during training."},{"location":"classifiers/logistic-regression.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\LogisticRegression;\nuse Rubix\\ML\\NeuralNet\\Optimizers\\Adam;\nuse Rubix\\ML\\NeuralNet\\CostFunctions\\CrossEntropy;\n\n$estimator = new LogisticRegression(64, new Adam(0.001), 1e-4, 100, 1e-4, 5, new CrossEntropy());\n</code></pre>"},{"location":"classifiers/logistic-regression.html#additional-methods","title":"Additional Methods","text":"<p>Return an iterable progress table with the steps from the last training session: <pre><code>public steps() : iterable\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('progress.csv', true);\n\n$extractor-&gt;export($estimator-&gt;steps());\n</code></pre> <p>Return the loss for each epoch from the last training session: <pre><code>public losses() : float[]|null\n</code></pre></p> <p>Return the underlying neural network instance or <code>null</code> if untrained: <pre><code>public network() : Network|null\n</code></pre></p>"},{"location":"classifiers/logit-boost.html","title":"Logit Boost","text":"<p>[source]</p>"},{"location":"classifiers/logit-boost.html#logit-boost","title":"Logit Boost","text":"<p>A stage-wise additive ensemble that uses regression trees to iteratively learn a Logistic Regression model for binary classification problems. Unlike standard Logistic Regression, Logit Boost has the ability to learn a smooth non-linear decision surface by training decision trees to follow the gradient of the cross entropy loss function. In addition, Logit Boost concentrates more effort on classifying samples that it is less certain about.</p> <p>Note</p> <p>Logit Boost utilizes progress monitoring via an internal validation set for snapshotting and early stopping. If there are not enough training samples to build an internal validation set given the user-specified holdout ratio then training will proceed with progress monitoring disabled.</p> <p>Interfaces: Estimator, Learner, Probabilistic, Verbose, Ranks Features, Persistable</p> <p>Data Type Compatibility: Depends on base learners</p>"},{"location":"classifiers/logit-boost.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 booster RegressionTree Learner The regressor used to fix up the error residuals of the base learner. 2 rate 0.1 float The learning rate of the ensemble i.e. the shrinkage applied to each step. 3 ratio 0.5 float The ratio of samples to subsample from the training set to train each booster. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 7 holdOut 0.1 float The proportion of training samples to use for internal validation. Set to 0 to disable. 8 metric F Beta Metric The metric used to score the generalization performance of the model during training."},{"location":"classifiers/logit-boost.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\LogitBoost;\nuse Rubix\\ML\\Regressors\\RegressionTree;\nuse Rubix\\ML\\CrossValidation\\Metrics\\FBeta;\n\n$estimator = new LogitBoost(new RegressionTree(4), 0.1, 0.5, 1000, 1e-4, 5, 0.1, new FBeta());\n</code></pre>"},{"location":"classifiers/logit-boost.html#additional-methods","title":"Additional Methods","text":"<p>Return an iterable progress table with the steps from the last training session: <pre><code>public steps() : iterable\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('progress.csv', true);\n\n$extractor-&gt;export($estimator-&gt;steps());\n</code></pre> <p>Return the validation score for each epoch from the last training session: <pre><code>public scores() : float[]|null\n</code></pre></p> <p>Return the loss for each epoch from the last training session: <pre><code>public losses() : float[]|null\n</code></pre></p>"},{"location":"classifiers/logit-boost.html#references","title":"References","text":"<ol> <li> <p>J. H. Friedman et al. (2000). Additive Logistic Regression: A Statistical View of Boosting.\u00a0\u21a9</p> </li> <li> <p>J. H. Friedman. (2001). Greedy Function Approximation: A Gradient Boosting Machine.\u00a0\u21a9</p> </li> <li> <p>J. H. Friedman. (1999). Stochastic Gradient Boosting.\u00a0\u21a9</p> </li> <li> <p>Y. Wei. et al. (2017). Early stopping for kernel boosting algorithms: A general analysis with localized complexities.\u00a0\u21a9</p> </li> <li> <p>G. Ke et al. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree.\u00a0\u21a9</p> </li> </ol>"},{"location":"classifiers/multilayer-perceptron.html","title":"Multilayer Perceptron","text":"<p>[source]</p>"},{"location":"classifiers/multilayer-perceptron.html#multilayer-perceptron","title":"Multilayer Perceptron","text":"<p>A multiclass feed-forward neural network classifier with user-defined hidden layers. The Multilayer Perceptron is a deep learning model capable of forming higher-order feature representations through layers of computation. In addition, the MLP features progress monitoring which stops training when it can no longer improve the validation score. It also utilizes network snapshotting to make sure that it always has the best model parameters even if progress began to decline during training.</p> <p>Note</p> <p>If there are not enough training samples to build an internal validation set with the user-specified holdout ratio then progress monitoring will be disabled.</p> <p>Interfaces: Estimator, Learner, Online, Probabilistic, Verbose, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"classifiers/multilayer-perceptron.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 hidden array An array composing the user-specified hidden layers of the network in order. 2 batchSize 128 int The number of training samples to process at a time. 3 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 4 l2Penalty 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 5 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 6 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 7 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 8 holdOut 0.1 float The proportion of training samples to use for internal validation. Set to 0 to disable. 9 costFn CrossEntropy ClassificationLoss The function that computes the loss associated with an erroneous activation during training. 10 metric FBeta Metric The validation metric used to score the generalization performance of the model during training."},{"location":"classifiers/multilayer-perceptron.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\MultilayerPerceptron;\nuse Rubix\\ML\\NeuralNet\\Layers\\Dense;\nuse Rubix\\ML\\NeuralNet\\Layers\\Dropout;\nuse Rubix\\ML\\NeuralNet\\Layers\\Activation;\nuse Rubix\\ML\\NeuralNet\\Layers\\PReLU;\nuse Rubix\\ML\\NeuralNet\\ActivationFunctions\\LeakyReLU;\nuse Rubix\\ML\\NeuralNet\\Optimizers\\Adam;\nuse Rubix\\ML\\NeuralNet\\CostFunctions\\CrossEntropy;\nuse Rubix\\ML\\CrossValidation\\Metrics\\MCC;\n\n$estimator = new MultilayerPerceptron([\n    new Dense(200),\n    new Activation(new LeakyReLU()),\n    new Dropout(0.3),\n    new Dense(100),\n    new Activation(new LeakyReLU()),\n    new Dropout(0.3),\n    new Dense(50),\n    new PReLU(),\n], 128, new Adam(0.001), 1e-4, 1000, 1e-3, 3, 0.1, new CrossEntropy(), new MCC());\n</code></pre>"},{"location":"classifiers/multilayer-perceptron.html#additional-methods","title":"Additional Methods","text":"<p>Return an iterable progress table with the steps from the last training session: <pre><code>public steps() : iterable\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('progress.csv', true);\n\n$extractor-&gt;export($estimator-&gt;steps());\n</code></pre> <p>Return the loss for each epoch from the last training session: <pre><code>public losses() : float[]|null\n</code></pre></p> <p>Return the validation score for each epoch from the last training session: <pre><code>public scores() : float[]|null\n</code></pre></p> <p>Returns the underlying neural network instance or <code>null</code> if untrained: <pre><code>public network() : Network|null\n</code></pre></p> <p>Export a Graphviz \"dot\" encoding of the neural network architecture. <pre><code>public exportGraphviz() : Encoding\n</code></pre></p> <pre><code>use Rubix\\ML\\Helpers\\Graphviz;\nuse Rubix\\ML\\Persisters\\Filesystem;\n\n$dot = $estimator-&gt;exportGraphviz();\n\nGraphviz::dotToImage($dot)-&gt;saveTo(new Filesystem('network.png'));\n</code></pre> <p></p>"},{"location":"classifiers/multilayer-perceptron.html#references","title":"References","text":"<ol> <li> <p>G. E. Hinton. (1989). Connectionist learning procedures.\u00a0\u21a9</p> </li> <li> <p>L. Prechelt. (1997). Early Stopping - but when?\u00a0\u21a9</p> </li> </ol>"},{"location":"classifiers/naive-bayes.html","title":"Naive Bayes","text":"<p>[source]</p>"},{"location":"classifiers/naive-bayes.html#naive-bayes","title":"Naive Bayes","text":"<p>Categorical Naive Bayes is a probability-based classifier that uses counting and Bayes' Theorem to derive the probabilities of a class given a sample of categorical features. The term naive refers to the fact that Naive Bayes treats each feature as if it was independent of the others even though this is usually not the case in real life.</p> <p>Note</p> <p>Each partial train has the overhead of recomputing the probability mass function for each feature per class. As such, it is better to train with fewer but larger training sets.</p> <p>Interfaces: Estimator, Learner, Online, Probabilistic, Persistable</p> <p>Data Type Compatibility: Categorical</p>"},{"location":"classifiers/naive-bayes.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 priors null array The class prior probabilities as an associative array with class labels as keys and their prior probabilities as values totalling 1. If null, then priors will automatically be computed from the training data. 2 smoothing 1.0 float The amount of Laplace smoothing added to the probabilities."},{"location":"classifiers/naive-bayes.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\NaiveBayes;\n\n$estimator = new NaiveBayes([\n    'spam' =&gt; 0.3,\n    'not spam' =&gt; 0.7,\n], 2.5);\n</code></pre>"},{"location":"classifiers/naive-bayes.html#additional-methods","title":"Additional Methods","text":"<p>Return the class prior probabilities: <pre><code>public priors() : float[]|null\n</code></pre></p> <p>Return the counts for each category per class: <pre><code>public counts() : array[]|null\n</code></pre></p>"},{"location":"classifiers/one-vs-rest.html","title":"One Vs Rest","text":"<p>[source]</p>"},{"location":"classifiers/one-vs-rest.html#one-vs-rest","title":"One Vs Rest","text":"<p>One Vs Rest is an ensemble learner that trains a binary classifier to predict a particular class vs every other class for every possible class. The final class prediction is the class whose binary classifier returned the highest probability. One of the features of One Vs Rest is that it allows you to build a multiclass classifier out of an ensemble of otherwise binary classifiers.</p> <p>Interfaces: Estimator, Learner, Probabilistic, Parallel, Persistable</p> <p>Data Type Compatibility: Depends on the base learner</p>"},{"location":"classifiers/one-vs-rest.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 base Learner Probabilistic"},{"location":"classifiers/one-vs-rest.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\OneVsRest;\nuse Rubix\\ML\\Classifiers\\LogisticRegression;\nuse Rubix\\ML\\NeuralNet\\Optimizers\\Stochastic;\n\n$estimator = new OneVsRest(new LogisticRegression(64, new Stochastic(0.001)));\n</code></pre>"},{"location":"classifiers/one-vs-rest.html#additional-methods","title":"Additional Methods","text":"<p>This estimator does not have any additional methods.</p>"},{"location":"classifiers/radius-neighbors.html","title":"Radius Neighbors","text":"<p>[source]</p>"},{"location":"classifiers/radius-neighbors.html#radius-neighbors","title":"Radius Neighbors","text":"<p>Radius Neighbors is a classifier that takes the distance-weighted vote of each neighbor within a cluster of a fixed user-defined radius to make a prediction. Since the radius of the search can be constrained, Radius Neighbors is more robust to outliers than K Nearest Neighbors. In addition, Radius Neighbors acts as a quasi-anomaly detector by flagging samples that have 0 neighbors within the search radius.</p> <p>Interfaces: Estimator, Learner, Probabilistic, Persistable</p> <p>Data Type Compatibility: Depends on distance kernel</p>"},{"location":"classifiers/radius-neighbors.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 radius 1.0 float The radius within which points are considered neighbors. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 outlierClass '?' string The class label for any samples that have 0 neighbors within the specified radius. 4 tree BallTree Spatial The spatial tree used to run range searches."},{"location":"classifiers/radius-neighbors.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\RadiusNeighbors;\nuse Rubix\\ML\\Graph\\Trees\\KDTree;\nuse Rubix\\ML\\Kernels\\Distance\\Manhattan;\n\n$estimator = new RadiusNeighbors(50.0, true, '?', new KDTree(100, new Manhattan()));\n</code></pre>"},{"location":"classifiers/radius-neighbors.html#additional-methods","title":"Additional Methods","text":"<p>Return the base spatial tree instance: <pre><code>public tree() : Spatial\n</code></pre></p>"},{"location":"classifiers/random-forest.html","title":"Random Forest","text":"<p>[source]</p>"},{"location":"classifiers/random-forest.html#random-forest","title":"Random Forest","text":"<p>Random Forest (RF) is a classifier that trains an ensemble of Decision Trees (Classification Trees or Extra Trees) on random subsets (bootstrap set) of the training data. Predictions are based on the probability scores returned from each tree in the ensemble, averaged and weighted equally. In addition to reliable predictions, Random Forest also returns reliable feature importance scores making it suitable for feature selection.</p> <p>Note</p> <p>The default base tree learner is a fully grown Classification Tree.</p> <p>Interfaces: Estimator, Learner, Probabilistic, Parallel, Ranks Features, Persistable</p> <p>Data Type Compatibility: Categorical, Continuous</p>"},{"location":"classifiers/random-forest.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 base ClassificationTree Learner The base learner. 2 estimators 100 int The number of learners to train in the ensemble. 3 ratio 0.2 float The ratio of samples from the training set to randomly subsample to train each base learner. 4 balanced false bool Should we sample the bootstrap set to compensate for imbalanced class labels?"},{"location":"classifiers/random-forest.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\RandomForest;\nuse Rubix\\ML\\Classifiers\\ClassificationTree;\n\n$estimator = new RandomForest(new ClassificationTree(10), 300, 0.1, true);\n</code></pre>"},{"location":"classifiers/random-forest.html#additional-methods","title":"Additional Methods","text":"<p>This estimator does not have any additional methods.</p>"},{"location":"classifiers/random-forest.html#references","title":"References","text":"<ol> <li> <p>L. Breiman. (2001). Random Forests.\u00a0\u21a9</p> </li> <li> <p>L. Breiman et al. (2005). Extremely Randomized Trees.\u00a0\u21a9</p> </li> </ol>"},{"location":"classifiers/softmax-classifier.html","title":"Softmax Classifier","text":"<p>[source]</p>"},{"location":"classifiers/softmax-classifier.html#softmax-classifier","title":"Softmax Classifier","text":"<p>A multiclass generalization of Logistic Regression using a single layer neural network with a Softmax output layer.</p> <p>Interfaces: Estimator, Learner, Online, Probabilistic, Verbose, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"classifiers/softmax-classifier.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 batchSize 256 int The number of training samples to process at a time. 2 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 3 alpha 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the training loss to wait before considering an early stop. 7 costFn CrossEntropy ClassificationLoss The function that computes the loss associated with an erroneous activation during training."},{"location":"classifiers/softmax-classifier.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\SoftmaxClassifier;\nuse Rubix\\ML\\NeuralNet\\Optimizers\\Momentum;\nuse Rubix\\ML\\NeuralNet\\CostFunctions\\CrossEntropy;\n\n$estimator = new SoftmaxClassifier(256, new Momentum(0.001), 1e-4, 300, 1e-4, 10, new CrossEntropy());\n</code></pre>"},{"location":"classifiers/softmax-classifier.html#additional-methods","title":"Additional Methods","text":"<p>Return an iterable progress table with the steps from the last training session: <pre><code>public steps() : iterable\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('progress.csv', true);\n\n$extractor-&gt;export($estimator-&gt;steps());\n</code></pre> <p>Return the loss for each epoch from the last training session: <pre><code>public losses() : float[]|null\n</code></pre></p> <p>Return the underlying neural network instance or <code>null</code> if untrained: <pre><code>public network() : Network|null\n</code></pre></p>"},{"location":"classifiers/svc.html","title":"SVC","text":"<p>[source]</p>"},{"location":"classifiers/svc.html#svc","title":"SVC","text":"<p>The multiclass Support Vector Machine (SVM) Classifier is a maximum margin classifier that can efficiently perform non-linear classification by implicitly mapping feature vectors into high-dimensional feature space using the kernel trick.</p> <p>Note</p> <p>This learner requires the SVM extension which uses the libsvm engine under the hood.</p> <p>Interfaces: Estimator, Learner</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"classifiers/svc.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 c 1.0 float The parameter that defines the width of the margin used to separate the classes. 2 kernel RBF Kernel The kernel function used to operate in higher dimensions. 3 shrinking true bool Should we use the shrinking heuristic? 4 tolerance 1e-3 float The minimum change in the cost function necessary to continue training. 5 cache size 100.0 float The size of the kernel cache in MB."},{"location":"classifiers/svc.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Classifiers\\SVC;\nuse Rubix\\ML\\Kernels\\SVM\\Linear;\n\n$estimator = new SVC(1.0, new Linear(), true, 1e-3, 100.0);\n</code></pre>"},{"location":"classifiers/svc.html#additional-methods","title":"Additional Methods","text":"<p>Save the model data to the filesystem: <pre><code>public save(string $path) : void\n</code></pre></p> <p>Load the model data from the filesystem: <pre><code>public load(string $path) : void\n</code></pre></p>"},{"location":"classifiers/svc.html#references","title":"References","text":"<ol> <li> <p>C. Chang et al. (2011). LIBSVM: A library for support vector machines.\u00a0\u21a9</p> </li> </ol>"},{"location":"clusterers/dbscan.html","title":"DBSCAN","text":"<p>[source]</p>"},{"location":"clusterers/dbscan.html#dbscan","title":"DBSCAN","text":"<p>Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a clustering algorithm able to find non-linearly separable and arbitrarily-shaped clusters given a radius and density constraint. In addition, DBSCAN can flag outliers (noise samples) and thus be used as a quasi-anomaly detector.</p> <p>Note</p> <p>Noise samples are assigned the cluster number -1.</p> <p>Interfaces: Estimator</p> <p>Data Type Compatibility: Depends on distance kernel</p>"},{"location":"clusterers/dbscan.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 radius 0.5 float The maximum distance between two points to be considered neighbors. 2 minDensity 5 int The minimum number of points within radius of each other to form a cluster. 3 tree BallTree Spatial The spatial tree used to run range searches."},{"location":"clusterers/dbscan.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Clusterers\\DBSCAN;\nuse Rubix\\ML\\Graph\\Trees\\BallTree;\nuse Rubix\\ML\\Kernels\\Distance\\Diagonal;\n\n$estimator = new DBSCAN(4.0, 5, new BallTree(20, new Diagonal()));\n</code></pre>"},{"location":"clusterers/dbscan.html#additional-methods","title":"Additional Methods","text":"<p>This estimator does not have any additional methods.</p>"},{"location":"clusterers/dbscan.html#references","title":"References","text":"<ol> <li> <p>M. Ester et al. (1996). A Density-Based Algorithm for Discovering Clusters.\u00a0\u21a9</p> </li> </ol>"},{"location":"clusterers/fuzzy-c-means.html","title":"Fuzzy C Means","text":"<p>[source]</p>"},{"location":"clusterers/fuzzy-c-means.html#fuzzy-c-means","title":"Fuzzy C Means","text":"<p>A distance-based soft-clustering algorithm that allows samples to belong to multiple clusters if they fall within a fuzzy region controlled by the fuzz hyper-parameter. Like K Means, Fuzzy C Means minimizes the inertia cost function, however, unlike K Means, FCM uses a batch solver that requires the entire training set to compute the update to the cluster centroids at each step.</p> <p>Interfaces: Estimator, Learner, Probabilistic, Verbose, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"clusterers/fuzzy-c-means.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 c int The number of target clusters. 2 fuzz 2.0 float Determines the bandwidth of the fuzzy area. 3 epochs 300 int The maximum number of training rounds to execute. 4 minChange 1e-4 float The minimum change in the inertia for the algorithm to continue training. 5 kernel Euclidean Distance The distance kernel used to compute the distance between sample points. 6 seeder PlusPlus Seeder The seeder used to initialize the cluster centroids."},{"location":"clusterers/fuzzy-c-means.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Clusterers\\FuzzyCMeans;\nuse Rubix\\ML\\Kernels\\Distance\\Euclidean;\nuse Rubix\\ML\\Clusterers\\Seeders\\Random;\n\n$estimator = new FuzzyCMeans(5, 1.2, 400, 1., new Euclidean(), new Random());\n</code></pre>"},{"location":"clusterers/fuzzy-c-means.html#additional-methods","title":"Additional Methods","text":"<p>Return the c computed centroids of the training set: <pre><code>public centroids() : array[]\n</code></pre></p> <p>Return an iterable progress table with the steps from the last training session: <pre><code>public steps() : iterable\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('progress.csv', true);\n\n$extractor-&gt;export($estimator-&gt;steps());\n</code></pre> <p>Returns the inertia at each epoch from the last round of training: <pre><code>public losses() : float[]|null\n</code></pre></p>"},{"location":"clusterers/fuzzy-c-means.html#references","title":"References","text":"<ol> <li> <p>J. C. Bezdek et al. (1984). FCM: The Fuzzy C-Means Clustering Algorithm.\u00a0\u21a9</p> </li> </ol>"},{"location":"clusterers/gaussian-mixture.html","title":"Gaussian Mixture","text":"<p>[source]</p>"},{"location":"clusterers/gaussian-mixture.html#gaussian-mixture","title":"Gaussian Mixture","text":"<p>A Gaussian Mixture model (GMM) is a probabilistic model for representing the presence of clusters within an overall population without requiring a sample to know which sub-population it belongs to beforehand. GMMs are similar to centroid-based clusterers like K Means but allow both the cluster centers (means) as well as the radii (variances) to be learned as well. For this reason, GMMs are especially useful for clusterings that are of different radii.</p> <p>Interfaces: Estimator, Learner, Probabilistic, Verbose, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"clusterers/gaussian-mixture.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 k int The number of target clusters. 2 smoothing 1e-9 float The amount of epsilon smoothing added to the variance of each feature. 3 epochs 100 int The maximum number of training rounds to execute. 4 minChange 1e-3 float The minimum change in the components necessary for the algorithm to continue training. 5 seeder PlusPlus Seeder The seeder used to initialize the Gaussian components."},{"location":"clusterers/gaussian-mixture.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Clusterers\\GaussianMixture;\nuse Rubix\\ML\\Clusterers\\Seeders\\KMC2;\n\n$estimator = new GaussianMixture(5, 1e-6, 100, 1e-4, new KMC2(50));\n</code></pre>"},{"location":"clusterers/gaussian-mixture.html#additional-methods","title":"Additional Methods","text":"<p>Return the cluster prior probabilities based on their representation over all training samples: <pre><code>public priors() : float[]\n</code></pre></p> <p>Return the running means of each feature column for each cluster: <pre><code>public means() : array[]\n</code></pre></p> <p>Return the variance of each feature column for each cluster: <pre><code>public variances() : array[]\n</code></pre></p> <p>Return an iterable progress table with the steps from the last training session: <pre><code>public steps() : iterable\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('progress.csv', true);\n\n$extractor-&gt;export($estimator-&gt;steps());\n</code></pre> <p>Return the loss for each epoch from the last training session: <pre><code>public losses() : float[]|null\n</code></pre></p>"},{"location":"clusterers/gaussian-mixture.html#references","title":"References","text":"<ol> <li> <p>A. P. Dempster et al. (1977). Maximum Likelihood from Incomplete Data via the EM Algorithm.\u00a0\u21a9</p> </li> <li> <p>J. Blomer et al. (2016). Simple Methods for Initializing the EM Algorithm for Gaussian Mixture Models.\u00a0\u21a9</p> </li> </ol>"},{"location":"clusterers/k-means.html","title":"K Means","text":"<p>[source]</p>"},{"location":"clusterers/k-means.html#k-means","title":"K Means","text":"<p>A fast online centroid-based hard clustering algorithm capable of grouping linearly separable data points given some prior knowledge of the target number of clusters (defined by k). K Means is trained using adaptive Mini Batch Gradient Descent and minimizes the inertia cost function at each epoch. Inertia is defined as the average sum of distances between each sample and its nearest cluster centroid.</p> <p>Interfaces: Estimator, Learner, Online, Probabilistic, Persistable, Verbose</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"clusterers/k-means.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 k int The number of target clusters. 2 batch size 128 int The size of each mini batch in samples. 3 epochs 1000 int The maximum number of training rounds to execute. 4 min change 1e-4 float The minimum change in the inertia for training to continue. 5 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 6 kernel Euclidean Distance The distance kernel used to compute the distance between sample points. 7 seeder PlusPlus Seeder The seeder used to initialize the cluster centroids."},{"location":"clusterers/k-means.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Clusterers\\KMeans;\nuse Rubix\\ML\\Kernels\\Distance\\Euclidean;\nuse Rubix\\ML\\Clusterers\\Seeders\\PlusPlus;\n\n$estimator = new KMeans(3, 128, 300, 10.0, 10, new Euclidean(), new PlusPlus());\n</code></pre>"},{"location":"clusterers/k-means.html#additional-methods","title":"Additional Methods","text":"<p>Return the k computed centroids of the training set: <pre><code>public centroids() : array[]\n</code></pre></p> <p>Return the number of training samples that each centroid is responsible for: <pre><code>public sizes() : int[]\n</code></pre></p> <p>Return an iterable progress table with the steps from the last training session: <pre><code>public steps() : iterable\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('progress.csv', true);\n\n$extractor-&gt;export($estimator-&gt;steps());\n</code></pre> <p>Return the loss for each epoch from the last training session: <pre><code>public losses() : float[]|null\n</code></pre></p>"},{"location":"clusterers/k-means.html#references","title":"References","text":"<ol> <li> <p>D. Sculley. (2010). Web-Scale K-Means Clustering.\u00a0\u21a9</p> </li> </ol>"},{"location":"clusterers/mean-shift.html","title":"Mean Shift","text":"<p>[source]</p>"},{"location":"clusterers/mean-shift.html#mean-shift","title":"Mean Shift","text":"<p>A hierarchical clustering algorithm that uses peak (maxima) finding to locate the candidate centroids of a training set given a radius constraint. Near-duplicate centroids are merged together and the algorithm iterates on the remaining candidates in subsequent steps until the centroids stabilize.</p> <p>Interfaces: Estimator, Learner, Probabilistic, Verbose, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"clusterers/mean-shift.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 radius float The bandwidth of the radial basis function. 2 ratio 0.1 float The ratio of samples from the training set to use as initial centroids. 3 epochs 100 int The maximum number of training rounds to execute. 4 minShift 1e-4 float The minimum shift in the position of the centroids necessary to continue training. 5 tree BallTree Spatial The spatial tree used to run range searches. 6 seeder Random Seeder The seeder used to initialize the cluster centroids."},{"location":"clusterers/mean-shift.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Clusterers\\MeanShift;\nuse Rubix\\ML\\Graph\\Trees\\BallTree;\nuse Rubix\\ML\\Clusterers\\Seeders\\KMC2;\n\n$estimator = new MeanShift(2.5, 2000, 1e-6, 0.05, new BallTree(100), new KMC2());\n</code></pre>"},{"location":"clusterers/mean-shift.html#additional-methods","title":"Additional Methods","text":"<p>Estimate the radius of a cluster that encompasses a certain percentage of the total training samples: <pre><code>public static estimateRadius(Dataset $dataset, float $percentile = 30.0, ?Distance $kernel = null) : float\n</code></pre></p> <p>Note</p> <p>Since radius estimation scales quadratically in the number of samples, for large datasets you can speed up the process by running it on a smaller subset of the training data.</p> <p>Return the centroids computed from the training set: <pre><code>public centroids() : array[]\n</code></pre></p> <p>Return an iterable progress table with the steps from the last training session: <pre><code>public steps() : iterable\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('progress.csv', true);\n\n$extractor-&gt;export($estimator-&gt;steps());\n</code></pre> <p>Returns the amount of centroid shift during each epoch of training: <pre><code>public losses() : float[]|null\n</code></pre></p>"},{"location":"clusterers/mean-shift.html#references","title":"References","text":"<ol> <li> <p>M. A. Carreira-Perpinan et al. (2015). A Review of Mean-shift Algorithms for Clustering.\u00a0\u21a9</p> </li> <li> <p>D. Comaniciu et al. (2012). Mean Shift: A Robust Approach Toward Feature Space Analysis.\u00a0\u21a9</p> </li> </ol>"},{"location":"clusterers/seeders/k-mc2.html","title":"K-MC2","text":"<p>[source]</p>"},{"location":"clusterers/seeders/k-mc2.html#k-mc2","title":"K-MC2","text":"<p>A fast Plus Plus approximator that replaces the brute force method with a substantially faster Markov Chain Monte Carlo (MCMC) sampling procedure with comparable results.</p>"},{"location":"clusterers/seeders/k-mc2.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 m 50 int The number of candidate nodes in the Markov Chain. 2 kernel Euclidean Distance The distance kernel used to compute the distance between samples."},{"location":"clusterers/seeders/k-mc2.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Clusterers\\Seeders\\KMC2;\nuse Rubix\\ML\\Kernels\\Distance\\Euclidean;\n\n$seeder = new KMC2(200, new Euclidean());\n</code></pre>"},{"location":"clusterers/seeders/k-mc2.html#_1","title":"K-MC2","text":"<ol> <li> <p>O. Bachem et al. (2016). Approximate K-Means++ in Sublinear Time.\u00a0\u21a9</p> </li> </ol>"},{"location":"clusterers/seeders/plus-plus.html","title":"Plus Plus","text":"<p>[source]</p>"},{"location":"clusterers/seeders/plus-plus.html#plus-plus","title":"Plus Plus","text":"<p>This seeder attempts to maximize the chances of seeding distant clusters while still remaining random. It does so by sequentially selecting random samples weighted by their distance from the previous seed.</p>"},{"location":"clusterers/seeders/plus-plus.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 kernel Euclidean Distance The distance kernel used to compute the distance between samples."},{"location":"clusterers/seeders/plus-plus.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Clusterers\\Seeders\\PlusPlus;\nuse Rubix\\ML\\Kernels\\Distance\\Minkowski;\n\n$seeder = new PlusPlus(new Minkowski(5.0));\n</code></pre>"},{"location":"clusterers/seeders/plus-plus.html#references","title":"References","text":"<ol> <li> <p>D. Arthur et al. (2006). k-means++: The Advantages of Careful Seeding.\u00a0\u21a9</p> </li> <li> <p>A. Stetco et al. (2015). Fuzzy C-means++: Fuzzy C-means with effective seeding initialization.\u00a0\u21a9</p> </li> </ol>"},{"location":"clusterers/seeders/preset.html","title":"Preset","text":"<p>[source]</p>"},{"location":"clusterers/seeders/preset.html#preset","title":"Preset","text":"<p>Generates centroids from a list of presets.</p>"},{"location":"clusterers/seeders/preset.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 centroids array A list of predefined cluster centroids to sample from."},{"location":"clusterers/seeders/preset.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Clusterers\\Seeders\\Preset;\n\n$seeder = new Preset([\n    ['foo', 14, 0.72],\n    ['bar', 16, 0.92],\n]);\n</code></pre>"},{"location":"clusterers/seeders/random.html","title":"Random","text":"<p>[source]</p>"},{"location":"clusterers/seeders/random.html#random","title":"Random","text":"<p>Completely random selection of seeds from a given dataset.</p>"},{"location":"clusterers/seeders/random.html#parameters","title":"Parameters","text":"<p>This seeder does not have any parameters.</p>"},{"location":"clusterers/seeders/random.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Clusterers\\Seeders\\Random;\n\n$seeder = new Random();\n</code></pre>"},{"location":"cross-validation/api.html","title":"Validator","text":"<p>Validators take an instance of a Learner, a Labeled dataset object, and a validation Metric and return a validation score that measures the generalization performance of the model using one of various cross validation techniques.</p> <p>Note</p> <p>There is no need to train the learner beforehand. The validator will automatically train the learner on subsets of the dataset created by the testing algorithm.</p>"},{"location":"cross-validation/api.html#test-a-learner","title":"Test a Learner","text":"<p>To train and test a Learner on a dataset and return the validation score: <pre><code>public test(Learner $estimator, Labeled $dataset, Metric $metric) : float\n</code></pre></p> <pre><code>use Rubix\\ML\\CrossValidation\\KFold;\nuse Rubix\\ML\\CrossValidation\\Metrics\\Accuracy;\n\n$validator = new KFold(10);\n\n$score = $validator-&gt;test($estimator, $dataset, new Accuracy());\n\necho $score;\n</code></pre> <pre><code>0.75\n</code></pre>"},{"location":"cross-validation/hold-out.html","title":"Hold Out","text":"<p>[source]</p>"},{"location":"cross-validation/hold-out.html#hold-out","title":"Hold Out","text":"<p>Hold Out is a quick and simple cross validation technique that uses a validation set that is held out from the training data. The advantages of Hold Out is that the validation score is quick to compute, however it does not allow the learner to both train and test on all the data in the training set.</p> <p>Interfaces: Validator</p>"},{"location":"cross-validation/hold-out.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 ratio 0.2 float The ratio of samples to hold out for testing."},{"location":"cross-validation/hold-out.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\HoldOut;\n\n$validator = new HoldOut(0.3);\n</code></pre>"},{"location":"cross-validation/k-fold.html","title":"K Fold","text":"<p>[source]</p>"},{"location":"cross-validation/k-fold.html#k-fold","title":"K Fold","text":"<p>K Fold is a cross validation technique that splits the training set into k individual folds and for each training round uses 1 of the folds to test the model and the rest as training data. The final score is the average validation score over all of the k rounds. K Fold has the advantage of both training and testing on each sample in the dataset at least once.</p> <p>Interfaces: Validator, Parallel</p>"},{"location":"cross-validation/k-fold.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 k 5 int The number of folds to split the dataset into."},{"location":"cross-validation/k-fold.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\KFold;\n\n$validator = new KFold(5, true);\n</code></pre>"},{"location":"cross-validation/leave-p-out.html","title":"Leave P Out","text":"<p>[source]</p>"},{"location":"cross-validation/leave-p-out.html#leave-p-out","title":"Leave P Out","text":"<p>Leave P Out tests a learner with a unique holdout set of size p for each iteration until all samples have been tested. Although Leave P Out can take long with large datasets and small values of p, it is especially suited for small datasets.</p> <p>Interfaces: Validator, Parallel</p>"},{"location":"cross-validation/leave-p-out.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 p 10 int The number of samples to leave out each round for testing."},{"location":"cross-validation/leave-p-out.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\LeavePOut;\n\n$validator = new LeavePOut(50);\n</code></pre>"},{"location":"cross-validation/monte-carlo.html","title":"Monte Carlo","text":"<p>[source]</p>"},{"location":"cross-validation/monte-carlo.html#monte-carlo","title":"Monte Carlo","text":"<p>Monte Carlo cross validation (or repeated random subsampling) is a technique that averages the validation score of a learner over a user-defined number of simulations where the learner is trained and tested on random splits of the dataset. The estimated validation score approaches the actual validation score as the number of simulations goes to infinity, however, only a tiny fraction of all possible simulations are needed to produce a pretty good approximation.</p> <p>Interfaces: Validator, Parallel</p>"},{"location":"cross-validation/monte-carlo.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 simulations 10 int The number of simulations i.e. random subsamplings of the dataset. 2 ratio 0.2 float The ratio of samples to hold out for testing."},{"location":"cross-validation/monte-carlo.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\MonteCarlo;\n\n$validator = new MonteCarlo(30, 0.1);\n</code></pre>"},{"location":"cross-validation/metrics/accuracy.html","title":"Accuracy","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/accuracy.html#accuracy","title":"Accuracy","text":"<p>A quick and simple classification and anomaly detection metric defined as the number of true positives over the number of samples in the testing set. Since Accuracy gives equal weight to false positives and false negatives, it is not a good metric for datasets with a highly imbalanced distribution of labels.</p> \\[ {\\displaystyle Accuracy = \\frac{TP}{TP + FP}} \\] <p>Estimator Compatibility: Classifier, Anomaly Detector</p> <p>Score Range: 0 to 1</p>"},{"location":"cross-validation/metrics/accuracy.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/accuracy.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\Accuracy;\n\n$metric = new Accuracy();\n</code></pre>"},{"location":"cross-validation/metrics/api.html","title":"Metrics","text":"<p>Validation metrics are for used evaluating the generalization performance of an estimator. They output a score based on the predictions and known ground-truth labels.</p> <p>Note</p> <p>Some regression metrics output the negative of their value to maintain the convention that scores get better as they increase.</p>"},{"location":"cross-validation/metrics/api.html#scoring-predictions","title":"Scoring Predictions","text":"<p>To compute a validation score, pass in the predictions from an estimator along with their expected labels.</p> <pre><code>public score(array $predictions, array $labels) : float\n</code></pre> <pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\FBeta;\n\n$predictions = $estimator-&gt;predict($dataset);\n\n$metric = new FBeta(1.0);\n\n$score = $metric-&gt;score($predictions, $dataset-&gt;labels());\n\necho $score;\n</code></pre> <pre><code>0.88\n</code></pre>"},{"location":"cross-validation/metrics/api.html#scoring-probabilities","title":"Scoring Probabilities","text":"<p>Metrics that implement the ProbabilisticMetric interface calculate a validation score derived from the estimated probabilities of a Probabilistic estimator and their corresponding ground-truth labels.</p> <pre><code>public score(array $probabilities, array $labels) : float\n</code></pre> <p>Note</p> <p>Metric assumes probabilities are values between 0 and 1 and their joint distribution sums to exactly 1 for each sample.</p> <pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\ProbabilisticAccuracy;\n\n$probabilities = $estimator-&gt;proba($dataset);\n\n$metric = new ProbabilisticAccuracy;\n\n$score = $metric-&gt;score($probabilities, $dataset-&gt;labels());\n</code></pre>"},{"location":"cross-validation/metrics/api.html#score-range","title":"Score Range","text":"<p>Output the minimum and maximum value the validation score can take in a 2-tuple.</p> <pre><code>public range() : Rubix\\ML\\Tuple{float, float}\n</code></pre> <pre><code>[$min, $max] = $metric-&gt;range()-&gt;list();\n\necho \"min: $min, max: $max\";\n</code></pre> <pre><code>min: 0.0, max: 1.0\n</code></pre>"},{"location":"cross-validation/metrics/brier-score.html","title":"Brier Score","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/brier-score.html#brier-score","title":"Brier Score","text":"<p>Brier Score is a strictly proper scoring metric that is equivalent to applying mean squared error to the probabilities of a probabilistic estimator.</p> <p>Note</p> <p>In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score.</p> <p>Estimator Compatibility: Probabilistic Classifier</p> <p>Score Range: -2 to 0</p>"},{"location":"cross-validation/metrics/brier-score.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/brier-score.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\BrierScore;\n\n$metric = new BrierScore();\n</code></pre>"},{"location":"cross-validation/metrics/brier-score.html#references","title":"References","text":"<ol> <li> <p>G. W. Brier. (1950). Verification of Forecasts Expresses in Terms of Probability.\u00a0\u21a9</p> </li> </ol>"},{"location":"cross-validation/metrics/completeness.html","title":"Completeness","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/completeness.html#completeness","title":"Completeness","text":"<p>A ground-truth clustering metric that measures the ratio of samples in a class that are also members of the same cluster. A cluster is said to be complete when all the samples in a class are contained in a cluster.</p> \\[ {\\displaystyle Completeness = 1-\\frac{H(K, C)}{H(K)}} \\] <p>Note</p> <p>Since this metric monotonically improves as the number of target clusters decreases, it should not be used as a metric to guide hyper-parameter tuning.</p> <p>Estimator Compatibility: Clusterer</p> <p>Score Range: 0 to 1</p>"},{"location":"cross-validation/metrics/completeness.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/completeness.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\Completeness;\n\n$metric = new Completeness();\n</code></pre>"},{"location":"cross-validation/metrics/f-beta.html","title":"F Beta","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/f-beta.html#f-beta","title":"F-Beta","text":"<p>A weighted harmonic mean of precision and recall, F-Beta is a both a versatile and balanced metric. The beta parameter controls the weight of precision in the combined score. As beta goes to infinity the score only considers recall, whereas when it goes to 0 it only considers precision. When beta is equal to 1, this metric is called an F1 score.</p> \\[ {\\displaystyle F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}}} \\] <p>Estimator Compatibility: Classifier, Anomaly Detector</p> <p>Score Range: 0 to 1</p>"},{"location":"cross-validation/metrics/f-beta.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 beta 1.0 float The ratio of weight given to precision over recall."},{"location":"cross-validation/metrics/f-beta.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\FBeta;\n\n$metric = new FBeta(0.7);\n</code></pre>"},{"location":"cross-validation/metrics/homogeneity.html","title":"Homogeneity","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/homogeneity.html#homogeneity","title":"Homogeneity","text":"<p>A ground-truth clustering metric that measures the ratio of samples in a cluster that are also members of the same class. A cluster is said to be homogeneous when the entire cluster is comprised of a single class of samples.</p> \\[ {\\displaystyle Homogeneity = 1-\\frac{H(C, K)}{H(C)}} \\] <p>Note</p> <p>Since this metric monotonically improves as the number of target clusters increases, it should not be used as a metric to guide hyper-parameter tuning.</p> <p>Estimator Compatibility: Clusterer</p> <p>Score Range: 0 to 1</p>"},{"location":"cross-validation/metrics/homogeneity.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/homogeneity.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\Homogeneity;\n\n$metric = new Homogeneity();\n</code></pre>"},{"location":"cross-validation/metrics/informedness.html","title":"Informedness","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/informedness.html#informedness","title":"Informedness","text":"<p>Informedness a multiclass generalization of Youden's J Statistic and can be interpreted as the probability that an estimator will make an informed prediction. Its value ranges from -1 through 1 and has a value of 0 when the test yields no useful information.</p> \\[ {\\displaystyle Informedness = {\\frac {\\text{TP}}{{\\text{TP}}+{\\text{FN}}}}+{\\frac {\\text{TP}}{{\\text{TN}}+{\\text{FP}}}}-1} \\] <p>Estimator Compatibility: Classifier, Anomaly Detector</p> <p>Score Range: -1 to 1</p>"},{"location":"cross-validation/metrics/informedness.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/informedness.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\Informedness;\n\n$metric = new Informedness();\n</code></pre>"},{"location":"cross-validation/metrics/informedness.html#references","title":"References","text":"<ol> <li> <p>W. J. Youden. (1950). Index for Rating Diagnostic Tests.\u00a0\u21a9</p> </li> </ol>"},{"location":"cross-validation/metrics/mcc.html","title":"MCC","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/mcc.html#mcc","title":"MCC","text":"<p>Matthews Correlation Coefficient (MCC) measures the quality of a classification by taking true and false positives and negatives into account. It is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. A coefficient of 1 represents a perfect prediction, 0 no better than random prediction, and \u22121 indicates total disagreement between prediction and observation.</p> \\[ {\\displaystyle \\mathrm {MCC} = {\\frac {\\mathrm {TP} \\times \\mathrm {TN} -\\mathrm {FP} \\times \\mathrm {FN} }{\\sqrt {(\\mathrm {TP} +\\mathrm {FP} )(\\mathrm {TP} +\\mathrm {FN} )(\\mathrm {TN} +\\mathrm {FP} )(\\mathrm {TN} +\\mathrm {FN} )}}}} \\] <p>Estimator Compatibility: Classifier, Anomaly Detector</p> <p>Score Range: -1 to 1</p>"},{"location":"cross-validation/metrics/mcc.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/mcc.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\MCC;\n\n$metric = new MCC();\n</code></pre>"},{"location":"cross-validation/metrics/mcc.html#references","title":"References","text":"<ol> <li> <p>B. W. Matthews. (1975). Decision of the Predicted and Observed Secondary Structure of T4 Phage Lysozyme.\u00a0\u21a9</p> </li> </ol>"},{"location":"cross-validation/metrics/mean-absolute-error.html","title":"Mean Absolute Error","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/mean-absolute-error.html#mean-absolute-error","title":"Mean Absolute Error","text":"<p>A scale-dependent metric that measures the average absolute error between a set of predictions and their ground-truth labels. One of the nice properties of MAE is that it has the same units of measurement as the labels being estimated.</p> \\[ {\\displaystyle \\mathrm {MAE} = {\\frac {1}{n}}{\\sum _{i=1}^{n}\\left |Y_{i}-\\hat {Y_{i}}\\right|}} \\] <p>Note</p> <p>In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score.</p> <p>Estimator Compatibility: Regressor</p> <p>Score Range: -\u221e to 0</p>"},{"location":"cross-validation/metrics/mean-absolute-error.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/mean-absolute-error.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\MeanAbsoluteError;\n\n$metric = new MeanAbsoluteError();\n</code></pre>"},{"location":"cross-validation/metrics/mean-squared-error.html","title":"Mean Squared Error","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/mean-squared-error.html#mean-squared-error","title":"Mean Squared Error","text":"<p>A scale-dependent regression metric that gives greater weight to error scores the worse they are. Formally, Mean Squared Error (MSE) is the average of the squared differences between a set of predictions and their target labels.</p> \\[ {\\displaystyle \\operatorname {MSE} = {\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}} \\] <p>Note</p> <p>In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score.</p> <p>Estimator Compatibility: Regressor</p> <p>Score Range: -\u221e to 0</p>"},{"location":"cross-validation/metrics/mean-squared-error.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/mean-squared-error.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\MeanSquaredError;\n\n$metric = new MeanSquaredError();\n</code></pre>"},{"location":"cross-validation/metrics/median-absolute-error.html","title":"Median Absolute Error","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/median-absolute-error.html#median-absolute-error","title":"Median Absolute Error","text":"<p>Median Absolute Error (MAD) is a robust measure of error, similar to MAE, that ignores highly erroneous predictions. Since MAD is a robust statistic, it works well even when used to measure non-normal distributions.</p> \\[ {\\displaystyle \\operatorname {MAD} = \\operatorname {median} (|Y_{i}-{\\tilde {Y}}|)} \\] <p>Note</p> <p>In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score.</p> <p>Estimator Compatibility: Regressor</p> <p>Score Range: -\u221e to 0</p>"},{"location":"cross-validation/metrics/median-absolute-error.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/median-absolute-error.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\MedianAbsoluteError;\n\n$metric = new MedianAbsoluteError();\n</code></pre>"},{"location":"cross-validation/metrics/probabilistic-accuracy.html","title":"Probabilistic Accuracy","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/probabilistic-accuracy.html#probabilistic-accuracy","title":"Probabilistic Accuracy","text":"<p>This metric comes from the sports betting domain, where it's used to measure the accuracy of predictions by looking at the probabilities of class predictions. Accordingly, this metric places additional weight on the \"confidence\" of each prediction.</p> <p>Estimator Compatibility: Probabilistic Classifier</p> <p>Score Range: 0 to 1</p>"},{"location":"cross-validation/metrics/probabilistic-accuracy.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/probabilistic-accuracy.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\ProbabilisticAccuracy;\n\n$metric = new ProbabilisticAccuracy();\n</code></pre>"},{"location":"cross-validation/metrics/probabilistic-accuracy.html#references","title":"References","text":"<ol> <li> <p>https://mercurius.io/en/learn/predicting-forecasting-football\u00a0\u21a9</p> </li> </ol>"},{"location":"cross-validation/metrics/r-squared.html","title":"R Squared","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/r-squared.html#r-squared","title":"R Squared","text":"<p>The coefficient of determination or R Squared (R\u00b2) is the proportion of the variance in the target labels that is explainable from the predictions. It gives an indication as to how well the predictions approximate the labels.</p> \\[ {\\displaystyle R^{2} = 1-{SS_{\\rm {res}} \\over SS_{\\rm {tot}}}} \\] <p>Estimator Compatibility: Regressor</p> <p>Score Range: -\u221e to 1</p>"},{"location":"cross-validation/metrics/r-squared.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/r-squared.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\RSquared;\n\n$metric = new RSquared();\n</code></pre>"},{"location":"cross-validation/metrics/rand-index.html","title":"Rand Index","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/rand-index.html#rand-index","title":"Rand Index","text":"<p>The Adjusted Rand Index is a measure of similarity between a clustering and some ground-truth that is adjusted for chance. It considers all pairs of samples that are assigned in the same or different clusters in the predicted and empirical clusterings.</p> \\[ {\\displaystyle ARI = {\\frac {\\left.\\sum _{ij}{\\binom {n_{ij}}{2}}-\\left[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}\\right]\\right/{\\binom {n}{2}}}{\\left.{\\frac {1}{2}}\\left[\\sum _{i}{\\binom {a_{i}}{2}}+\\sum _{j}{\\binom {b_{j}}{2}}\\right]-\\left[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}\\right]\\right/{\\binom {n}{2}}}}} \\] <p>Estimator Compatibility: Clusterer</p> <p>Score Range: -1 to 1</p>"},{"location":"cross-validation/metrics/rand-index.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/rand-index.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\RandIndex;\n\n$metric = new RandIndex();\n</code></pre>"},{"location":"cross-validation/metrics/rand-index.html#references","title":"References","text":"<ol> <li> <p>W. M. Rand. (1971). Objective Criteria for the Evaluation of  Clustering Methods.\u00a0\u21a9</p> </li> </ol>"},{"location":"cross-validation/metrics/rmse.html","title":"RMSE","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/rmse.html#rmse","title":"RMSE","text":"<p>The Root Mean Squared Error (RMSE) is equivalent to the standard deviation of the error residuals in a regression problem. Since RMSE is just the square root of the MSE, RMSE is also sensitive to outliers because larger errors have a disproportionately large effect on the score.</p> \\[ {\\displaystyle \\operatorname {RMSE} = {\\sqrt{ \\frac {1}{n} \\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}}}} \\] <p>Note</p> <p>In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score.</p> <p>Estimator Compatibility: Regressor</p> <p>Score Range: -\u221e to 0</p>"},{"location":"cross-validation/metrics/rmse.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/rmse.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\RMSE;\n\n$metric = new RMSE();\n</code></pre>"},{"location":"cross-validation/metrics/smape.html","title":"SMAPE","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/smape.html#smape","title":"SMAPE","text":"<p>Symmetric Mean Absolute Percentage Error (SMAPE) is a scale-independent regression metric that expresses the relative error of a set of predictions and their labels as a percentage. It is an improvement over the non-symmetric MAPE in that it is both upper and lower bounded.</p> \\[ {\\displaystyle {\\text{SMAPE}} = {\\frac {100\\%}{n}}\\sum _{t=1}^{n}{\\frac {\\left|F_{t}-A_{t}\\right|}{(|A_{t}|+|F_{t}|)/2}}} \\] <p>Note</p> <p>In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score.</p> <p>Estimator Compatibility: Regressor</p> <p>Score Range: -100 to 0</p>"},{"location":"cross-validation/metrics/smape.html#parameters","title":"Parameters","text":"<p>This metric does not have any parameters.</p>"},{"location":"cross-validation/metrics/smape.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\SMAPE;\n\n$metric = new SMAPE();\n</code></pre>"},{"location":"cross-validation/metrics/smape.html#references","title":"References","text":"<ol> <li> <p>V. Kreinovich. et al. (2014). How to Estimate Forecasting Quality: A System Motivated Derivation of Symmetric Mean Absolute Percentage Error (SMAPE) and Other Similar Characteristics.\u00a0\u21a9</p> </li> </ol>"},{"location":"cross-validation/metrics/top-k-accuracy.html","title":"Top K Accuracy","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/top-k-accuracy.html#top-k-accuracy","title":"Top K Accuracy","text":"<p>Top K Accuracy looks at the k classes with the highest predicted probabilities when calculating the accuracy score. If one of the top k classes matches the ground-truth, then the prediction is considered accurate.</p> <p>Estimator Compatibility: Probabilistic Classifier</p> <p>Score Range: 0 to 1</p>"},{"location":"cross-validation/metrics/top-k-accuracy.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 k 3 int The number of classes with the highest predicted probability to consider."},{"location":"cross-validation/metrics/top-k-accuracy.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\TopKAccuracy;\n\n$metric = new TopKAccuracy(5);\n</code></pre>"},{"location":"cross-validation/metrics/v-measure.html","title":"V Measure","text":"<p>[source]</p>"},{"location":"cross-validation/metrics/v-measure.html#v-measure","title":"V Measure","text":"<p>V Measure is an entropy-based clustering metric that balances Homogeneity and Completeness. It has the additional property of being symmetric in that the predictions and ground-truth can be swapped without changing the score.</p> \\[ {\\displaystyle V_{\\beta} = \\frac{(1+\\beta)hc}{\\beta h + c}} \\] <p>Estimator Compatibility: Clusterer</p> <p>Score Range: 0 to 1</p>"},{"location":"cross-validation/metrics/v-measure.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 beta 1.0 float The ratio of weight given to homogeneity over completeness."},{"location":"cross-validation/metrics/v-measure.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Metrics\\VMeasure;\n\n$metric = new VMeasure(1.0);\n</code></pre>"},{"location":"cross-validation/metrics/v-measure.html#references","title":"References","text":"<ol> <li> <p>A. Rosenberg et al. (2007). V-Measure: A conditional entropy-based external cluster evaluation measure.\u00a0\u21a9</p> </li> </ol>"},{"location":"cross-validation/reports/aggregate-report.html","title":"Aggregate Report","text":"<p>[source]</p>"},{"location":"cross-validation/reports/aggregate-report.html#aggregate-report","title":"Aggregate Report","text":"<p>A report generator that aggregates the output of multiple reports.</p> <p>Estimator Compatibility: Depends on base reports</p>"},{"location":"cross-validation/reports/aggregate-report.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 reports array An array of report generators to aggregate keyed by a user-specified name."},{"location":"cross-validation/reports/aggregate-report.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Reports\\AggregateReport;\nuse Rubix\\ML\\CrossValidation\\Reports\\ConfusionMatrix;\nuse Rubix\\ML\\CrossValidation\\Reports\\MulticlassBreakdown;\n\n$report = new AggregateReport([\n    'breakdown' =&gt; new MulticlassBreakdown(),\n    'matrix' =&gt; new ConfusionMatrix(),\n]);\n</code></pre>"},{"location":"cross-validation/reports/api.html","title":"Report Generators","text":"<p>Report generators output detailed reports from a validation set and a set of predictions. They are used in cross-validation to ascertain the generalization performance of an estimator.</p>"},{"location":"cross-validation/reports/api.html#generate-a-report","title":"Generate a Report","text":"<p>To generate a report from the predictions of an estimator given the ground truth labels: <pre><code>public generate(array $predictions, array $labels) : Report\n</code></pre></p> <pre><code>use Rubix\\ML\\Reports\\ConfusionMatrix;\n\n$predictions = $estimator-&gt;predict($dataset);\n\n$report = new ConfusionMatrix();\n\n$results = $report-&gt;generate($predictions, $dataset-&gt;labels());\n</code></pre>"},{"location":"cross-validation/reports/api.html#report-objects","title":"Report Objects","text":"<p>The results of a report will be returned in a Report object whose attributes can be accessed like an associative array. In addition, report objects can be echoed to the terminal or even written to a file.</p>"},{"location":"cross-validation/reports/api.html#printing-the-report","title":"Printing the Report","text":"<p>To display the human-readable form of the report, you can <code>echo</code> it out to the terminal.</p> <pre><code>echo $results;\n</code></pre> <pre><code>{\n    \"dog\": {\n        \"dog\": 12,\n        \"cat\": 3,\n        \"turtle\": 0\n    },\n    \"cat\": {\n        \"dog\": 2,\n        \"cat\": 9,\n        \"turtle\": 1\n    },\n    \"turtle\": {\n        \"dog\": 1,\n        \"cat\": 0,\n        \"turtle\": 11\n    }\n}\n</code></pre>"},{"location":"cross-validation/reports/api.html#accessing-report-attributes","title":"Accessing Report Attributes","text":"<p>You can access individual report attributes by treating the report object as an associative array.</p> <pre><code>$accuracy = $results['accuracy'];\n</code></pre>"},{"location":"cross-validation/reports/api.html#encoding-the-report","title":"Encoding the Report","text":"<p>To return a JSON encoding that can be written to a file, call the <code>toJSON()</code> method on the report object. <pre><code>public toJSON(bool $pretty = true) : Encoding\n</code></pre></p> <pre><code>$encoding = $report-&gt;toJSON();\n</code></pre>"},{"location":"cross-validation/reports/confusion-matrix.html","title":"Confusion Matrix","text":"<p>[source]</p>"},{"location":"cross-validation/reports/confusion-matrix.html#confusion-matrix","title":"Confusion Matrix","text":"<p>A Confusion Matrix is a square matrix (table) that visualizes the true positives, false positives, true negatives, and false negatives of a set of predictions and their corresponding labels.</p> <p>Estimator Compatibility: Classifier, Anomaly Detector</p>"},{"location":"cross-validation/reports/confusion-matrix.html#parameters","title":"Parameters","text":"<p>This report does not have any parameters.</p>"},{"location":"cross-validation/reports/confusion-matrix.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Reports\\ConfusionMatrix;\n\n$report = new ConfusionMatrix();\n\n$result = $report-&gt;generate($predictions, $labels);\n\necho $result;\n</code></pre> <pre><code>{\n    \"dog\": {\n        \"dog\": 12,\n        \"cat\": 3,\n        \"turtle\": 0\n    },\n    \"cat\": {\n        \"dog\": 2,\n        \"cat\": 9,\n        \"turtle\": 1\n    },\n    \"turtle\": {\n        \"dog\": 1,\n        \"cat\": 0,\n        \"turtle\": 11\n    }\n}\n</code></pre>"},{"location":"cross-validation/reports/contingency-table.html","title":"Contingency Table","text":"<p>[source]</p>"},{"location":"cross-validation/reports/contingency-table.html#contingency-table","title":"Contingency Table","text":"<p>A Contingency Table is used to display the frequency distribution of class labels among a clustering. It is similar to a Confusion Matrix but uses the labels to establish ground-truth for a clustering problem instead.</p> <p>Estimator Compatibility: Clusterer</p>"},{"location":"cross-validation/reports/contingency-table.html#parameters","title":"Parameters","text":"<p>This report does not have any parameters.</p>"},{"location":"cross-validation/reports/contingency-table.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Reports\\ContingencyTable;\n\n$report = new ContingencyTable();\n\n$result = $report-&gt;generate($predictions, $labels);\n\necho $result;\n</code></pre> <pre><code>[\n    {\n        \"lamb\": 11,\n        \"wolf\": 2\n    },\n    {\n        \"lamb\": 1,\n        \"wolf\": 5\n    }\n]\n</code></pre>"},{"location":"cross-validation/reports/error-analysis.html","title":"Error Analysis","text":"<p>[source]</p>"},{"location":"cross-validation/reports/error-analysis.html#error-analysis","title":"Error Analysis","text":"<p>The Error Analysis report measures the differences between the predicted and target values of a regression problem using multiple error measurements (MAE, MSE, RMSE, MAPE, etc.) as well as statistics regarding the distribution of errors.</p> <p>Estimator Compatibility: Regressor</p>"},{"location":"cross-validation/reports/error-analysis.html#parameters","title":"Parameters","text":"<p>This report does not have any parameters.</p>"},{"location":"cross-validation/reports/error-analysis.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Reports\\ErrorAnalysis;\n\n$report = new ErrorAnalysis();\n\n$results = $report-&gt;generate($predictions, $labels);\n\necho $results;\n</code></pre> <pre><code>{\n    \"mean absolute error\": 0.8,\n    \"median absolute error\": 1,\n    \"mean squared error\": 1,\n    \"mean absolute percentage error\": 14.02077497665733,\n    \"rms error\": 1,\n    \"mean squared log error\": 0.019107097505647368,\n    \"r squared\": 0.9958930551562692,\n    \"error mean\": -0.2,\n    \"error standard deviation\": 0.9898464007663,\n    \"error skewness\": -0.22963966338592326,\n    \"error kurtosis\": -1.0520833333333324,\n    \"error min\": -2,\n    \"error 25%\": -1.0,\n    \"error median\": 0.0,\n    \"error 75%\": 0.75,\n    \"error max\": 1,\n    \"cardinality\": 10\n}\n</code></pre>"},{"location":"cross-validation/reports/multiclass-breakdown.html","title":"Multiclass Breakdown","text":"<p>[source]</p>"},{"location":"cross-validation/reports/multiclass-breakdown.html#multi-class-breakdown","title":"Multi-class Breakdown","text":"<p>A multiclass classification report that computes a number of metrics (Accuracy, Precision, Recall, etc.) derived from their confusion matrix on an overall and individual class basis.</p> <p>Estimator Compatibility: Classifier, Anomaly Detector</p>"},{"location":"cross-validation/reports/multiclass-breakdown.html#parameters","title":"Parameters","text":"<p>This report does not have any parameters.</p>"},{"location":"cross-validation/reports/multiclass-breakdown.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\CrossValidation\\Reports\\MulticlassBreakdown;\n\n$report = new MulticlassBreakdown();\n\n$results = $report-&gt;generate($predictions, $labels);\n\necho $results;\n</code></pre> <pre><code>{\n    \"overall\": {\n        \"accuracy\": 0.6,\n        \"accuracy balanced\": 0.5833333333333333,\n        \"f1 score\": 0.5833333333333333,\n        \"precision\": 0.5833333333333333,\n        \"recall\": 0.5833333333333333,\n        \"specificity\": 0.5833333333333333,\n        \"negative predictive value\": 0.5833333333333333,\n        \"false discovery rate\": 0.4166666666666667,\n        \"miss rate\": 0.4166666666666667,\n        \"fall out\": 0.4166666666666667,\n        \"false omission rate\": 0.4166666666666667,\n        \"mcc\": 0.16666666666666666,\n        \"informedness\": 0.16666666666666652,\n        \"markedness\": 0.16666666666666652,\n        \"true positives\": 3,\n        \"true negatives\": 3,\n        \"false positives\": 2,\n        \"false negatives\": 2,\n        \"cardinality\": 5\n    },\n    \"classes\": {\n        \"wolf\": {\n            \"accuracy\": 0.6,\n            \"accuracy balanced\": 0.5833333333333333,\n            \"f1 score\": 0.6666666666666666,\n            \"precision\": 0.6666666666666666,\n            \"recall\": 0.6666666666666666,\n            \"specificity\": 0.5,\n            \"negative predictive value\": 0.5,\n            \"false discovery rate\": 0.33333333333333337,\n            \"miss rate\": 0.33333333333333337,\n            \"fall out\": 0.5,\n            \"false omission rate\": 0.5,\n            \"informedness\": 0.16666666666666652,\n            \"markedness\": 0.16666666666666652,\n            \"mcc\": 0.16666666666666666,\n            \"true positives\": 2,\n            \"true negatives\": 1,\n            \"false positives\": 1,\n            \"false negatives\": 1,\n            \"cardinality\": 3,\n            \"proportion\": 0.6\n        },\n        \"lamb\": {\n            \"accuracy\": 0.6,\n            \"accuracy balanced\": 0.5833333333333333,\n            \"f1 score\": 0.5,\n            \"precision\": 0.5,\n            \"recall\": 0.5,\n            \"specificity\": 0.6666666666666666,\n            \"negative predictive value\": 0.6666666666666666,\n            \"false discovery rate\": 0.5,\n            \"miss rate\": 0.5,\n            \"fall out\": 0.33333333333333337,\n            \"false omission rate\": 0.33333333333333337,\n            \"informedness\": 0.16666666666666652,\n            \"markedness\": 0.16666666666666652,\n            \"mcc\": 0.16666666666666666,\n            \"true positives\": 1,\n            \"true negatives\": 2,\n            \"false positives\": 1,\n            \"false negatives\": 1,\n            \"cardinality\": 2,\n            \"proportion\": 0.4\n        }\n    }\n}\n</code></pre>"},{"location":"datasets/api.html","title":"Dataset Objects","text":"<p>Data are passed in specialized in-memory containers called Dataset objects. Dataset objects are table-like data structures that have operations for data manipulation. They can hold a heterogeneous mix of data types and they make it easy to transport data in a canonical way. Datasets consist of a matrix of samples in which each row constitutes a sample and each column represents the value of the feature represented by that column. They have the additional constraint that each feature column must contain values of the same high-level data type. Some datasets can contain labels for training or cross validation. In the example below, we instantiate a new Labeled dataset object by passing the samples and their labels as arguments to the constructor.</p> <pre><code>use Rubix\\ML\\Datasets\\Labeled;\n\n$samples = [\n    [0.1, 20, 'furry'],\n    [2.0, -5, 'rough'],\n];\n\n$labels = ['not monster', 'monster'];\n\n$dataset = new Labeled($samples, $labels);\n</code></pre>"},{"location":"datasets/api.html#factory-methods","title":"Factory Methods","text":"<p>Build a dataset with the records of a 2-dimensional iterable data table: <pre><code>public static fromIterator(Traversable $iterator) : self\n</code></pre></p> <p>Note</p> <p>When building a Labeled dataset, the label values should be in the last column of the data table.</p> <pre><code>use Rubix\\ML\\Datasets\\Labeled;\nuse Rubix\\ML\\Datasets\\Extractors\\CSV;\n\n$dataset = Labeled::fromIterator(new CSV('example.csv'));\n</code></pre>"},{"location":"datasets/api.html#properties","title":"Properties","text":"<p>Return the number of rows in the dataset: <pre><code>public numSamples() : int\n</code></pre></p> <p>Return the number of columns in the samples matrix: <pre><code>public numFeatures() : int\n</code></pre></p> <p>Return a 2-tuple with the shape of the samples matrix: <pre><code>public shape() : array{int, int}\n</code></pre></p> <pre><code>[$m, $n] = $dataset-&gt;shape();\n\necho \"$m x $n\";\n</code></pre> <pre><code>1000 x 30\n</code></pre>"},{"location":"datasets/api.html#data-types","title":"Data Types","text":"<p>Return the data types for each column in the data table: <pre><code>public types() : Rubix\\ML\\DataType[]\n</code></pre></p> <p>Return the data types for each feature column: <pre><code>public featureTypes() : Rubix\\ML\\DataType[]\n</code></pre></p> <p>Return the data type for a given column offset: <pre><code>public featureType(int $offset) : Rubix\\ML\\DataType\n</code></pre></p> <pre><code>echo $dataset-&gt;featureType(15);\n</code></pre> <pre><code>categorical\n</code></pre>"},{"location":"datasets/api.html#selecting","title":"Selecting","text":"<p>Return all the samples in the dataset in a 2-dimensional array: <pre><code>public samples() : array[]\n</code></pre></p> <p>Select a single row containing the sample at a given offset beginning at 0: <pre><code>public sample(int $offset) : mixed[]\n</code></pre></p> <p>Return the columns of the sample matrix: <pre><code>public features() : array[]\n</code></pre></p> <p>Select the values of a feature column at a given offset : <pre><code>public feature(int $offset) : mixed[]\n</code></pre></p>"},{"location":"datasets/api.html#dropping","title":"Dropping","text":"<p>Drop a feature at a given column offset from the dataset: <pre><code>public dropFeature(int $offset) : self\n</code></pre></p>"},{"location":"datasets/api.html#head-and-tail","title":"Head and Tail","text":"<p>Return the first n rows of data in a new dataset object: <pre><code>public head(int $n = 10) : self\n</code></pre></p> <pre><code>$subset = $dataset-&gt;head(10);\n</code></pre> <p>Return the last n rows of data in a new dataset object: <pre><code>public tail(int $n = 10) : self\n</code></pre></p>"},{"location":"datasets/api.html#taking-and-leaving","title":"Taking and Leaving","text":"<p>Remove n rows from the dataset and return them in a new dataset: <pre><code>public take(int $n = 1) : self\n</code></pre></p> <p>Leave n samples on the dataset and return the rest in a new dataset: <pre><code>public leave(int $n = 1) : self\n</code></pre></p>"},{"location":"datasets/api.html#splitting","title":"Splitting","text":"<p>Split the dataset into left and right subsets: <pre><code>public split(float $ratio = 0.5) : array{self, self}\n</code></pre></p> <pre><code>[$training, $testing] = $dataset-&gt;split(0.8);\n</code></pre>"},{"location":"datasets/api.html#folding","title":"Folding","text":"<p>Fold the dataset to form k equal size datasets: <pre><code>public fold(int $k = 10) : self[]\n</code></pre></p> <p>Note</p> <p>If there are not enough samples to completely fill the last fold of the dataset then it will contain slightly fewer samples than the rest of the folds.</p> <pre><code>$folds = $dataset-&gt;fold(8);\n</code></pre>"},{"location":"datasets/api.html#slicing-and-splicing","title":"Slicing and Splicing","text":"<p>Return an n size portion of the dataset in a new dataset: <pre><code>public slice(int $offset, int $n) : self\n</code></pre></p> <p>Remove a size n chunk of the dataset starting at offset and return it in a new dataset: <pre><code>public splice(int $offset, int $n) : self\n</code></pre></p>"},{"location":"datasets/api.html#batching","title":"Batching","text":"<p>Batch the dataset into subsets containing a maximum of n rows per batch: <pre><code>public batch(int $n = 50) : self[]\n</code></pre></p> <pre><code>$batches = $dataset-&gt;batch(250);\n</code></pre>"},{"location":"datasets/api.html#randomization","title":"Randomization","text":"<p>Randomize the order of the dataset and return it for method chaining: <pre><code>public randomize() : self\n</code></pre></p> <p>Generate a random subset of the dataset without replacement of size n: <pre><code>public randomSubset(int $n) : self\n</code></pre></p> <pre><code>$subset = $dataset-&gt;randomSubset(50);\n</code></pre> <p>Generate a random subset with replacement: <pre><code>public randomSubsetWithReplacement(int $n) : self\n</code></pre></p> <pre><code>$subset = $dataset-&gt;randomSubsetWithReplacement(500);\n</code></pre> <p>Generate a random weighted subset with replacement of size n: <pre><code>public randomWeightedSubsetWithReplacement(int $n, array $weights) : self\n</code></pre></p> <pre><code>$subset = $dataset-&gt;randomWeightedSubsetWithReplacement(200, $weights);\n</code></pre>"},{"location":"datasets/api.html#applying-transformations","title":"Applying Transformations","text":"<p>You can apply a Transformer to the samples in a Dataset object by passing it as an argument to the <code>apply()</code> method on the dataset object. If a Stateful transformer has not been fitted beforehand, it will automatically be fitted before being applied to the samples. <pre><code>public apply(Transformer $transformer) : self\n</code></pre></p> <pre><code>use Rubix\\ML\\Transformers\\RobustStandardizer;\n\n$dataset-&gt;apply(new RobustStandardizer);\n</code></pre> <p>To reverse the transformation, pass a Reversible transformer to the dataset objects <code>reverseApply()</code> method. <pre><code>public apply(Reversible $transformer) : self\n</code></pre></p> <pre><code>use Rubix\\ML\\Transformers\\MaxAbsoluteScaler;\n\n$transformer = new MaxAbsoluteScaler();\n\n$dataset-&gt;apply($transformer);\n\n// Do something\n\n$dataset-&gt;reverseApply($transformer);\n</code></pre>"},{"location":"datasets/api.html#filtering","title":"Filtering","text":"<p>Filter the records of the dataset using a callback function to determine if a row should be included in the return dataset: <pre><code>public filter(callable $callback) : self\n</code></pre></p> <pre><code>$tallPeople = function ($record) {\n    return $record[3] &gt; 178.5;\n};\n\n$dataset = $dataset-&gt;filter($tallPeople);\n</code></pre>"},{"location":"datasets/api.html#stacking","title":"Stacking","text":"<p>Stack any number of dataset objects on top of each other to form a single dataset: <pre><code>public static stack(array $datasets) : self\n</code></pre></p> <p>Note</p> <p>Datasets must have the same number of feature columns i.e. dimensionality.</p> <pre><code>use Rubix\\ML\\Datasets\\Labeled;\n\n$dataset = Labeled::stack([\n    $dataset1,\n    $dataset2,\n    $dataset3,\n    // ...\n]);\n</code></pre>"},{"location":"datasets/api.html#merging-and-joining","title":"Merging and Joining","text":"<p>To merge the rows of this dataset with another dataset: <pre><code>public merge(Dataset $dataset) : self\n</code></pre></p> <p>Note</p> <p>Datasets must have the same number of columns.</p> <pre><code>$dataset = $dataset1-&gt;merge($dataset2);\n</code></pre> <p>To join the columns of this dataset with another dataset: <pre><code>public join(Dataset $dataset) : self\n</code></pre></p> <p>Note</p> <p>Datasets must have the same number of rows.</p> <pre><code>$dataset = $dataset1-&gt;join($dataset2);\n</code></pre>"},{"location":"datasets/api.html#descriptive-statistics","title":"Descriptive Statistics","text":"<p>Return an array of statistics such as the central tendency, dispersion and shape of each continuous feature column and the joint probabilities of each category for every categorical feature column: <pre><code>public describe() : Rubix\\ML\\Report\n</code></pre></p> <pre><code>echo $dataset-&gt;describe();\n</code></pre> <pre><code>[\n    {\n        \"offset\": 0,\n        \"type\": \"categorical\",\n        \"num categories\": 2,\n        \"probabilities\": {\n            \"friendly\": 0.6666666666666666,\n            \"loner\": 0.3333333333333333\n        }\n    },\n    {\n        \"offset\": 1,\n        \"type\": \"continuous\",\n        \"mean\": 0.3333333333333333,\n        \"standard deviation\": 3.129252661934191,\n        \"skewness\": -0.4481030843690633,\n        \"kurtosis\": -1.1330702741786107,\n        \"min\": -5,\n        \"25%\": -1.375,\n        \"median\": 0.8,\n        \"75%\": 2.825,\n        \"max\": 4\n    }\n]\n</code></pre>"},{"location":"datasets/api.html#sorting","title":"Sorting","text":"<p>Sort the records in the dataset using a callback for comparisons between samples. The callback function accepts two records to be compared and should return <code>true</code> if the records should be swapped. <pre><code>public function sort(callable $callback) : self\n</code></pre></p> <pre><code>$sorted = $dataset-&gt;sort(function ($recordA, $recordB) {\n    return $recordA[2] &gt; $recordB[2];\n});\n</code></pre>"},{"location":"datasets/api.html#de-duplication","title":"De-duplication","text":"<p>Remove duplicate rows from the dataset: <pre><code>public deduplicate() : self\n</code></pre></p>"},{"location":"datasets/api.html#exporting","title":"Exporting","text":"<p>Export the dataset to the location and format given by a Writable extractor: <pre><code>public exportTo(Writable $extractor) : void\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\NDJSON;\n\n$dataset-&gt;exportTo(new NDJSON('example.ndjson'));\n</code></pre>"},{"location":"datasets/labeled.html","title":"Labeled","text":"<p>[source]</p>"},{"location":"datasets/labeled.html#labeled","title":"Labeled","text":"<p>A Labeled dataset is used to train supervised learners and for testing a model by providing the ground-truth. In addition to the standard dataset API, a labeled dataset can perform operations such as stratification and sorting the dataset using the label column.</p> <p>Note</p> <p>Since PHP silently converts integer strings (ex. <code>'1'</code>) to integers in some circumstances, you should not use integer strings as class labels. Instead, use an appropriate non-integer string class name such as <code>'class 1'</code>, <code>'#1'</code>, or <code>'first'</code>.</p>"},{"location":"datasets/labeled.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 samples array A 2-dimensional array consisting of rows of samples and columns with feature values. 2 labels array A 1-dimensional array of labels that correspond to each sample in the dataset. 2 verify true bool Should we verify the data?"},{"location":"datasets/labeled.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Datasets\\Labeled;\n\n$samples = [\n    [0.1, 20, 'furry'],\n    [2.0, -5, 'rough'],\n    [0.01, 5, 'furry'],\n];\n\n$labels = ['not monster', 'monster', 'not monster'];\n\n$dataset = new Labeled($samples, $labels);\n</code></pre>"},{"location":"datasets/labeled.html#additional-methods","title":"Additional Methods","text":""},{"location":"datasets/labeled.html#selectors","title":"Selectors","text":"<p>Return the labels of the dataset in an array: <pre><code>public labels() : array\n</code></pre></p> <p>Return a single label at the given row offset: <pre><code>public label(int $offset) : mixed\n</code></pre></p> <p>Return all of the possible outcomes i.e. the unique labels in an array: <pre><code>public possibleOutcomes() : array\n</code></pre></p> <pre><code>print_r($dataset-&gt;possibleOutcomes());\n</code></pre> <pre><code>Array\n(\n    [0] =&gt; female\n    [1] =&gt; male\n)\n</code></pre>"},{"location":"datasets/labeled.html#data-types","title":"Data Types","text":"<p>Return the data type of the label: <pre><code>public labelType() : Rubix\\ML\\DataType\n</code></pre></p> <pre><code>echo $dataset-&gt;labelType();\n</code></pre> <pre><code>continuous\n</code></pre>"},{"location":"datasets/labeled.html#stratification","title":"Stratification","text":"<p>Group samples by their class label and return them in their own dataset: <pre><code>public stratifyByLabel() : array\n</code></pre></p> <pre><code>$strata = $dataset-&gt;stratifyByLabel();\n</code></pre> <p>Split the dataset into left and right subsets such that the proportions of class labels remain intact: <pre><code>public stratifiedSplit($ratio = 0.5) : array\n</code></pre></p> <pre><code>[$training, $testing] = $dataset-&gt;stratifiedSplit(0.8);\n</code></pre> <p>Return k equal size subsets of the dataset such that class proportions remain intact: <pre><code>public stratifiedFold($k = 10) : array\n</code></pre></p> <pre><code>$folds = $dataset-&gt;stratifiedFold(3);\n</code></pre>"},{"location":"datasets/labeled.html#transform-labels","title":"Transform Labels","text":"<p>Transform the labels in the dataset using a callback function and return self for method chaining: <pre><code>public transformLabels(callable $fn) : self\n</code></pre></p> <p>Note</p> <p>The callback function called for each individual label and should return the transformed label as a continuous or categorical value.</p> <pre><code>$dataset-&gt;transformLabels('intval');\n\n//\n\n$dataset-&gt;transformLabels(function ($label) {\n    return $label &gt; 0.5 ? 'yes' : 'no';\n});\n</code></pre>"},{"location":"datasets/labeled.html#describe-by-label","title":"Describe by Label","text":"<p>Describe the features of the dataset broken down by categorical label: <pre><code>public describeByLabel() : Report\n</code></pre></p> <pre><code>echo $dataset-&gt;describeByLabel();\n</code></pre> <pre><code>{\n    \"not monster\": [\n        {\n            \"type\": \"categorical\",\n            \"num categories\": 2,\n            \"probabilities\": {\n                \"friendly\": 0.75,\n                \"loner\": 0.25\n            }\n        },\n        {\n            \"type\": \"continuous\",\n            \"mean\": 1.125,\n            \"variance\": 12.776875,\n            \"standard deviation\": 3.574475485997911,\n            \"skewness\": -1.0795676577113944,\n            \"kurtosis\": -0.7175867765792474,\n            \"min\": -5,\n            \"25%\": 0.6999999999999993,\n            \"median\": 2.75,\n            \"75%\": 3.175,\n            \"max\": 4\n        }\n    ],\n    \"monster\": [\n        {\n            \"type\": \"categorical\",\n            \"num categories\": 2,\n            \"probabilities\": {\n                \"loner\": 0.5,\n                \"friendly\": 0.5\n            }\n        },\n        {\n            \"type\": \"continuous\",\n            \"mean\": -1.25,\n            \"standard deviation\": 0.25,\n            \"skewness\": 0,\n            \"kurtosis\": -2,\n            \"min\": -1.5,\n            \"25%\": -1.375,\n            \"median\": -1.25,\n            \"75%\": -1.125,\n            \"max\": -1\n        }\n    ]\n}\n</code></pre>"},{"location":"datasets/unlabeled.html","title":"Unlabeled","text":"<p>[source]</p>"},{"location":"datasets/unlabeled.html#unlabeled","title":"Unlabeled","text":"<p>Unlabeled datasets are used to train unsupervised learners and for feeding unknown samples into an estimator to make predictions. As their name implies, they do not require a corresponding label for each sample.</p>"},{"location":"datasets/unlabeled.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 samples array A 2-dimensional array consisting of rows of samples and columns with feature values. 2 verify true bool Should we verify the data?"},{"location":"datasets/unlabeled.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Datasets\\Unlabeled;\n\n$samples = [\n    [0.1, 20, 'furry'],\n    [2.0, -5, 'rough'],\n    [0.001, -10, 'rough'],\n];\n\n$dataset = new Unlabeled($samples);\n</code></pre>"},{"location":"datasets/unlabeled.html#additional-methods","title":"Additional Methods","text":"<p>This dataset does not have any additional methods.</p>"},{"location":"datasets/generators/agglomerate.html","title":"Agglomerate","text":"<p>[source]</p>"},{"location":"datasets/generators/agglomerate.html#agglomerate","title":"Agglomerate","text":"<p>An Agglomerate is a collection of generators with each of them given a user-defined label. Agglomerates are useful for classification, clustering, and anomaly detection problems where the target label is a discrete value.</p> <p>Data Types: Depends on base generators</p> <p>Label Type: Categorical</p>"},{"location":"datasets/generators/agglomerate.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 generators array A collection of generators indexed by their given label. 2 weights Auto array A set of arbitrary weight values corresponding to a generator's proportion of the overall agglomeration. If no weights are given, each generator is assigned equal weight."},{"location":"datasets/generators/agglomerate.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Datasets\\Generators\\Agglomerate;\nuse Rubix\\ML\\Datasets\\Generators\\Blob;\nuse Rubix\\ML\\Datasets\\Generators\\HalfMoon;\nuse Rubix\\ML\\Datasets\\Generators\\Circle;\n\n$generator = new Agglomerate([\n    'foo' =&gt; new Blob([5, 2], 1.0),\n    'bar' =&gt; new HalfMoon(-3, 5, 1.5, 90.0, 0.1),\n    'baz' =&gt; new Circle(2, -4, 2.0, 0.05),\n], [\n    3.5, 4.0, 5.0,\n]);\n</code></pre>"},{"location":"datasets/generators/agglomerate.html#additional-methods","title":"Additional Methods","text":"<p>Return the normalized weight values of each generator in the agglomerate: <pre><code>public weights() : array\n</code></pre></p>"},{"location":"datasets/generators/api.html","title":"Generators","text":"<p>Dataset generators produce synthetic datasets of a user-specified shape and dimensionality. Synthetic data is useful for a number of tasks including experimentation, testing, benchmarking, and demonstration purposes.</p>"},{"location":"datasets/generators/api.html#generate-a-dataset","title":"Generate a Dataset","text":"<p>To generate a Dataset object with n records: <pre><code>public generate(int $n) : Dataset\n</code></pre></p> <pre><code>use Rubix\\ML\\Datasets\\Generators\\HalfMoon;\n\n$generator = new HalfMoon(0.0, 0.0);\n\n$dataset = $generator-&gt;generate(1000);\n</code></pre>"},{"location":"datasets/generators/blob.html","title":"Blob","text":"<p>[source]</p>"},{"location":"datasets/generators/blob.html#blob","title":"Blob","text":"<p>A normally distributed (Gaussian) n-dimensional blob of samples centered at a given vector. The standard deviation can be set for the whole blob or for each feature column independently. When a global standard deviation is used, the resulting blob will be isotropic and will converge asymptotically to a sphere.</p> <p>Data Types: Continuous</p> <p>Label Type: Unlabeled</p>"},{"location":"datasets/generators/blob.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 center [0, 0] array An array containing the coordinates of the center of the blob. 2 stddev 1.0 float or array Either the global standard deviation or an array with the standard deviation on a per feature column basis."},{"location":"datasets/generators/blob.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Datasets\\Generators\\Blob;\n\n$generator = new Blob([-1.2, -5., 2.6, 0.8, 10.], 0.25);\n</code></pre>"},{"location":"datasets/generators/blob.html#additional-methods","title":"Additional Methods","text":"<p>This generator does not have any additional methods.</p>"},{"location":"datasets/generators/circle.html","title":"Circle","text":"<p>[source]</p>"},{"location":"datasets/generators/circle.html#circle","title":"Circle","text":"<p>Creates a dataset of points forming a circle in 2 dimensions. The label of each sample is the random value used to generate the projection measured in degrees.</p> <p>Data Types: Continuous</p> <p>Label Type: Continuous</p>"},{"location":"datasets/generators/circle.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 x 0.0 float The x coordinate of the center of the circle. 2 y 0.0 float The y coordinate of the center of the circle. 3 scale 1.0 float The scaling factor of the circle. 4 noise 0.1 float The amount of Gaussian noise to add to each data point as a ratio of the scaling factor."},{"location":"datasets/generators/circle.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Datasets\\Generators\\Circle;\n\n$generator = new Circle(0.0, 0.0, 100, 0.1);\n</code></pre>"},{"location":"datasets/generators/circle.html#additional-methods","title":"Additional Methods","text":"<p>This generator does not have any additional methods.</p>"},{"location":"datasets/generators/half-moon.html","title":"Half Moon","text":"<p>[source]</p>"},{"location":"datasets/generators/half-moon.html#half-moon","title":"Half Moon","text":"<p>Generates a dataset consisting of 2-d samples that form the shape of a half moon when plotted on a scatter plot chart.</p> <p>Data Types: Continuous</p> <p>Label Type: Continuous</p>"},{"location":"datasets/generators/half-moon.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 x 0.0 float The x coordinate of the center of the half moon. 2 y 0.0 float The y coordinate of the center of the half moon. 3 scale 1.0 float The scaling factor of the half moon. 4 rotate 90.0 float The amount in degrees to rotate the half moon counterclockwise. 5 noise 0.1 float The amount of Gaussian noise to add to each data point as a percentage of the scaling factor."},{"location":"datasets/generators/half-moon.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Datasets\\Generators\\HalfMoon;\n\n$generator = new HalfMoon(4.0, 0.0, 6, 180.0, 0.2);\n</code></pre>"},{"location":"datasets/generators/half-moon.html#additional-methods","title":"Additional Methods","text":"<p>This generator does not have any additional methods.</p>"},{"location":"datasets/generators/hyperplane.html","title":"Hyperplane","text":"<p>[source]</p>"},{"location":"datasets/generators/hyperplane.html#hyperplane","title":"Hyperplane","text":"<p>Generates a labeled dataset whose samples form a hyperplane in n-dimensional vector space and whose labels are continuous values drawn from a uniform random distribution between -1 and 1. When the number of coefficients is either 1, 2 or 3, the samples form points, lines, and planes respectively. Due to its linearity, Hyperplane is especially useful for testing linear regression models.</p> <p>Data Types: Continuous</p> <p>Label Type: Continuous</p>"},{"location":"datasets/generators/hyperplane.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 coefficients [1, -1] array The n coefficients of the hyperplane where n is the dimensionality. 2 intercept 0.0 float The y intercept term. 3 noise 0.1 float The factor of gaussian noise to add to the data points."},{"location":"datasets/generators/hyperplane.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Datasets\\Generators\\Hyperplane;\n\n$generator = new Hyperplane([0.1, 3, -5, 0.01], 150.0, 0.25);\n</code></pre>"},{"location":"datasets/generators/hyperplane.html#additional-methods","title":"Additional Methods","text":"<p>This generator does not have any additional methods.</p>"},{"location":"datasets/generators/swiss-roll.html","title":"Swiss Roll","text":"<p>[source]</p>"},{"location":"datasets/generators/swiss-roll.html#swiss-roll","title":"Swiss Roll","text":"<p>Generate a non-linear 3-dimensional dataset resembling a swiss roll or spiral. The labels are the seeds to the swiss roll transformation.</p> <p>Data Types: Continuous</p> <p>Label Type: Continuous</p>"},{"location":"datasets/generators/swiss-roll.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 x 0.0 float The x coordinate of the center of the swiss roll. 2 y 0.0 float The y coordinate of the center of the swiss roll. 3 z 0.0 float The z coordinate of the center of the swiss roll. 4 scale 1.0 float The scaling factor of the swiss roll. 5 depth 21.0 float The depth of the swiss roll i.e the scale of the y axis. 6 noise 0.1 float The standard deviation of the gaussian noise."},{"location":"datasets/generators/swiss-roll.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Datasets\\Generators\\SwissRoll;\n\n$generator = new SwissRoll(5.5, 1.5, -2.0, 10, 21.0, 0.2);\n</code></pre>"},{"location":"datasets/generators/swiss-roll.html#additional-methods","title":"Additional Methods","text":"<p>This generator does not have any additional methods.</p>"},{"location":"extractors/api.html","title":"Extractors","text":"<p>Extractors are data table iterators that help you import data from various source formats such as CSV, NDJSON, and SQL in an efficient way. They implement one of the standard PHP Traversable interfaces and are compatible anywhere the iterable pseudotype is accepted. Extractors that implement the Writable interface can be used to save other iterators such as dataset objects and other extractors.</p>"},{"location":"extractors/api.html#iterate","title":"Iterate","text":"<p>Calling <code>foreach</code> on an extractor object iterates over the rows of the data table. In the example below, we'll use the CSV extractor to print out the rows of the dataset to the console.</p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\nforeach (new CSV('example.csv') as $row) {\n    print_r($row);\n}\n</code></pre> <p>We can also instantiate a new Dataset object by passing an extractor to the <code>fromIterator()</code> method.</p> <pre><code>use Rubix\\ML\\Datasets\\Labeled;\nuse Rubix\\ML\\Extractors\\NDJSON;\n\n$dataset = Labeled::fromIterator(new NDJSON('example.ndjson'));\n</code></pre>"},{"location":"extractors/api.html#export","title":"Export","text":"<p>Extractors that implement the Exporter interface have an additional <code>export()</code> method that takes an iterable type and exports the data to storage.</p> <pre><code>public export(iterable $iterator, ?array $header = null) : void\n</code></pre> <pre><code>$extractor-&gt;export($dataset);\n</code></pre> <p>Note</p> <p>The extractor will overwrite any existing data if the file or database already exists.</p>"},{"location":"extractors/api.html#return-an-iterator","title":"Return an Iterator","text":"<p>To return the underlying iterator wrapped by the extractor object: <pre><code>public getIterator() : Traversable\n</code></pre></p> <p>The example below shows how you can instantiate a new dataset object using only a portion of the source dataset by wrapping an underlying iterator with the standard PHP library's Limit Iterator.</p> <pre><code>use Rubix\\ML\\Extractors\\NDJSON;\nuse Rubix\\ML\\Datasets\\Unlabeled;\nuse LimitIterator;\n\n$extractor = new NDJSON('example.ndjson');\n\n$iterator = new LimitIterator($extractor-&gt;getIterator(), 500, 1000);\n\n$dataset = Unlabeled::fromIterator($iterator);\n</code></pre>"},{"location":"extractors/column-filter.html","title":"Column Filter","text":"<p>[source]</p>"},{"location":"extractors/column-filter.html#column-filter","title":"Column Filter","text":"<p>Interfaces: Extractor</p>"},{"location":"extractors/column-filter.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 iterator Traversable The base iterator. 2 keys array The string and/or integer keys of the columns to filter from the table"},{"location":"extractors/column-filter.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Extractors\\ColumnFilter;\nuse Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new ColumnFilter(new CSV('example.csv', true), [\n    'texture', 'class',\n]);\n</code></pre>"},{"location":"extractors/column-filter.html#additional-methods","title":"Additional Methods","text":"<p>This extractor does not have any additional methods.</p>"},{"location":"extractors/column-picker.html","title":"Column Picker","text":"<p>[source]</p>"},{"location":"extractors/column-picker.html#column-picker","title":"Column Picker","text":"<p>An extractor that wraps another iterator and selects and reorders the columns of the data table according to the keys specified by the user. The key of a column may either be a string or a column number (integer) depending on the way the columns are indexed in the base iterator.</p> <p>Interfaces: Extractor</p>"},{"location":"extractors/column-picker.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 iterator Traversable The base iterator. 2 keys array The string and/or integer keys of the columns to pick and reorder from the table"},{"location":"extractors/column-picker.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Extractors\\ColumnPicker;\nuse Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new ColumnPicker(new CSV('example.csv', true), [\n    'attitude', 'texture', 'class', 'rating',\n]);\n</code></pre>"},{"location":"extractors/column-picker.html#additional-methods","title":"Additional Methods","text":"<p>This extractor does not have any additional methods.</p>"},{"location":"extractors/concatenator.html","title":"Concatenator","text":"<p>[source]</p>"},{"location":"extractors/concatenator.html#concatenator","title":"Concatenator","text":"<p>Combines multiple iterators by concatenating the output of one iterator with the output of the next iterator in the series.</p> <p>Interfaces: Extractor</p>"},{"location":"extractors/concatenator.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 iterators iterable The iterators to concatenate together."},{"location":"extractors/concatenator.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Extractors\\Concatenator;\nuse Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new Concatenator([\n    new CSV('dataset1.csv'),\n    new CSV('dataset2.csv'),\n    new CSV('dataset3.csv'),\n]);\n</code></pre>"},{"location":"extractors/concatenator.html#additional-methods","title":"Additional Methods","text":"<p>This extractor does not have any additional methods.</p>"},{"location":"extractors/csv.html","title":"CSV","text":"<p>[source]</p>"},{"location":"extractors/csv.html#csv","title":"CSV","text":"<p>A plain-text format that use newlines to delineate rows and a user-specified delimiter (usually a comma) to separate the values of each column in a data table. Comma-Separated Values (CSV) format is a common format but suffers from not being able to retain type information - thus, all data is imported as categorical data (strings) by default.</p> <p>Note</p> <p>This implementation of CSV is based on the definition in RFC 4180.</p> <p>Interfaces: Extractor, Writable</p>"},{"location":"extractors/csv.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 path string The path to the CSV file. 2 header false bool Does the CSV document have a header as the first row? 3 delimiter ',' string The character that delineates the values of the columns of the data table. 4 enclosure '\"' string The character used to enclose a cell that contains a delimiter in the body. 5 escape '\\' string The character used as an escape character (one character only)."},{"location":"extractors/csv.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('example.csv', true, ',', '\"','\\\\');\n</code></pre>"},{"location":"extractors/csv.html#additional-methods","title":"Additional Methods","text":"<p>Return the column titles of the data table. <pre><code>public header() : array\n</code></pre></p>"},{"location":"extractors/csv.html#references","title":"References","text":"<ol> <li> <p>T. Shafranovich. (2005). Common Format and MIME Type for Comma-Separated Values (CSV) Files.\u00a0\u21a9</p> </li> </ol>"},{"location":"extractors/deduplicator.html","title":"Deduplicator","text":"<p>[source]</p>"},{"location":"extractors/deduplicator.html#deduplicator","title":"Deduplicator","text":"<p>Removes duplicate records from a dataset while the records are in flight. Deduplicator uses a Bloom filter under the hood to probabilistically identify records that have already been seen before.</p> <p>Note</p> <p>Due to its probabilistic nature, Deduplicator may mistakenly drop unique records at a bounded rate.</p> <p>Interfaces: Extractor</p>"},{"location":"extractors/deduplicator.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 iterator Traversable The base iterator. 2 maxFalsePositiveRate 0.001 float The false positive rate to remain below. 3 numHashes 4 int The number of hash functions used, i.e. the number of slices per layer. Set to null for auto. 4 layerSize 32000000 int The size of each layer of the filter in bits."},{"location":"extractors/deduplicator.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Extractors\\Deduplicator;\nuse Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new Deduplicator(new CSV('example.csv', true), 0.01, 3, 32000000);\n</code></pre>"},{"location":"extractors/deduplicator.html#additional-methods","title":"Additional Methods","text":"<p>Return the number of records that have been dropped so far. <pre><code>public dropped() : int\n</code></pre></p>"},{"location":"extractors/ndjson.html","title":"NDJSON","text":"<p>[source]</p>"},{"location":"extractors/ndjson.html#ndjson","title":"NDJSON","text":"<p>NDJSON or Newline Delimited JSON files contain rows of data encoded in Javascript Object Notation (JSON) arrays or objects. The format is like a mix of JSON and CSV and has the advantage of retaining data type information and being read into memory incrementally.</p> <p>Note</p> <p>Empty lines are ignored by the parser.</p> <p>Interfaces: Extractor, Writable</p>"},{"location":"extractors/ndjson.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 path string The path to the NDJSON file."},{"location":"extractors/ndjson.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Extractors\\NDJSON;\n\n$extractor = new NDJSON('example.ndjson');\n</code></pre>"},{"location":"extractors/ndjson.html#additional-methods","title":"Additional Methods","text":"<p>This extractor does not have any additional methods.</p>"},{"location":"extractors/sql-table.html","title":"SQL Table","text":"<p>[source]</p>"},{"location":"extractors/sql-table.html#sql-table","title":"SQL Table","text":"<p>The SQL table extractor iterates over the rows of a relational database table. It works with the PHP Data Objects (PDO) interface to connect to a broad selection of databases such MySQL, PostgreSQL, and Sqlite.</p> <p>Note</p> <p>This extractor requires the PDO extension.</p> <p>Note</p> <p>The order in which the rows are iterated over is not guaranteed. Use a custom query with <code>ORDER BY</code> statement if ordering matters.</p> <p>Interfaces: Extractor</p>"},{"location":"extractors/sql-table.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 connection PDO The PDO connection to the database. 2 table string The name of the table to select from. 3 batch size 256 int The number of rows of the table to load in a single query."},{"location":"extractors/sql-table.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Extractors\\SQLTable;\nuse PDO;\n\n$connection = new PDO('sqlite:/example.sqlite');\n\n$this-&gt;extractor = new SQLTable($connection, 'users', 256);\n</code></pre>"},{"location":"extractors/sql-table.html#additional-methods","title":"Additional Methods","text":"<p>Return the column titles of the data table. <pre><code>public header() : array\n</code></pre></p>"},{"location":"graph/trees/ball-tree.html","title":"Ball Tree","text":"<p>[source]</p>"},{"location":"graph/trees/ball-tree.html#ball-tree","title":"Ball Tree","text":"<p>A binary spatial tree that partitions a dataset into successively smaller and tighter ball nodes whose boundaries are defined by a hypersphere. Ball Tree works well in higher dimensions since the partitioning schema does not rely on a finite number of 1-dimensional axis aligned splits such as with k-d tree.</p> <p>Interfaces: Binary Tree, Spatial</p> <p>Data Type Compatibility: Depends on distance kernel</p>"},{"location":"graph/trees/ball-tree.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 maxLeafSize 30 int The maximum number of samples that each leaf node can contain. 2 kernel Euclidean Distance The distance kernel used to compute the distance between sample points."},{"location":"graph/trees/ball-tree.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Graph\\Trees\\BallTree;\nuse Rubix\\ML\\Kernels\\Distance\\Euclidean;\n\n$tree = new BallTree(40, new Euclidean());\n</code></pre>"},{"location":"graph/trees/ball-tree.html#additional-methods","title":"Additional Methods","text":"<p>This tree does not have any additional methods.</p>"},{"location":"graph/trees/ball-tree.html#references","title":"References","text":"<ol> <li> <p>S. M. Omohundro. (1989). Five Balltree Construction Algorithms.\u00a0\u21a9</p> </li> <li> <p>M. Dolatshah et al. (2015). Ball*-tree: Efficient spatial indexing for constrained nearest-neighbor search in metric spaces.\u00a0\u21a9</p> </li> </ol>"},{"location":"graph/trees/k-d-tree.html","title":"K-d Tree","text":"<p>[source]</p>"},{"location":"graph/trees/k-d-tree.html#k-d-tree","title":"K-d Tree","text":"<p>A multi-dimensional binary spatial tree for fast nearest neighbor queries. The K-d tree construction algorithm separates data points into bounded hypercubes or boxes that are used to determine which branches to prune off during nearest neighbor and range searches enabling them to complete in sub-linear time.</p> <p>Interfaces: Binary Tree, Spatial</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"graph/trees/k-d-tree.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 maxLeafSize 30 int The maximum number of samples that each leaf node can contain. 2 kernel Euclidean Distance The distance kernel used to compute the distance between sample points."},{"location":"graph/trees/k-d-tree.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Graph\\Trees\\KDTree;\nuse Rubix\\ML\\Kernels\\Distance\\Euclidean;\n\n$tree = new KDTree(30, new Euclidean());\n</code></pre>"},{"location":"graph/trees/k-d-tree.html#additional-methods","title":"Additional Methods","text":"<p>This tree does not have any additional methods.</p>"},{"location":"graph/trees/k-d-tree.html#references","title":"References","text":"<ol> <li> <p>J. L. Bentley. (1975). Multidimensional Binary Search Trees Used for Associative Searching.\u00a0\u21a9</p> </li> </ol>"},{"location":"helpers/params.html","title":"Params","text":"<p>Generate distributions of values to use in conjunction with Grid Search or other forms of model selection and/or cross validation.</p>"},{"location":"helpers/params.html#generate-params","title":"Generate Params","text":"<p>To generate a unique distribution of integer parameters: <pre><code>public static ints(int $min, int $max, int $n = 10) : array\n</code></pre></p> <pre><code>use Rubix\\ML\\Helpers\\Params;\n\n$ints = Params::ints(0, 100, 5);\n\nprint_r($ints);\n</code></pre> <pre><code>Array\n(\n    [0] =&gt; 88\n    [1] =&gt; 48\n    [2] =&gt; 64\n    [3] =&gt; 100\n    [4] =&gt; 42\n)\n</code></pre> <p>To generate a random distribution of floating point parameters: <pre><code>public static floats(float $min, float $max, int $n = 10) : array\n</code></pre></p> <pre><code>use Rubix\\ML\\Helpers\\Params;\n\n$floats = Params::floats(0, 100, 5);\n\nprint_r($floats);\n</code></pre> <pre><code>Array\n(\n    [0] =&gt; 42.65728\n    [1] =&gt; 66.74335\n    [2] =&gt; 15.17243\n    [3] =&gt; 71.92631\n    [4] =&gt; 4.638863\n)\n</code></pre> <p>To generate a uniformly spaced grid of parameters: <pre><code>public static grid(float $min, float $max, int $n = 10) : array\n</code></pre></p> <pre><code>use Rubix\\ML\\Helpers\\Params;\n\n$grid = Params::grid(0, 100, 5);\n\nprint_r($grid);\n</code></pre> <pre><code>Array\n(\n    [0] =&gt; 0\n    [1] =&gt; 25\n    [2] =&gt; 50\n    [3] =&gt; 75\n    [4] =&gt; 100\n)\n</code></pre>"},{"location":"kernels/distance/canberra.html","title":"Canberra","text":"<p>[source]</p>"},{"location":"kernels/distance/canberra.html#canberra","title":"Canberra","text":"<p>A weighted version of the Manhattan distance, Canberra examines the sum of a series of fractional differences between two samples. Canberra can be very sensitive when both coordinates are near zero.</p> \\[ Canberra(\\mathbf {a} ,\\mathbf {b} )=\\sum _{i=1}^{n}{\\frac {|a_{i}-b_{i}|}{|a_{i}|+|b_{i}|}} \\] <p>Data Type Compatibility: Continuous</p>"},{"location":"kernels/distance/canberra.html#parameters","title":"Parameters","text":"<p>This kernel does not have any parameters.</p>"},{"location":"kernels/distance/canberra.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\Distance\\Canberra;\n\n$kernel = new Canberra();\n</code></pre>"},{"location":"kernels/distance/canberra.html#references","title":"References","text":"<ol> <li> <p>G. N. Lance et al. (1967). Mixed-data classificatory programs I. Agglomerative Systems.\u00a0\u21a9</p> </li> </ol>"},{"location":"kernels/distance/cosine.html","title":"Cosine","text":"<p>[source]</p>"},{"location":"kernels/distance/cosine.html#cosine","title":"Cosine","text":"<p>Cosine Similarity is a measure that ignores the magnitude of the distance between two non-zero vectors thus acting as strictly a judgement of orientation. Two vectors with the same orientation have a cosine similarity of 1, whereas two vectors oriented at 90\u00b0 relative to each other have a similarity of 0, and two vectors diametrically opposed have a similarity of -1. To be used as a distance function, we subtract the Cosine Similarity from 1 in order to satisfy the positive semi-definite condition, therefore the Cosine distance is a number between 0 and 2.</p> \\[ {\\displaystyle {\\text{Cosine}}=1 - {\\mathbf {A} \\cdot \\mathbf {B}  \\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}=1 - {\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}}} \\] <p>Data Type Compatibility: Continuous</p>"},{"location":"kernels/distance/cosine.html#parameters","title":"Parameters","text":"<p>This kernel does not have any parameters.</p>"},{"location":"kernels/distance/cosine.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\Distance\\Cosine;\n\n$kernel = new Cosine();\n</code></pre>"},{"location":"kernels/distance/diagonal.html","title":"Diagonal","text":"<p>[source]</p>"},{"location":"kernels/distance/diagonal.html#diagonal","title":"Diagonal","text":"<p>The Diagonal (a.k.a. Chebyshev) distance is a measure that constrains movement to horizontal, vertical, and diagonal. An example of a game that uses diagonal movement is chess.</p> \\[ {\\displaystyle Diagonal(a,b)=\\max _{i}(|a_{i}-b_{i}|)} \\] <p>Data Type Compatibility: Continuous</p>"},{"location":"kernels/distance/diagonal.html#parameters","title":"Parameters","text":"<p>This kernel does not have any parameters.</p>"},{"location":"kernels/distance/diagonal.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\Distance\\Diagonal;\n\n$kernel = new Diagonal();\n</code></pre>"},{"location":"kernels/distance/euclidean.html","title":"Euclidean","text":"<p>[source]</p>"},{"location":"kernels/distance/euclidean.html#euclidean","title":"Euclidean","text":"<p>The straight line (bee line) distance between two points. Euclidean distance has the nice property of being invariant under any rotation.</p> \\[ Euclidean\\left(a,b\\right) = \\sqrt {\\sum _{i=1}^{n}  \\left( a_{i}-b_{i}\\right)^2}  \\] <p>Data Type Compatibility: Continuous</p>"},{"location":"kernels/distance/euclidean.html#parameters","title":"Parameters","text":"<p>This kernel does not have any parameters.</p>"},{"location":"kernels/distance/euclidean.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\Distance\\Euclidean;\n\n$kernel = new Euclidean();\n</code></pre>"},{"location":"kernels/distance/euclidean.html#references","title":"References","text":"<ol> <li> <p>J. K. Dixon. (1978). Pattern Recognition with Partly Missing Data.\u00a0\u21a9</p> </li> </ol>"},{"location":"kernels/distance/gower.html","title":"Gower","text":"<p>[source]</p>"},{"location":"kernels/distance/gower.html#gower","title":"Gower","text":"<p>A robust distance kernel that measures samples consisting of a mix of categorical and continuous data types while also handling missing (NaN) values. When comparing continuous data, the Gower metric is equivalent to the normalized Manhattan distance and when comparing categorical data it is equivalent to the Hamming distance.</p> <p>Note: The Gower metric expects all continuous variables to have a standardized range. The default range works for values that have been normalized between 0 and 1.</p> <p>Data Type Compatibility: Continuous, Categorical</p>"},{"location":"kernels/distance/gower.html#parameters","title":"Parameters","text":"# Param Default Type Description 1 range 1.0 float The standardized range of the continuous feature columns. Ex. [0, 1] has a range of 1, [-1, 1] has a range of 2, and so forth."},{"location":"kernels/distance/gower.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\Distance\\Gower;\n\n$kernel = new Gower(2.0);\n</code></pre>"},{"location":"kernels/distance/gower.html#references","title":"References","text":"<ul> <li>J. C. Gower. (1971). A General Coefficient of Similarity and Some of Its Properties.</li> </ul>"},{"location":"kernels/distance/hamming.html","title":"Hamming","text":"<p>[source]</p>"},{"location":"kernels/distance/hamming.html#hamming","title":"Hamming","text":"<p>A categorical distance function that measures distance as the number of substitutions necessary to convert one sample to the other.</p> <p>Data Type Compatibility: Categorical</p>"},{"location":"kernels/distance/hamming.html#parameters","title":"Parameters","text":"<p>This kernel does not have any parameters.</p>"},{"location":"kernels/distance/hamming.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\Distance\\Hamming;\n\n$kernel = new Hamming();\n</code></pre>"},{"location":"kernels/distance/hamming.html#references","title":"References","text":"<ol> <li> <p>R. W. Hamming. (1950). Error detecting and error correcting codes.\u00a0\u21a9</p> </li> </ol>"},{"location":"kernels/distance/jaccard.html","title":"Jaccard","text":"<p>[source]</p>"},{"location":"kernels/distance/jaccard.html#jaccard","title":"Jaccard","text":"<p>The generalized Jaccard distance is a measure of distance with a range from 0 to 1 and can be thought of as the size of the intersection divided by the size of the union of two points if they were consisted only of binary random variables.</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"kernels/distance/jaccard.html#parameters","title":"Parameters","text":"<p>This kernel does not have any parameters.</p>"},{"location":"kernels/distance/jaccard.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\Distance\\Jaccard;\n\n$kernel = new Jaccard();\n</code></pre>"},{"location":"kernels/distance/manhattan.html","title":"Manhattan","text":"<p>[source]</p>"},{"location":"kernels/distance/manhattan.html#manhattan","title":"Manhattan","text":"<p>A distance metric that constrains movement to horizontal and vertical, similar to navigating the city blocks of Manhattan. An example of a board game that uses this type of movement is Checkers.</p> \\[ Manhattan(\\mathbf {a} ,\\mathbf {b})=\\|\\mathbf {a} -\\mathbf {b} \\|_{1}=\\sum _{i=1}^{n}|a_{i}-b_{i}| \\] <p>Data Type Compatibility: Continuous</p>"},{"location":"kernels/distance/manhattan.html#parameters","title":"Parameters","text":"<p>This kernel does not have any parameters.</p>"},{"location":"kernels/distance/manhattan.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\Distance\\Manhattan;\n\n$kernel = new Manhattan();\n</code></pre>"},{"location":"kernels/distance/minkowski.html","title":"Minkowski","text":"<p>[source]</p>"},{"location":"kernels/distance/minkowski.html#minkowski","title":"Minkowski","text":"<p>The Minkowski distance can be considered as a generalization of both the Euclidean and Manhattan distances. When the lambda parameter is set to 1 or 2, the distance is equivalent to Manhattan and Euclidean respectively.</p> \\[ {\\displaystyle Minkowski\\left(a,b\\right)=\\left(\\sum _{i=1}^{n}|a_{i}-b_{i}|^{p}\\right)^{\\frac {1}{p}}} \\] <p>Data Type Compatibility: Continuous</p>"},{"location":"kernels/distance/minkowski.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 lambda 3.0 float Controls the curvature of the unit circle drawn from a point at a fixed distance."},{"location":"kernels/distance/minkowski.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\Distance\\Minkowski;\n\n$kernel = new Minkowski(4.0);\n</code></pre>"},{"location":"kernels/distance/safe-euclidean.html","title":"Safe Euclidean","text":"<p>[source]</p>"},{"location":"kernels/distance/safe-euclidean.html#safe-euclidean","title":"Safe Euclidean","text":"<p>An Euclidean distance metric suitable for samples that may contain NaN (not a number) values i.e. missing data. The Safe Euclidean metric approximates the Euclidean distance function by dropping NaN values and scaling the distance according to the proportion of non-NaNs (in either a or b or both) to compensate.</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"kernels/distance/safe-euclidean.html#parameters","title":"Parameters","text":"<p>This kernel does not have any parameters.</p>"},{"location":"kernels/distance/safe-euclidean.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\Distance\\SafeEuclidean;\n\n$kernel = new SafeEuclidean();\n</code></pre>"},{"location":"kernels/distance/safe-euclidean.html#references","title":"References","text":"<ol> <li> <p>J. K. Dixon. (1978). Pattern Recognition with Partly Missing Data.\u00a0\u21a9</p> </li> </ol>"},{"location":"kernels/distance/sparse-cosine.html","title":"Sparse Cosine","text":"<p>[source]</p>"},{"location":"kernels/distance/sparse-cosine.html#sparse-cosine","title":"Sparse Cosine","text":"<p>A version of the Cosine distance kernel that is specifically optimized for computing sparse vectors.</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"kernels/distance/sparse-cosine.html#parameters","title":"Parameters","text":"<p>This kernel does not have any parameters.</p>"},{"location":"kernels/distance/sparse-cosine.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\Distance\\SparseCosine;\n\n$kernel = new SparseCosine();\n</code></pre>"},{"location":"kernels/svm/linear.html","title":"Linear","text":"<p>[source]</p>"},{"location":"kernels/svm/linear.html#linear","title":"Linear","text":"<p>A simple linear kernel computed by the dot product of two vectors.</p>"},{"location":"kernels/svm/linear.html#parameters","title":"Parameters","text":"<p>This kernel does not have any parameters.</p>"},{"location":"kernels/svm/linear.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\SVM\\Linear;\n\n$kernel = new Linear();\n</code></pre>"},{"location":"kernels/svm/polynomial.html","title":"Polynomial","text":"<p>[source]</p>"},{"location":"kernels/svm/polynomial.html#polynomial","title":"Polynomial","text":"<p>This kernel projects a sample vector using polynomials of the p'th degree.</p>"},{"location":"kernels/svm/polynomial.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 degree 3 int The degree of the polynomial. 2 gamma null float The kernel coefficient. 3 coef0 0. float The independent term."},{"location":"kernels/svm/polynomial.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\SVM\\Polynomial;\n\n$kernel = new Polynomial(3, null, 0.);\n</code></pre>"},{"location":"kernels/svm/rbf.html","title":"RBF","text":"<p>[source]</p>"},{"location":"kernels/svm/rbf.html#rbf","title":"RBF","text":"<p>Non linear radial basis function (RBF) computes the distance from a centroid or origin.</p>"},{"location":"kernels/svm/rbf.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 gamma null float The kernel coefficient."},{"location":"kernels/svm/rbf.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\SVM\\RBF;\n\n$kernel = new RBF(null);\n</code></pre>"},{"location":"kernels/svm/sigmoidal.html","title":"Sigmoidal","text":"<p>[source]</p>"},{"location":"kernels/svm/sigmoidal.html#sigmoidal","title":"Sigmoidal","text":"<p>S shaped nonliearity kernel with output values ranging from -1 to 1.</p>"},{"location":"kernels/svm/sigmoidal.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 gamma null float The kernel coefficient. 2 coef0 0. float The independent term."},{"location":"kernels/svm/sigmoidal.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Kernels\\SVM\\Sigmoidal;\n\n$kernel = new Sigmoidal(null, 0.);\n</code></pre>"},{"location":"loggers/screen.html","title":"Screen","text":"<p>[source]</p>"},{"location":"loggers/screen.html#screen","title":"Screen","text":"<p>A logger that displays log messages to the standard output.</p>"},{"location":"loggers/screen.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 channel '' string The channel name that appears on each line. 2 timestampFormat 'Y-m-d H:i:s' string The format of the timestamp."},{"location":"loggers/screen.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Loggers\\Screen;\n\n$logger = new Screen('mlp', 'Y-m-d H:i:s');\n</code></pre>"},{"location":"neural-network/activation-functions/elu.html","title":"ELU","text":"<p>[source]</p>"},{"location":"neural-network/activation-functions/elu.html#elu","title":"ELU","text":"<p>Exponential Linear Units are a type of rectifier that soften the transition from non-activated to activated using the exponential function. As such, ELU produces smoother gradients than the piecewise linear ReLU function.</p> \\[ {\\displaystyle ELU = {\\begin{cases}\\alpha \\left(e^{x}-1\\right)&amp;{\\text{if }}x\\leq 0\\\\x&amp;{\\text{if }}x&gt;0\\end{cases}}} \\]"},{"location":"neural-network/activation-functions/elu.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 alpha 1.0 float The value at which leakage will begin to saturate. Ex. alpha = 1.0 means that the output will never be less than -1.0 when inactivated."},{"location":"neural-network/activation-functions/elu.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ELU;\n\n$activationFunction = new ELU(2.5);\n</code></pre>"},{"location":"neural-network/activation-functions/elu.html#references","title":"References","text":"<ol> <li> <p>D. A. Clevert et al. (2016). Fast and Accurate Deep Network Learning by Exponential Linear Units.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/activation-functions/gelu.html","title":"GELU","text":"<p>[source]</p>"},{"location":"neural-network/activation-functions/gelu.html#gelu","title":"GELU","text":"<p>Gaussian Error Linear Units (GELUs) are rectifiers that are gated by the magnitude of their input rather than the sign of their input as with ReLU variants. Their output can be interpreted as the expected value of a neuron with random dropout regularization applied.</p>"},{"location":"neural-network/activation-functions/gelu.html#parameters","title":"Parameters","text":"<p>This activation function does not have any parameters.</p>"},{"location":"neural-network/activation-functions/gelu.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\ActivationFunctions\\GELU;\n\n$activationFunction = new GELU();\n</code></pre>"},{"location":"neural-network/activation-functions/gelu.html#references","title":"References","text":"<ul> <li>D. Hendrycks et al. (2018). Gaussian Error Linear Units (GELUs).</li> </ul>"},{"location":"neural-network/activation-functions/hyperbolic-tangent.html","title":"Hyperbolic Tangent","text":"<p>[source]</p>"},{"location":"neural-network/activation-functions/hyperbolic-tangent.html#hyperbolic-tangent","title":"Hyperbolic Tangent","text":"<p>An S-shaped function that squeezes the input value into an output space between -1 and 1. Hyperbolic Tangent (or tanh) has the advantage of being zero centered, however is known to saturate with highly positive or negative input values which can slow down training if the activations become too intense.</p> \\[ {\\displaystyle \\tanh(x)={\\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}} \\]"},{"location":"neural-network/activation-functions/hyperbolic-tangent.html#parameters","title":"Parameters","text":"<p>This activation function does not have any parameters.</p>"},{"location":"neural-network/activation-functions/hyperbolic-tangent.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\ActivationFunctions\\HyperbolicTangent;\n\n$activationFunction = new HyperbolicTangent();\n</code></pre>"},{"location":"neural-network/activation-functions/leaky-relu.html","title":"Leaky ReLU","text":"<p>[source]</p>"},{"location":"neural-network/activation-functions/leaky-relu.html#leaky-relu","title":"Leaky ReLU","text":"<p>Leaky Rectified Linear Units are activation functions that output <code>x</code> when x is greater or equal to 0 or <code>x</code> scaled by a small leakage coefficient when the input is less than 0. Leaky rectifiers have the benefit of allowing a small gradient to flow through during backpropagation even though they might not have activated during the forward pass.</p> \\[ {\\displaystyle LeakyReLU = {\\begin{cases}\\lambda x&amp;{\\text{if }}x&lt;0\\\\x&amp;{\\text{if }}x\\geq 0\\end{cases}}} \\]"},{"location":"neural-network/activation-functions/leaky-relu.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 leakage 0.1 float The amount of leakage as a proportion of the input value to allow to pass through when not inactivated."},{"location":"neural-network/activation-functions/leaky-relu.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\ActivationFunctions\\LeakyReLU;\n\n$activationFunction = new LeakyReLU(0.3);\n</code></pre>"},{"location":"neural-network/activation-functions/leaky-relu.html#references","title":"References","text":"<ol> <li> <p>A. L. Maas et al. (2013). Rectifier Nonlinearities Improve Neural Network Acoustic Models.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/activation-functions/relu.html","title":"ReLU","text":"<p>[source]</p>"},{"location":"neural-network/activation-functions/relu.html#relu","title":"ReLU","text":"<p>Rectified Linear Units (ReLU) only output the positive signal of the input. They have the benefit of having a monotonic derivative and are cheap to compute.</p> \\[ {\\displaystyle ReLU = {\\begin{aligned}&amp;{\\begin{cases}0&amp;{\\text{if }}x\\leq 0\\\\x&amp;{\\text{if }}x&gt;0\\end{cases}}=&amp;\\max\\{0,x\\}\\end{aligned}}} \\]"},{"location":"neural-network/activation-functions/relu.html#parameters","title":"Parameters","text":"<p>This activation function does not have any parameters.</p>"},{"location":"neural-network/activation-functions/relu.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ReLU;\n\n$activationFunction = new ReLU(0.1);\n</code></pre>"},{"location":"neural-network/activation-functions/relu.html#references","title":"References","text":"<ol> <li> <p>A. L. Maas et al. (2013). Rectifier Nonlinearities Improve Neural Network Acoustic Models.\u00a0\u21a9</p> </li> <li> <p>K. Konda et al. (2015). Zero-bias Autoencoders and the Benefits of Co-adapting Features.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/activation-functions/selu.html","title":"SELU","text":"<p>[source]</p>"},{"location":"neural-network/activation-functions/selu.html#selu","title":"SELU","text":"<p>Scaled Exponential Linear Units (SELU) are a self-normalizing activation function based on the ELU activation function. Neuronal activations of SELU networks automatically converge toward zero mean and unit variance, unlike explicitly normalized networks such as those with Batch Norm hidden layers.</p> \\[ {\\displaystyle SELU = 1.0507 {\\begin{cases}1.67326 (e^{x}-1)&amp;{\\text{if }}x&lt;0\\\\x&amp;{\\text{if }}x\\geq 0\\end{cases}}} \\]"},{"location":"neural-network/activation-functions/selu.html#parameters","title":"Parameters","text":"<p>This actvation function does not have any parameters.</p>"},{"location":"neural-network/activation-functions/selu.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\ActivationFunctions\\SELU;\n\n$activationFunction = new SELU();\n</code></pre>"},{"location":"neural-network/activation-functions/selu.html#references","title":"References","text":"<ol> <li> <p>G. Klambauer et al. (2017). Self-Normalizing Neural Networks.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/activation-functions/sigmoid.html","title":"Sigmoid","text":"<p>[source]</p>"},{"location":"neural-network/activation-functions/sigmoid.html#sigmoid","title":"Sigmoid","text":"<p>A bounded S-shaped function (sometimes called the Logistic function) with an output value between 0 and 1. The output of the sigmoid function has the advantage of being interpretable as a probability, however it is not zero-centered and tends to saturate if inputs become large.</p> \\[ {\\displaystyle Sigmoid = {\\frac {1}{1+e^{-x}}}} \\]"},{"location":"neural-network/activation-functions/sigmoid.html#parameters","title":"Parameters","text":"<p>This activation function does not have any parameters.</p>"},{"location":"neural-network/activation-functions/sigmoid.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\ActivationFunctions\\Sigmoid;\n\n$activationFunction = new Sigmoid();\n</code></pre>"},{"location":"neural-network/activation-functions/silu.html","title":"SiLU","text":"<p>[source]</p>"},{"location":"neural-network/activation-functions/silu.html#silu","title":"SiLU","text":"<p>Sigmoid Linear Units are smooth and non-monotonic rectified activation functions. Their inputs are weighted by the Sigmoid activation function acting as a self-gating mechanism.</p>"},{"location":"neural-network/activation-functions/silu.html#parameters","title":"Parameters","text":"<p>This activation function does not have any parameters.</p>"},{"location":"neural-network/activation-functions/silu.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\ActivationFunctions\\SiLU;\n\n$activationFunction = new SiLU();\n</code></pre>"},{"location":"neural-network/activation-functions/silu.html#references","title":"References","text":"<ol> <li> <p>S. Elwing et al. (2017). Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/activation-functions/soft-plus.html","title":"Soft Plus","text":"<p>[source]</p>"},{"location":"neural-network/activation-functions/soft-plus.html#soft-plus","title":"Soft Plus","text":"<p>A smooth approximation of the piecewise linear ReLU activation function.</p> \\[ {\\displaystyle Soft-Plus = \\log \\left(1+e^{x}\\right)} \\]"},{"location":"neural-network/activation-functions/soft-plus.html#parameters","title":"Parameters","text":"<p>This activation function does not have any parameters.</p>"},{"location":"neural-network/activation-functions/soft-plus.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\ActivationFunctions\\SoftPlus;\n\n$activationFunction = new SoftPlus();\n</code></pre>"},{"location":"neural-network/activation-functions/soft-plus.html#references","title":"References","text":"<ol> <li> <p>X. Glorot et al. (2011). Deep Sparse Rectifier Neural Networks.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/activation-functions/softmax.html","title":"Softmax","text":"<p>[source]</p>"},{"location":"neural-network/activation-functions/softmax.html#softmax","title":"Softmax","text":"<p>The Softmax function is a generalization of the Sigmoid function that squashes each activation between 0 and 1 with the addition that all activations add up to 1. Together, these properties allow the output of the Softmax function to be interpretable as a joint probability distribution.</p> \\[ {\\displaystyle Softmax = {\\frac {e^{x_{i}}}{\\sum _{j=1}^{J}e^{x_{j}}}}} \\]"},{"location":"neural-network/activation-functions/softmax.html#parameters","title":"Parameters","text":"<p>This activation function does not have any parameters.</p>"},{"location":"neural-network/activation-functions/softmax.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\ActivationFunctions\\Softmax;\n\n$activationFunction = new Softmax();\n</code></pre>"},{"location":"neural-network/activation-functions/softsign.html","title":"Soft Sign","text":"<p>[source]</p>"},{"location":"neural-network/activation-functions/softsign.html#softsign","title":"Softsign","text":"<p>A smooth sigmoid-shaped function that squashes the input between -1 and 1.</p> \\[ {\\displaystyle Softsign = {\\frac {x}{1+|x|}}} \\]"},{"location":"neural-network/activation-functions/softsign.html#parameters","title":"Parameters","text":"<p>This activation function does not have any parameters.</p>"},{"location":"neural-network/activation-functions/softsign.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\ActivationFunctions\\Softsign;\n\n$activationFunction = new Softsign();\n</code></pre>"},{"location":"neural-network/activation-functions/softsign.html#references","title":"References","text":"<ol> <li> <p>X. Glorot et al. (2010). Understanding the Difficulty of Training Deep Feedforward Neural Networks.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/activation-functions/thresholded-relu.html","title":"Thresholded ReLU","text":"<p>[source]</p>"},{"location":"neural-network/activation-functions/thresholded-relu.html#thresholded-relu","title":"Thresholded ReLU","text":"<p>A version of the ReLU function that activates only if the input is above some user-specified threshold level.</p> \\[ {\\displaystyle ThresholdedReLU = {\\begin{aligned}&amp;{\\begin{cases}0&amp;{\\text{if }}x\\leq \\theta \\\\x&amp;{\\text{if }}x&gt;\\theta\\end{cases}}\\end{aligned}}} \\]"},{"location":"neural-network/activation-functions/thresholded-relu.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 threshold 1.0 float The threshold at which the neuron is activated."},{"location":"neural-network/activation-functions/thresholded-relu.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ThresholdedReLU;\n\n$activationFunction = new ThresholdedReLU(0.5);\n</code></pre>"},{"location":"neural-network/activation-functions/thresholded-relu.html#references","title":"References","text":"<ol> <li> <p>K. Konda et al. (2015). Zero-bias autoencoders and the benefits of co-adapting features.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/cost-functions/cross-entropy.html","title":"Cross Entropy","text":"<p>[source]</p>"},{"location":"neural-network/cost-functions/cross-entropy.html#cross-entropy","title":"Cross Entropy","text":"<p>Cross Entropy (or log loss) measures the performance of a classification model whose output is a joint probability distribution over the possible classes. Entropy increases as the predicted probability distribution diverges from the actual distribution.</p> \\[ Cross Entropy = -\\sum_{c=1}^My_{o,c}\\log(p_{o,c}) \\]"},{"location":"neural-network/cost-functions/cross-entropy.html#parameters","title":"Parameters","text":"<p>This cost function does not have any parameters.</p>"},{"location":"neural-network/cost-functions/cross-entropy.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\CostFunctions\\CrossEntropy;\n\n$costFunction = new CrossEntropy();\n</code></pre>"},{"location":"neural-network/cost-functions/huber-loss.html","title":"Huber Loss","text":"<p>[source]</p>"},{"location":"neural-network/cost-functions/huber-loss.html#huber-loss","title":"Huber Loss","text":"<p>The pseudo Huber Loss function transitions between L1 and L2 loss at a given pivot point (defined by delta) such that the function becomes more quadratic as the loss decreases. The combination of L1 and L2 losses make Huber more robust to outliers while maintaining smoothness near the minimum.</p> \\[ L_{\\delta}=     \\left\\{\\begin{matrix}         \\frac{1}{2}(y - \\hat{y})^{2} &amp; if \\left | (y - \\hat{y})  \\right | &lt; \\delta\\\\         \\delta ((y - \\hat{y}) - \\frac1 2 \\delta) &amp; otherwise     \\end{matrix}\\right. \\]"},{"location":"neural-network/cost-functions/huber-loss.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 delta 1.0 float The pivot point i.e the point where numbers larger will be evaluated with an L1 loss while number smaller will be evaluated with an L2 loss."},{"location":"neural-network/cost-functions/huber-loss.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\CostFunctions\\HuberLoss;\n\n$costFunction = new HuberLoss(0.5);\n</code></pre>"},{"location":"neural-network/cost-functions/least-squares.html","title":"Least Squares","text":"<p>[source]</p>"},{"location":"neural-network/cost-functions/least-squares.html#least-squares","title":"Least Squares","text":"<p>Least Squares (or quadratic loss) is a function that computes the average squared error (MSE) between the target output given by the labels and the actual output of the network. It produces a smooth bowl-shaped gradient that is highly-influenced by large errors.</p> \\[ Least Squares = \\sum_{i=1}^{D}(y_i-\\hat{y}_i)^2 \\]"},{"location":"neural-network/cost-functions/least-squares.html#parameters","title":"Parameters","text":"<p>This cost function does not have any parameters.</p>"},{"location":"neural-network/cost-functions/least-squares.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\CostFunctions\\LeastSquares;\n\n$costFunction = new LeastSquares();\n</code></pre>"},{"location":"neural-network/cost-functions/relative-entropy.html","title":"Relative Entropy","text":"<p>[source]</p>"},{"location":"neural-network/cost-functions/relative-entropy.html#relative-entropy","title":"Relative Entropy","text":"<p>Relative Entropy (or Kullback-Leibler divergence) is a measure of how the expectation and activation of the network diverge. It is different from Cross Entropy in that it is asymmetric and thus does not qualify as a statistical measure of error.</p> \\[ KL(\\hat{y} || y) = \\sum_{c=1}^{M}\\hat{y}_c \\log{\\frac{\\hat{y}_c}{y_c}} \\]"},{"location":"neural-network/cost-functions/relative-entropy.html#parameters","title":"Parameters","text":"<p>This cost function does not have any parameters.</p>"},{"location":"neural-network/cost-functions/relative-entropy.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\CostFunctions\\RelativeEntropy;\n\n$costFunction = new RelativeEntropy();\n</code></pre>"},{"location":"neural-network/hidden-layers/activation.html","title":"Activation","text":"<p>[source]</p>"},{"location":"neural-network/hidden-layers/activation.html#activation","title":"Activation","text":"<p>Activation layers apply a user-defined non-linear activation function to their inputs. They often work in conjunction with Dense layers as a way to transform their output.</p>"},{"location":"neural-network/hidden-layers/activation.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 activationFn ActivationFunction The function that computes the output of the layer."},{"location":"neural-network/hidden-layers/activation.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Layers\\Activation;\nuse Rubix\\ML\\NeuralNet\\ActivationFunctions\\ReLU;\n\n$layer = new Activation(new ReLU());\n</code></pre>"},{"location":"neural-network/hidden-layers/batch-norm.html","title":"Batch Norm","text":"<p>[source]</p>"},{"location":"neural-network/hidden-layers/batch-norm.html#batch-norm","title":"Batch Norm","text":"<p>Batch Norm layers normalize the activations of the previous layer such that the mean activation is close to 0 and the standard deviation is close to 1. Adding Batch Norm reduces the amount of covariate shift within the network which makes it possible to use higher learning rates and thus converge faster under some circumstances.</p>"},{"location":"neural-network/hidden-layers/batch-norm.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 decay 0.9 float The decay rate of the previous running averages of the global mean and variance. 2 betaInitializer Constant Initializer The initializer of the beta parameter. 3 gammaInitializer Constant Initializer The initializer of the gamma parameter."},{"location":"neural-network/hidden-layers/batch-norm.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Layers\\BatchNorm;\nuse Rubix\\ML\\NeuralNet\\Initializers\\Constant;\nuse Rubix\\ML\\NeuralNet\\Initializers\\Normal;\n\n$layer = new BatchNorm(0.7, new Constant(0.), new Normal(1.));\n</code></pre>"},{"location":"neural-network/hidden-layers/batch-norm.html#references","title":"References","text":"<ol> <li> <p>S. Ioffe et al. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/hidden-layers/dense.html","title":"Dense","text":"<p>[source]</p>"},{"location":"neural-network/hidden-layers/dense.html#dense","title":"Dense","text":"<p>Dense (or fully connected) hidden layers are layers of neurons that connect to each node in the previous layer by a parameterized synapse. They perform a linear transformation on their input and are usually followed by an Activation layer. The majority of the trainable parameters in a standard feed forward neural network are contained within Dense hidden layers.</p>"},{"location":"neural-network/hidden-layers/dense.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 neurons int The number of nodes in the layer. 2 l2Penalty 0.0 float The amount of L2 regularization applied to the weights. 3 bias true bool Should the layer include a bias parameter? 4 weightInitializer He Initializer The initializer of the weight parameter. 5 biasInitializer Constant Initializer The initializer of the bias parameter."},{"location":"neural-network/hidden-layers/dense.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Layers\\Dense;\nuse Rubix\\ML\\NeuralNet\\Initializers\\He;\nuse Rubix\\ML\\NeuralNet\\Initializers\\Constant;\n\n$layer = new Dense(100, 1e-4, true, new He(), new Constant(0.0));\n</code></pre>"},{"location":"neural-network/hidden-layers/dropout.html","title":"Dropout","text":"<p>[source]</p>"},{"location":"neural-network/hidden-layers/dropout.html#dropout","title":"Dropout","text":"<p>Dropout is a regularization technique to reduce overfitting in neural networks by preventing complex co-adaptations on training data. It works by temporarily disabling output nodes during each training pass. It also acts as an efficient way of performing model averaging with the parameters of neural networks.</p>"},{"location":"neural-network/hidden-layers/dropout.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 ratio 0.5 float The ratio of nodes that are dropped during each training pass."},{"location":"neural-network/hidden-layers/dropout.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Layers\\Dropout;\n\n$layer = new Dropout(0.2);\n</code></pre>"},{"location":"neural-network/hidden-layers/dropout.html#references","title":"References","text":"<ol> <li> <p>N. Srivastava et al. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/hidden-layers/noise.html","title":"Noise","text":"<p>[source]</p>"},{"location":"neural-network/hidden-layers/noise.html#noise","title":"Noise","text":"<p>This layer adds random Gaussian noise to the inputs with a user-defined standard deviation. Noise added to neural network activations acts as a regularizer by indirectly adding a penalty to the weights through the cost function in the output layer.</p>"},{"location":"neural-network/hidden-layers/noise.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 stddev 0.1 float The standard deviation of the Gaussian noise added to the inputs."},{"location":"neural-network/hidden-layers/noise.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Layers\\Noise;\n\n$layer = new Noise(1e-3);\n</code></pre>"},{"location":"neural-network/hidden-layers/noise.html#references","title":"References","text":"<ol> <li> <p>C. Gulcehre et al. (2016). Noisy Activation Functions.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/hidden-layers/prelu.html","title":"PReLU","text":"<p>[source]</p>"},{"location":"neural-network/hidden-layers/prelu.html#prelu","title":"PReLU","text":"<p>Parametric Rectified Linear Units are leaky rectifiers whose leakage coefficient is learned during training. Unlike standard Leaky ReLUs whose leakage remains constant, PReLU layers can adjust the leakage to better suite the model on a per node basis.</p> \\[ {\\displaystyle PReLU = {\\begin{cases}\\alpha x&amp;{\\text{if }}x&lt;0\\\\x&amp;{\\text{if }}x\\geq 0\\end{cases}}} \\]"},{"location":"neural-network/hidden-layers/prelu.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 initializer Constant Initializer The initializer of the leakage parameter."},{"location":"neural-network/hidden-layers/prelu.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Layers\\PReLU;\nuse Rubix\\ML\\NeuralNet\\Initializers\\Normal;\n\n$layer = new PReLU(new Normal(0.5));\n</code></pre>"},{"location":"neural-network/hidden-layers/prelu.html#references","title":"References","text":"<ol> <li> <p>K. He et al. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/hidden-layers/swish.html","title":"Swish","text":"<p>[source]</p>"},{"location":"neural-network/hidden-layers/swish.html#swish","title":"Swish","text":"<p>Swish is a parametric activation layer that utilizes smooth rectified activation functions. The trainable beta parameter allows each activation function in the layer to tailor its output to the training set by interpolating between the linear function and ReLU.</p>"},{"location":"neural-network/hidden-layers/swish.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 initializer Constant Initializer The initializer of the beta parameter."},{"location":"neural-network/hidden-layers/swish.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Layers\\Swish;\nuse Rubix\\ML\\NeuralNet\\Initializers\\Constant;\n\n$layer = new Swish(new Constant(1.0));\n</code></pre>"},{"location":"neural-network/hidden-layers/swish.html#references","title":"References","text":"<ol> <li> <p>P. Ramachandran er al. (2017). Swish: A Self-gated Activation Function.\u00a0\u21a9</p> </li> <li> <p>P. Ramachandran et al. (2017). Searching for Activation Functions.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/initializers/constant.html","title":"Constant","text":"<p>[source]</p>"},{"location":"neural-network/initializers/constant.html#constant","title":"Constant","text":"<p>Initialize the parameter to a user-specified constant value.</p>"},{"location":"neural-network/initializers/constant.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 value 0.0 float The value to initialize the parameter to."},{"location":"neural-network/initializers/constant.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Initializers\\Constant;\n\n$initializer = new Constant(1.0);\n</code></pre>"},{"location":"neural-network/initializers/he.html","title":"He","text":"<p>[source]</p>"},{"location":"neural-network/initializers/he.html#he","title":"He","text":"<p>The He initializer was designed to initialize parameters that feed into rectified Activation layers such as those employing ReLU, Leaky ReLU, or ELU. It draws values from a uniform distribution with limits defined as +/- (6 / (fanIn + fanOut)) ** (1. / sqrt(2)).</p>"},{"location":"neural-network/initializers/he.html#parameters","title":"Parameters","text":"<p>This initializer does not have any parameters.</p>"},{"location":"neural-network/initializers/he.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Initializers\\He;\n\n$initializer = new He();\n</code></pre>"},{"location":"neural-network/initializers/he.html#references","title":"References","text":"<ol> <li> <p>K. He et al. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/initializers/lecun.html","title":"LeCun","text":"<p>[source]</p>"},{"location":"neural-network/initializers/lecun.html#le-cun","title":"Le Cun","text":"<p>Proposed by Yan Le Cun in a paper in 1998, this initializer was one of the first published attempts to control the variance of activations between layers through weight initialization. It remains a good default choice for many hidden layer configurations.</p>"},{"location":"neural-network/initializers/lecun.html#parameters","title":"Parameters","text":"<p>This initializer does not have any parameters.</p>"},{"location":"neural-network/initializers/lecun.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Initializers\\LeCun;\n\n$initializer = new LeCun();\n</code></pre>"},{"location":"neural-network/initializers/lecun.html#references","title":"References","text":"<ol> <li> <p>Y. Le Cun et al. (1998). Efficient Backprop.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/initializers/normal.html","title":"Normal","text":"<p>[source]</p>"},{"location":"neural-network/initializers/normal.html#normal","title":"Normal","text":"<p>Generates a random weight matrix from a Gaussian distribution with user-specified standard deviation.</p>"},{"location":"neural-network/initializers/normal.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 stddev 0.05 float The standard deviation of the distribution to sample from."},{"location":"neural-network/initializers/normal.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Initializers\\Normal;\n\n$initializer = new Normal(0.1);\n</code></pre>"},{"location":"neural-network/initializers/uniform.html","title":"Uniform","text":"<p>[source]</p>"},{"location":"neural-network/initializers/uniform.html#uniform","title":"Uniform","text":"<p>Generates a random uniform distribution centered at 0 and bounded at both ends by the parameter beta.</p>"},{"location":"neural-network/initializers/uniform.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 beta 0.05 float The upper and lower bound of the distribution."},{"location":"neural-network/initializers/uniform.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Initializers\\Uniform;\n\n$initializer = new Uniform(1e-3);\n</code></pre>"},{"location":"neural-network/initializers/xavier-1.html","title":"Xavier 1","text":"<p>[source]</p>"},{"location":"neural-network/initializers/xavier-1.html#xavier-1","title":"Xavier 1","text":"<p>The Xavier 1 initializer draws from a uniform distribution [-limit, limit] where limit is equal to sqrt(6 / (fanIn + fanOut)). This initializer is best suited for layers that feed into an activation layer that outputs a value between 0 and 1 such as Softmax or Sigmoid.</p>"},{"location":"neural-network/initializers/xavier-1.html#parameters","title":"Parameters","text":"<p>This initializer does not have any parameters.</p>"},{"location":"neural-network/initializers/xavier-1.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Initializers\\Xavier1;\n\n$initializer = new Xavier1();\n</code></pre>"},{"location":"neural-network/initializers/xavier-1.html#references","title":"References","text":"<ol> <li> <p>X. Glorot et al. (2010). Understanding the Difficulty of Training Deep Feedforward Neural Networks.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/initializers/xavier-2.html","title":"Xavier 2","text":"<p>[source]</p>"},{"location":"neural-network/initializers/xavier-2.html#xavier-2","title":"Xavier 2","text":"<p>The Xavier 2 initializer draws from a uniform distribution [-limit, limit] where limit is equal to (6 / (fanIn + fanOut)) ** 0.25. This initializer is best suited for layers that feed into an activation layer that outputs values between -1 and 1 such as Hyperbolic Tangent and Softsign.</p>"},{"location":"neural-network/initializers/xavier-2.html#parameters","title":"Parameters","text":"<p>This initializer does not have any parameters.</p>"},{"location":"neural-network/initializers/xavier-2.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Initializers\\Xavier2;\n\n$initializer = new Xavier2();\n</code></pre>"},{"location":"neural-network/initializers/xavier-2.html#references","title":"References","text":"<ol> <li> <p>X. Glorot et al. (2010). Understanding the Difficulty of Training Deep Feedforward Neural Networks.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/optimizers/adagrad.html","title":"AdaGrad","text":"<p>[source]</p>"},{"location":"neural-network/optimizers/adagrad.html#adagrad","title":"AdaGrad","text":"<p>Short for Adaptive Gradient, the AdaGrad Optimizer speeds up the learning of parameters that do not change often and slows down the learning of parameters that do enjoy heavy activity. Due to AdaGrad's infinitely decaying step size, training may be slow or fail to converge using a low learning rate.</p>"},{"location":"neural-network/optimizers/adagrad.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 rate 0.01 float The learning rate that controls the global step size."},{"location":"neural-network/optimizers/adagrad.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Optimizers\\AdaGrad;\n\n$optimizer = new AdaGrad(0.125);\n</code></pre>"},{"location":"neural-network/optimizers/adagrad.html#references","title":"References","text":"<ol> <li> <p>J. Duchi et al. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/optimizers/adam.html","title":"Adam","text":"<p>[source]</p>"},{"location":"neural-network/optimizers/adam.html#adam","title":"Adam","text":"<p>Short for Adaptive Moment Estimation, the Adam Optimizer combines both Momentum and RMS properties. In addition to storing an exponentially decaying average of past squared gradients like RMSprop, Adam also keeps an exponentially decaying average of past gradients, similar to Momentum. Whereas Momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction.</p>"},{"location":"neural-network/optimizers/adam.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 rate 0.001 float The learning rate that controls the global step size. 2 momentumDecay 0.1 float The decay rate of the accumulated velocity. 3 normDecay 0.001 float The decay rate of the rms property."},{"location":"neural-network/optimizers/adam.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Optimizers\\Adam;\n\n$optimizer = new Adam(0.0001, 0.1, 0.001);\n</code></pre>"},{"location":"neural-network/optimizers/adam.html#references","title":"References","text":"<ol> <li> <p>D. P. Kingma et al. (2014). Adam: A Method for Stochastic Optimization.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/optimizers/adamax.html","title":"AdaMax","text":"<p>[source]</p>"},{"location":"neural-network/optimizers/adamax.html#adamax","title":"AdaMax","text":"<p>A version of the Adam optimizer that replaces the RMS property with the infinity norm of the past gradients. As such, AdaMax is generally more suitable for sparse parameter updates and noisy gradients.</p>"},{"location":"neural-network/optimizers/adamax.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 rate 0.001 float The learning rate that controls the global step size. 2 momentumDecay 0.1 float The decay rate of the accumulated velocity. 3 normDecay 0.001 float The decay rate of the infinity norm."},{"location":"neural-network/optimizers/adamax.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Optimizers\\AdaMax;\n\n$optimizer = new AdaMax(0.0001, 0.1, 0.001);\n</code></pre>"},{"location":"neural-network/optimizers/adamax.html#references","title":"References","text":"<ol> <li> <p>D. P. Kingma et al. (2014). Adam: A Method for Stochastic Optimization.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/optimizers/cyclical.html","title":"Cyclical","text":"<p>[source]</p>"},{"location":"neural-network/optimizers/cyclical.html#cyclical","title":"Cyclical","text":"<p>The Cyclical optimizer uses a global learning rate that cycles between the lower and upper bound over a designated period while also decaying the upper bound by a factor at each step. Cyclical learning rates have been shown to help escape bad local minima and saddle points of the gradient.</p>"},{"location":"neural-network/optimizers/cyclical.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 lower 0.001 float The lower bound on the learning rate. 2 upper 0.006 float The upper bound on the learning rate. 3 steps 100 int The number of steps in every half cycle. 4 decay 0.99994 float The exponential decay factor to decrease the learning rate by every step."},{"location":"neural-network/optimizers/cyclical.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Optimizers\\Cyclical;\n\n$optimizer = new Cyclical(0.001, 0.005, 1000);\n</code></pre>"},{"location":"neural-network/optimizers/cyclical.html#references","title":"References","text":"<ol> <li> <p>L. N. Smith. (2017). Cyclical Learning Rates for Training Neural Networks.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/optimizers/momentum.html","title":"Momentum","text":"<p>[source]</p>"},{"location":"neural-network/optimizers/momentum.html#momentum","title":"Momentum","text":"<p>Momentum accelerates each update step by accumulating velocity from past updates and adding a factor of the previous velocity to the current step. Momentum can help speed up training and escape bad local minima when compared with Stochastic Gradient Descent.</p>"},{"location":"neural-network/optimizers/momentum.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 rate 0.001 float The learning rate that controls the global step size. 2 decay 0.1 float The decay rate of the accumulated velocity. 3 lookahead false bool Should we employ Nesterov's lookahead (NAG) when updating the parameters?"},{"location":"neural-network/optimizers/momentum.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Optimizers\\Momentum;\n\n$optimizer = new Momentum(0.01, 0.1, true);\n</code></pre>"},{"location":"neural-network/optimizers/momentum.html#references","title":"References","text":"<ol> <li> <p>D. E. Rumelhart et al. (1988). Learning representations by back-propagating errors.\u00a0\u21a9</p> </li> <li> <p>I. Sutskever et al. (2013). On the importance of initialization and momentum in deep learning.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/optimizers/rms-prop.html","title":"RMS Prop","text":"<p>[source]</p>"},{"location":"neural-network/optimizers/rms-prop.html#rms-prop","title":"RMS Prop","text":"<p>An adaptive gradient technique that divides the current gradient over a rolling window of the magnitudes of recent gradients. Unlike AdaGrad, RMS Prop does not suffer from an infinitely decaying step size.</p>"},{"location":"neural-network/optimizers/rms-prop.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 rate 0.001 float The learning rate that controls the global step size. 2 decay 0.1 float The decay rate of the rms property."},{"location":"neural-network/optimizers/rms-prop.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Optimizers\\RMSProp;\n\n$optimizer = new RMSProp(0.01, 0.1);\n</code></pre>"},{"location":"neural-network/optimizers/rms-prop.html#references","title":"References","text":"<ol> <li> <p>T. Tieleman et al. (2012). Lecture 6e rmsprop: Divide the gradient by a running average of its recent magnitude.\u00a0\u21a9</p> </li> </ol>"},{"location":"neural-network/optimizers/step-decay.html","title":"Step Decay","text":"<p>[source]</p>"},{"location":"neural-network/optimizers/step-decay.html#step-decay","title":"Step Decay","text":"<p>A learning rate decay optimizer that reduces the global learning rate by a factor whenever it reaches a new floor. The number of steps needed to reach a new floor is defined by the steps hyper-parameter.</p>"},{"location":"neural-network/optimizers/step-decay.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 rate 0.01 float The learning rate that controls the global step size. 2 steps 100 int The size of every floor in steps. i.e. the number of steps to take before applying another factor of decay. 3 decay 1e-3 float The factor to decrease the learning rate at each floor."},{"location":"neural-network/optimizers/step-decay.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Optimizers\\StepDecay;\n\n$optimizer = new StepDecay(0.1, 50, 1e-3);\n</code></pre>"},{"location":"neural-network/optimizers/stochastic.html","title":"Stochastic","text":"<p>[source]</p>"},{"location":"neural-network/optimizers/stochastic.html#stochastic","title":"Stochastic","text":"<p>A constant learning rate optimizer based on vanilla Stochastic Gradient Descent.</p>"},{"location":"neural-network/optimizers/stochastic.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 rate 0.01 float The learning rate that controls the global step size."},{"location":"neural-network/optimizers/stochastic.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\NeuralNet\\Optimizers\\Stochastic;\n\n$optimizer = new Stochastic(0.01);\n</code></pre>"},{"location":"persisters/api.html","title":"Persisters","text":"<p>Persisters are responsible for persisting Encoding objects to storage and are also used by the Persistent Model meta-estimator to save and restore models that have been serialized.</p>"},{"location":"persisters/api.html#save","title":"Save","text":"<p>To save an encoding: <pre><code>public save(Encoding $encoding) : void\n</code></pre></p> <pre><code>$persister-&gt;save($encoding);\n</code></pre>"},{"location":"persisters/api.html#load","title":"Load","text":"<p>To load an encoding from persistence: <pre><code>public load() : Encoding\n</code></pre></p> <pre><code>$encoding = $persister-&gt;load();\n</code></pre>"},{"location":"persisters/filesystem.html","title":"Filesystem","text":"<p>[source]</p>"},{"location":"persisters/filesystem.html#filesystem","title":"Filesystem","text":"<p>Filesystems are local or remote storage drives that are organized by files and folders. The Filesystem persister saves models to a file at a given path and can automatically keep a history of past saved models.</p>"},{"location":"persisters/filesystem.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 path string The path to the model file on the filesystem. 2 history false bool Should we keep a history of past saves?"},{"location":"persisters/filesystem.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Persisters\\Filesystem;\nuse Rubix\\ML\\Serializers\\RBX;\nuse Rubix\\ML\\Classifiers\\KNearestNeighbors;\nuse Rubix\\ML\\Kernels\\Distance\\Manhattan;\n\n$persistable = new KNearestNeighbors(3, false, new Manhattan());\n\n$persister = new Filesystem('/path/to/example.rbx', true);\n\n$serializer = new RBX(6);\n\n$encoding = $serializer-&gt;serialize($persistable);\n\n$persister-&gt;save($encoding);\n</code></pre>"},{"location":"persisters/filesystem.html#example_1","title":"Example","text":"<pre><code>use Rubix\\ML\\Persisters\\Filesystem;\nuse Rubix\\ML\\Serializers\\RBX;\n\n$persister = new Filesystem('/path/to/example.rbx', true);\n\n$encoding = $persister-&gt;load();\n\n$serializer = new RBX(6);\n\n$persistable = $serializer-&gt;deserialize($encoding);\n</code></pre>"},{"location":"persisters/filesystem.html#additional-methods","title":"Additional Methods","text":"<p>This persister does not have any additional methods.</p>"},{"location":"regressors/adaline.html","title":"Adaline","text":"<p>[source]</p>"},{"location":"regressors/adaline.html#adaline","title":"Adaline","text":"<p>Adaptive Linear Neuron is a single layer feed-forward neural network with a continuous linear output neuron suitable for regression tasks. Training is equivalent to solving L2 regularized linear regression (Ridge) online using Mini Batch Gradient Descent.</p> <p>Interfaces: Estimator, Learner, Online, Ranks Features, Verbose, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"regressors/adaline.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 batchSize 128 int The number of training samples to process at a time. 2 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 3 l2Penalty 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the training loss to wait before considering an early stop. 7 costFn LeastSquares RegressionLoss The function that computes the loss associated with an erroneous activation during training."},{"location":"regressors/adaline.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Regressors\\Adaline;\nuse Rubix\\ML\\NeuralNet\\Optimizers\\Adam;\nuse Rubix\\ML\\NeuralNet\\CostFunctions\\HuberLoss;\n\n$estimator = new Adaline(256, new Adam(0.001), 1e-4, 500, 1e-6, 5, new HuberLoss(2.5));\n</code></pre>"},{"location":"regressors/adaline.html#additional-methods","title":"Additional Methods","text":"<p>Return an iterable progress table with the steps from the last training session: <pre><code>public steps() : iterable\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('progress.csv', true);\n\n$extractor-&gt;export($estimator-&gt;steps());\n</code></pre> <p>Return the loss for each epoch from the last training session: <pre><code>public losses() : float[]|null\n</code></pre></p> <p>Return the underlying neural network instance or <code>null</code> if untrained: <pre><code>public network() : Network|null\n</code></pre></p>"},{"location":"regressors/adaline.html#references","title":"References","text":"<ol> <li> <p>B. Widrow. (1960). An Adaptive \"Adaline\" Neuron Using Chemical \"Memistors\".\u00a0\u21a9</p> </li> </ol>"},{"location":"regressors/extra-tree-regressor.html","title":"Extra Tree Regressor","text":"<p>[source]</p>"},{"location":"regressors/extra-tree-regressor.html#extra-tree-regressor","title":"Extra Tree Regressor","text":"<p>Extremely Randomized Regression Trees differ from standard Regression Trees in that they choose candidate splits at random rather than searching the entire feature column for the best value to split on. Extra Trees are also faster to build and their predictions have higher variance than a regular decision tree regressor.</p> <p>Interfaces: Estimator, Learner, Ranks Features, Persistable</p> <p>Data Type Compatibility: Categorical, Continuous</p>"},{"location":"regressors/extra-tree-regressor.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 maxHeight PHP_INT_MAX int The maximum height of the tree. 2 maxLeafSize 3 int The max number of samples that a leaf node can contain. 3 minPurityIncrease 1e-7 float The minimum increase in purity necessary to continue splitting a subtree. 4 maxFeatures Auto int The max number of feature columns to consider when determining a best split."},{"location":"regressors/extra-tree-regressor.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Regressors\\ExtraTreeRegressor;\n\n$estimator = new ExtraTreeRegressor(30, 5, 0.05, null);\n</code></pre>"},{"location":"regressors/extra-tree-regressor.html#additional-methods","title":"Additional Methods","text":"<p>Export a Graphviz \"dot\" encoding of the decision tree structure. <pre><code>public exportGraphviz() : Encoding\n</code></pre></p> <p>Return the number of levels in the tree. <pre><code>public height() : ?int\n</code></pre></p> <p>Return a factor that quantifies the skewness of the distribution of nodes in the tree. <pre><code>public balance() : ?int\n</code></pre></p>"},{"location":"regressors/extra-tree-regressor.html#references","title":"References","text":"<ol> <li> <p>P. Geurts et al. (2005). Extremely Randomized Trees.\u00a0\u21a9</p> </li> </ol>"},{"location":"regressors/gradient-boost.html","title":"Gradient Boost","text":"<p>[source]</p>"},{"location":"regressors/gradient-boost.html#gradient-boost","title":"Gradient Boost","text":"<p>Gradient Boost (GBM) is a stage-wise additive ensemble that uses a Gradient Descent boosting scheme for training boosters (Decision Trees) to correct the error residuals of a base learner.</p> <p>Note</p> <p>The default booster is a Regression Tree with a max height of 3.</p> <p>Note</p> <p>Gradient Boost utilizes progress monitoring via an internal validation set for snapshotting and early stopping. If there are not enough training samples to build an internal validation set given the user-specified holdout ratio then training will proceed with progress monitoring disabled.</p> <p>Interfaces: Estimator, Learner, Verbose, Ranks Features, Persistable</p> <p>Data Type Compatibility: Categorical and Continuous</p>"},{"location":"regressors/gradient-boost.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 booster RegressionTree Learner The regressor used to up the error residuals of the base learner. 2 rate 0.1 float The learning rate of the ensemble i.e. the shrinkage applied to each step. 3 ratio 0.5 float The ratio of samples to subsample from the training set to train each booster. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 7 holdOut 0.1 float The proportion of training samples to use for internal validation. Set to 0 to disable. 8 metric RMSE Metric The metric used to score the generalization performance of the model during training."},{"location":"regressors/gradient-boost.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Regressors\\GradientBoost;\nuse Rubix\\ML\\Regressors\\RegressionTree;\nuse Rubix\\ML\\CrossValidation\\Metrics\\SMAPE;\n\n$estimator = new GradientBoost(new RegressionTree(3), 0.1, 0.8, 1000, 1e-4, 10, 0.1, new SMAPE());\n</code></pre>"},{"location":"regressors/gradient-boost.html#additional-methods","title":"Additional Methods","text":"<p>Return an iterable progress table with the steps from the last training session: <pre><code>public steps() : iterable\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('progress.csv', true);\n\n$extractor-&gt;export($estimator-&gt;steps());\n</code></pre> <p>Return the validation score for each epoch from the last training session: <pre><code>public scores() : float[]|null\n</code></pre></p> <p>Return the loss for each epoch from the last training session: <pre><code>public losses() : float[]|null\n</code></pre></p>"},{"location":"regressors/gradient-boost.html#references","title":"References","text":"<ol> <li> <p>J. H. Friedman. (2001). Greedy Function Approximation: A Gradient Boosting Machine.\u00a0\u21a9</p> </li> <li> <p>J. H. Friedman. (1999). Stochastic Gradient Boosting.\u00a0\u21a9</p> </li> <li> <p>Y. Wei. et al. (2017). Early stopping for kernel boosting algorithms: A general analysis with localized complexities.\u00a0\u21a9</p> </li> <li> <p>G. Ke et al. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree.\u00a0\u21a9</p> </li> </ol>"},{"location":"regressors/kd-neighbors-regressor.html","title":"K-d Neighbors Regressor","text":"<p>[source]</p>"},{"location":"regressors/kd-neighbors-regressor.html#k-d-neighbors-regressor","title":"K-d Neighbors Regressor","text":"<p>A fast implementation of KNN Regressor using a spatially-aware binary tree for nearest neighbors search. K-d Neighbors Regressor works by locating the neighborhood of a sample via binary search and then does a brute force search only on the samples close to or within the neighborhood of the unknown sample. The main advantage of K-d Neighbors over brute force KNN is inference speed, however, it cannot be partially trained.</p> <p>Interfaces: Estimator, Learner, Persistable</p> <p>Data Type Compatibility: Depends on distance kernel</p>"},{"location":"regressors/kd-neighbors-regressor.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 k 5 int The number of nearest neighbors to consider when making a prediction. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 tree KDTree Spatial The spatial tree used to run nearest neighbor searches."},{"location":"regressors/kd-neighbors-regressor.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Regressors\\KDNeighborsRegressor;\nuse Rubix\\ML\\Graph\\Trees\\BallTree;\n\n$estimator = new KDNeighborsRegressor(20, true, new BallTree(50));\n</code></pre>"},{"location":"regressors/kd-neighbors-regressor.html#additional-methods","title":"Additional Methods","text":"<p>Return the base spatial tree instance: <pre><code>public tree() : Spatial\n</code></pre></p>"},{"location":"regressors/knn-regressor.html","title":"KNN Regressor","text":"<p>[source]</p>"},{"location":"regressors/knn-regressor.html#knn-regressor","title":"KNN Regressor","text":"<p>K Nearest Neighbors (KNN) is a brute-force distance-based learner that locates the k nearest training samples from the training set and averages their labels to make a prediction. K Nearest Neighbors (KNN) is considered a lazy learner because it performs most of its computation at inference time.</p> <p>Note</p> <p>For a faster spatial tree-accelerated version of KNN, see KD Neighbors Regressor.</p> <p>Interfaces: Estimator, Learner, Online, Persistable</p> <p>Data Type Compatibility: Depends on distance kernel</p>"},{"location":"regressors/knn-regressor.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 k 5 int The number of nearest neighbors to consider when making a prediction. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 kernel Euclidean Distance The distance kernel used to compute the distance between sample points."},{"location":"regressors/knn-regressor.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Regressors\\KNNRegressor;\nuse Rubix\\ML\\Kernels\\Distance\\SafeEuclidean;\n\n$estimator = new KNNRegressor(5, false, new SafeEuclidean());\n</code></pre>"},{"location":"regressors/knn-regressor.html#additional-methods","title":"Additional Methods","text":"<p>This estimator does not have any additional methods.</p>"},{"location":"regressors/mlp-regressor.html","title":"MLP Regressor","text":"<p>[source]</p>"},{"location":"regressors/mlp-regressor.html#mlp-regressor","title":"MLP Regressor","text":"<p>A multilayer feed-forward neural network with a continuous output layer suitable for regression problems. The Multilayer Perceptron regressor is able to handle complex non-linear regression problems by forming higher-order representations of the input features using intermediate user-defined hidden layers. The MLP also has network snapshotting and progress monitoring to ensure that the model achieves the highest validation score per a given training time budget.</p> <p>Note</p> <p>If there are not enough training samples to build an internal validation set with the user-specified holdout ratio then progress monitoring will be disabled.</p> <p>Interfaces: Estimator, Learner, Online, Verbose, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"regressors/mlp-regressor.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 hidden array An array composing the user-specified hidden layers of the network in order. 2 batchSize 128 int The number of training samples to process at a time. 3 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 4 l2Penalty 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 5 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 6 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 7 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 8 holdOut 0.1 float The proportion of training samples to use for internal validation. Set to 0 to disable. 9 costFn LeastSquares RegressionLoss The function that computes the loss associated with an erroneous activation during training. 10 metric RMSE Metric The metric used to score the generalization performance of the model during training."},{"location":"regressors/mlp-regressor.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Regressors\\MLPRegressor;\nuse Rubix\\ML\\NeuralNet\\CostFunctions\\LeastSquares;\nuse Rubix\\ML\\NeuralNet\\Layers\\Dense;\nuse Rubix\\ML\\NeuralNet\\Layers\\Activation;\nuse Rubix\\ML\\NeuralNet\\ActivationFunctions\\ReLU;\nuse Rubix\\ML\\NeuralNet\\Optimizers\\RMSProp;\nuse Rubix\\ML\\CrossValidation\\Metrics\\RSquared;\n\n$estimator = new MLPRegressor([\n    new Dense(100),\n    new Activation(new ReLU()),\n    new Dense(100),\n    new Activation(new ReLU()),\n    new Dense(50),\n    new Activation(new ReLU()),\n    new Dense(50),\n    new Activation(new ReLU()),\n], 128, new RMSProp(0.001), 1e-3, 100, 1e-5, 3, 0.1, new LeastSquares(), new RSquared());\n</code></pre>"},{"location":"regressors/mlp-regressor.html#additional-methods","title":"Additional Methods","text":"<p>Return an iterable progress table with the steps from the last training session: <pre><code>public steps() : iterable\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('progress.csv', true);\n\n$extractor-&gt;export($estimator-&gt;steps());\n</code></pre> <p>Return the validation score for each epoch from the last training session: <pre><code>public scores() : float[]|null\n</code></pre></p> <p>Return the loss for each epoch from the last training session: <pre><code>public losses() : float[]|null\n</code></pre></p> <p>Returns the underlying neural network instance or <code>null</code> if untrained: <pre><code>public network() : Network|null\n</code></pre></p> <p>Export a Graphviz \"dot\" encoding of the neural network architecture. <pre><code>public exportGraphviz() : Encoding\n</code></pre></p> <pre><code>use Rubix\\ML\\Helpers\\Graphviz;\nuse Rubix\\ML\\Persisters\\Filesystem;\n\n$dot = $estimator-&gt;exportGraphviz();\n\nGraphviz::dotToImage($dot)-&gt;saveTo(new Filesystem('network.png'));\n</code></pre> <p></p>"},{"location":"regressors/mlp-regressor.html#references","title":"References","text":"<ol> <li> <p>G. E. Hinton. (1989). Connectionist learning procedures.\u00a0\u21a9</p> </li> <li> <p>L. Prechelt. (1997). Early Stopping - but when?\u00a0\u21a9</p> </li> </ol>"},{"location":"regressors/radius-neighbors-regressor.html","title":"Radius Neighbors Regressor","text":"<p>[source]</p>"},{"location":"regressors/radius-neighbors-regressor.html#radius-neighbors-regressor","title":"Radius Neighbors Regressor","text":"<p>This is the regressor version of Radius Neighbors implementing a binary spatial tree under the hood for fast radius queries. The prediction is a weighted average of each label from the training set that is within a fixed user-defined radius.</p> <p>Note: Samples with 0 neighbors within radius will be predicted NaN.</p> <p>Interfaces: Estimator, Learner, Persistable</p> <p>Data Type Compatibility: Depends on distance kernel</p>"},{"location":"regressors/radius-neighbors-regressor.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 radius 1.0 float The radius within which points are considered neighbors. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 tree BallTree Spatial The spatial tree used to run range searches."},{"location":"regressors/radius-neighbors-regressor.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Regressors\\RadiusNeighborsRegressor;\nuse Rubix\\ML\\Graph\\Trees\\BallTree;\nuse Rubix\\ML\\Kernels\\Distance\\Diagonal;\n\n$estimator = new RadiusNeighborsRegressor(0.5, false, new BallTree(30, new Diagonal()));\n</code></pre>"},{"location":"regressors/radius-neighbors-regressor.html#additional-methods","title":"Additional Methods","text":"<p>Return the base spatial tree instance: <pre><code>public tree() : Spatial\n</code></pre></p>"},{"location":"regressors/regression-tree.html","title":"Regression Tree","text":"<p>[source]</p>"},{"location":"regressors/regression-tree.html#regression-tree","title":"Regression Tree","text":"<p>A decision tree based on the CART (Classification and Regression Tree) learning algorithm that performs greedy splitting by minimizing the variance of the labels at each node split. Regression Trees can be used on their own or as the booster in algorithms such as Gradient Boost.</p> <p>Interfaces: Estimator, Learner, Ranks Features, Persistable</p> <p>Data Type Compatibility: Categorical, Continuous</p>"},{"location":"regressors/regression-tree.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 maxHeight PHP_INT_MAX int The maximum height of the tree. 2 maxLeafSize 3 int The max number of samples that a leaf node can contain. 3 minPurityIncrease 1e-7 float The minimum increase in purity necessary to continue splitting a subtree. 4 maxFeatures Auto int The max number of feature columns to consider when determining a best split. 5 maxBins Auto int The maximum number of bins to consider when determining a split with a continuous feature as the split point."},{"location":"regressors/regression-tree.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Regressors\\RegressionTree;\n\n$estimator = new RegressionTree(20, 2, 1e-3, 10, null);\n</code></pre>"},{"location":"regressors/regression-tree.html#additional-methods","title":"Additional Methods","text":"<p>Export a Graphviz \"dot\" encoding of the decision tree structure. <pre><code>public exportGraphviz() : Encoding\n</code></pre></p> <pre><code>use Rubix\\ML\\Helpers\\Graphviz;\nuse Rubix\\ML\\Persisters\\Filesystem;\n\n$dot = $estimator-&gt;exportGraphviz();\n\nGraphviz::dotToImage($dot)-&gt;saveTo(new Filesystem('tree.png'));\n</code></pre> <p>Return the number of levels in the tree. <pre><code>public height() : ?int\n</code></pre></p> <p>Return a factor that quantifies the skewness of the distribution of nodes in the tree. <pre><code>public balance() : ?int\n</code></pre></p>"},{"location":"regressors/regression-tree.html#references","title":"References:","text":"<ol> <li> <p>W. Y. Loh. (2011). Classification and Regression Trees.\u00a0\u21a9</p> </li> <li> <p>K. Alsabti. et al. (1998). CLOUDS: A Decision Tree Classifier for Large Datasets.\u00a0\u21a9</p> </li> </ol>"},{"location":"regressors/ridge.html","title":"Ridge","text":"<p>[source]</p>"},{"location":"regressors/ridge.html#ridge","title":"Ridge","text":"<p>L2 regularized linear regression solved using a closed-form solution. The addition of regularization, controlled by the alpha hyper-parameter, makes Ridge less likely to overfit the training data than ordinary least squares (OLS).</p> <p>Interfaces: Estimator, Learner, Ranks Features, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"regressors/ridge.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 l2Penalty 1.0 float The strength of the L2 regularization penalty."},{"location":"regressors/ridge.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Regressors\\Ridge;\n\n$estimator = new Ridge(2.0);\n</code></pre>"},{"location":"regressors/ridge.html#additional-methods","title":"Additional Methods","text":"<p>Return the weights of features in the decision function. <pre><code>public coefficients() : array|null\n</code></pre></p> <p>Return the bias added to the decision function. <pre><code>public bias() : float|null\n</code></pre></p>"},{"location":"regressors/svr.html","title":"SVR","text":"<p>[source]</p>"},{"location":"regressors/svr.html#svr","title":"SVR","text":"<p>The Support Vector Machine Regressor (SVR) is a maximum margin algorithm for the purposes of regression. Similarly to the SVC, the model produced by SVR depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction given by parameter epsilon. Thus, the value of epsilon defines a margin of tolerance where no penalty is given to errors.</p> <p>Note</p> <p>This estimator requires the SVM extension which uses the libsvm engine under the hood.</p> <p>Interfaces: Estimator, Learner</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"regressors/svr.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 c 1.0 float The parameter that defines the width of the margin used to separate the classes. 2 epsilon 0.1 float Specifies the margin within which no penalty is associated in the training loss. 3 kernel RBF Kernel The kernel function used to operate in higher dimensions. 4 shrinking true bool Should we use the shrinking heuristic? 5 tolerance 1e-3 float The minimum change in the cost function necessary to continue training. 6 cache size 100.0 float The size of the kernel cache in MB."},{"location":"regressors/svr.html#additional-methods","title":"Additional Methods","text":"<p>Save the model data to the filesystem: <pre><code>public save(string $path) : void\n</code></pre></p> <p>Load the model data from the filesystem: <pre><code>public load(string $path) : void\n</code></pre></p>"},{"location":"regressors/svr.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Regressors\\SVR;\nuse Rubix\\ML\\Kernels\\SVM\\RBF;\n\n$estimator = new SVR(1.0, 0.03, new RBF(), true, 1e-3, 256.0);\n</code></pre>"},{"location":"regressors/svr.html#references","title":"References","text":"<ol> <li> <p>C. Chang et al. (2011). LIBSVM: A library for support vector machines.\u00a0\u21a9</p> </li> <li> <p>A. Smola et al. (2003). A Tutorial on Support Vector Regression.\u00a0\u21a9</p> </li> </ol>"},{"location":"serializers/api.html","title":"Serializers","text":"<p>Serializers take objects that implement the Persistable interface and convert them into blobs of data called encodings. Encodings can then be used to either store an object or to reinstantiate an object from storage.</p>"},{"location":"serializers/api.html#serialize","title":"Serialize","text":"<p>To serialize a persistable object into an encoding: <pre><code>public serialize(Persistable $persistable) : Encoding\n</code></pre></p> <pre><code>$encoding = $serializer-&gt;serialize($persistable);\n</code></pre>"},{"location":"serializers/api.html#deserialize","title":"Deserialize","text":"<p>To deserialize a persistable object from an encoding: <pre><code>public deserialize(Encoding $encoding) : Persistable\n</code></pre></p> <pre><code>$persistable = $serializer-&gt;deserialize($encoding);\n</code></pre>"},{"location":"serializers/gzip-native.html","title":"Gzip Native","text":"<p>[source]</p>"},{"location":"serializers/gzip-native.html#gzip-native","title":"Gzip Native","text":"<p>Gzip Native wraps the native PHP serialization format in an outer compression layer based on the DEFLATE algorithm with a header and CRC32 checksum.</p>"},{"location":"serializers/gzip-native.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 level 6 int The compression level between 0 and 9, 0 meaning no compression."},{"location":"serializers/gzip-native.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Serializers\\GzipNative;\n\n$serializer = new GzipNative(1);\n</code></pre>"},{"location":"serializers/gzip-native.html#references","title":"References","text":"<ol> <li> <p>P. Deutsch. (1996). RFC 1951 - DEFLATE Compressed Data Format Specification version.\u00a0\u21a9</p> </li> </ol>"},{"location":"serializers/native.html","title":"Native","text":"<p>[source]</p>"},{"location":"serializers/native.html#native","title":"Native","text":"<p>The native bytecode format that comes bundled with PHP core.</p>"},{"location":"serializers/native.html#parameters","title":"Parameters","text":"<p>This serializer does not have any parameters.</p>"},{"location":"serializers/native.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Serializers\\Native;\n\n$serializer = new Native();\n</code></pre>"},{"location":"serializers/rbx.html","title":"RBX","text":"<p>[source]</p>"},{"location":"serializers/rbx.html#rbx","title":"RBX","text":"<p>Rubix Object File format (RBX) is a format designed to reliably store and share serialized PHP objects. Based on PHP's native serialization format, RBX adds additional layers of compression, data integrity checks, and class compatibility detection all in one robust format.</p> <p>Note</p> <p>We recommend to use the <code>.rbx</code> file extension when storing RBX-serialized PHP objects.</p>"},{"location":"serializers/rbx.html#parameters","title":"Parameters","text":""},{"location":"serializers/rbx.html#parameters_1","title":"Parameters","text":"# Name Default Type Description 1 level 6 int The compression level between 0 and 9, 0 meaning no compression."},{"location":"serializers/rbx.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Serializers\\RBX;\n\n$serializer = new RBX(6);\n</code></pre>"},{"location":"strategies/constant.html","title":"Constant","text":"<p>[source]'</p>"},{"location":"strategies/constant.html#constant","title":"Constant","text":"<p>Always guess the same value.</p> <p>Data Type: Continuous</p>"},{"location":"strategies/constant.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 value 0.0 float The value to constantly guess."},{"location":"strategies/constant.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Strategies\\Constant;\n\n$strategy = new Constant(0.0);\n</code></pre>"},{"location":"strategies/k-most-frequent.html","title":"K Most Frequent","text":"<p>[source]</p>"},{"location":"strategies/k-most-frequent.html#k-most-frequent","title":"K Most Frequent","text":"<p>This Strategy outputs one of k most frequently occurring classes at random with equal probability.</p> <p>Data Type: Categorical</p>"},{"location":"strategies/k-most-frequent.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 k 1 int The number of most frequent classes to consider."},{"location":"strategies/k-most-frequent.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Strategies\\KMostFrequent;\n\n$strategy = new KMostFrequent(5);\n</code></pre>"},{"location":"strategies/mean.html","title":"Mean","text":"<p>[source]</p>"},{"location":"strategies/mean.html#mean","title":"Mean","text":"<p>This strategy always predicts the mean of the fitted data.</p> <p>Data Type: Continuous</p>"},{"location":"strategies/mean.html#parameters","title":"Parameters","text":"<p>This strategy does not have any parameters.</p>"},{"location":"strategies/mean.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Strategies\\Mean;\n\n$strategy = new Mean();\n</code></pre>"},{"location":"strategies/percentile.html","title":"Percentile","text":"<p>[source]</p>"},{"location":"strategies/percentile.html#blurry-percentile","title":"Blurry Percentile","text":"<p>A strategy that always guesses the p-th percentile of the fitted data.</p> <p>Data Type: Continuous</p>"},{"location":"strategies/percentile.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 p 50.0 float The percentile of the fitted data to use as a guess."},{"location":"strategies/percentile.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Strategies\\Percentile;\n\n$strategy = new Percentile(90.0);\n</code></pre>"},{"location":"strategies/prior.html","title":"Prior","text":"<p>[source]</p>"},{"location":"strategies/prior.html#prior","title":"Prior","text":"<p>A strategy where the probability of guessing a class is equal to the class's prior probability.</p> <p>Data Type: Categorical</p>"},{"location":"strategies/prior.html#parameters","title":"Parameters","text":"<p>This strategy does not have any parameters.</p>"},{"location":"strategies/prior.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Strategies\\Prior;\n\n$strategy = new Prior();\n</code></pre>"},{"location":"strategies/wild-guess.html","title":"Wild Guess","text":"<p>[source]</p>"},{"location":"strategies/wild-guess.html#wild-guess","title":"Wild Guess","text":"<p>Guess a random number somewhere between the minimum and maximum computed by fitting a collection of values.</p> <p>Data Type: Continuous</p>"},{"location":"strategies/wild-guess.html#parameters","title":"Parameters","text":"<p>This strategy does not have any parameters.</p>"},{"location":"strategies/wild-guess.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Strategies\\WildGuess;\n\n$strategy = new WildGuess();\n</code></pre>"},{"location":"tokenizers/k-skip-n-gram.html","title":"K-Skip-N-Gram","text":"<p>[source]</p>"},{"location":"tokenizers/k-skip-n-gram.html#k-skip-n-gram","title":"K-Skip-N-Gram","text":"<p>K-skip-n-grams are a technique similar to n-grams, whereby n-grams are formed but in addition to allowing adjacent sequences of words, the next k words will be skipped forming n-grams of the new forward looking sequences. The tokenizer outputs tokens ranging from min to max number of words per token.</p>"},{"location":"tokenizers/k-skip-n-gram.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 min 2 int The minimum number of words in a single token. 2 max 2 int The maximum number of words in a single token. 3 skip 2 int The number of words to skip over to form new sequences."},{"location":"tokenizers/k-skip-n-gram.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Tokenizers\\KSkipNGram;\n\n$tokenizer = new KSkipNGram(2, 3, 2);\n</code></pre>"},{"location":"tokenizers/n-gram.html","title":"N-Gram","text":"<p>[source]</p>"},{"location":"tokenizers/n-gram.html#n-gram","title":"N-gram","text":"<p>N-grams are sequences of n-words of a given string. The N-gram tokenizer outputs tokens of contiguous words ranging from min to max number of words per token.</p>"},{"location":"tokenizers/n-gram.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 min 2 int The minimum number of contiguous words to a token. 2 max 2 int The maximum number of contiguous words to a token."},{"location":"tokenizers/n-gram.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Tokenizers\\NGram;\n\n$tokenizer = new NGram(1, 3);\n</code></pre>"},{"location":"tokenizers/sentence.html","title":"Sentence","text":"<p>[source]</p>"},{"location":"tokenizers/sentence.html#word-tokenizer","title":"Word Tokenizer","text":"<p>This tokenizer matches sentences starting with a letter and ending with a punctuation mark.</p>"},{"location":"tokenizers/sentence.html#parameters","title":"Parameters","text":"<p>This tokenizer does not have any parameters.</p>"},{"location":"tokenizers/sentence.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Tokenizers\\Sentence;\n\n$tokenizer = new Sentence();\n</code></pre>"},{"location":"tokenizers/whitespace.html","title":"Whitespace","text":"<p>[source]</p>"},{"location":"tokenizers/whitespace.html#whitespace","title":"Whitespace","text":"<p>Tokens are delimited by a user-specified whitespace character.</p>"},{"location":"tokenizers/whitespace.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 delimiter ' ' string The whitespace character that delimits each token."},{"location":"tokenizers/whitespace.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Tokenizers\\Whitespace;\n\n$tokenizer = new Whitespace(',');\n</code></pre>"},{"location":"tokenizers/word-stemmer.html","title":"Word Stemmer","text":"<p>[source]</p>"},{"location":"tokenizers/word-stemmer.html#word-stemmer","title":"Word Stemmer","text":"<p>Word Stemmer reduces inflected and derived words to their root form using the Snowball method. For example, the sentence \"Majority voting is likely foolish\" stems to \"Major vote is like foolish.\"</p> <p>Note</p> <p>For a complete list of supported languages you can visit the PHP Stemmer documentation.</p>"},{"location":"tokenizers/word-stemmer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 language string The minimum number of contiguous words to a token."},{"location":"tokenizers/word-stemmer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Tokenizers\\WordStemmer;\n\n$tokenizer = new WordStemmer('english');\n</code></pre>"},{"location":"tokenizers/word.html","title":"Word","text":"<p>[source]</p>"},{"location":"tokenizers/word.html#word-tokenizer","title":"Word Tokenizer","text":"<p>The Word tokenizer uses a regular expression to tokenize the words in a blob of text.</p>"},{"location":"tokenizers/word.html#parameters","title":"Parameters","text":"<p>This tokenizer does not have any parameters.</p>"},{"location":"tokenizers/word.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Tokenizers\\Word;\n\n$tokenizer = new Word();\n</code></pre>"},{"location":"transformers/api.html","title":"Transformer","text":"<p>Transformers take Dataset objects and modify the features contained within. They are often used as part of a transformer Pipeline or they can be used on their own.</p>"},{"location":"transformers/api.html#transform-a-dataset","title":"Transform a Dataset","text":"<p>To transform a dataset, pass a transformer object to the <code>apply()</code> method on a Dataset object like in the example below.</p> <pre><code>use Rubix\\ML\\Transformers\\MinMaxNormalizer;\n\n$dataset-&gt;apply(new MinMaxNormalizer());\n</code></pre> <p>The transformer can directly transform the samples in place via the <code>transform()</code> method given a samples array: <pre><code>public transform(array &amp;$samples) : void\n</code></pre></p> <pre><code>$transformer-&gt;transform($samples);\n</code></pre>"},{"location":"transformers/api.html#stateful","title":"Stateful","text":"<p>Stateful transformers are those that require fitting before they can transform. The <code>fit()</code> method takes a dataset as input and pre-computes any necessary information in order to carry out future transformations. You can think of fitting a transformer like training a learner.</p>"},{"location":"transformers/api.html#fit-a-dataset","title":"Fit a Dataset","text":"<p>To fit the transformer to a training set: <pre><code>public fit(Dataset $dataset) : void\n</code></pre></p> <p>Check if the transformer has been fitted: <pre><code>public fitted() : bool\n</code></pre></p> <pre><code>use Rubix\\ML\\Transformers\\OneHotEncoder;\n\n$transformer = new OneHotEncoder();\n\n$transformer-&gt;fit($dataset);\n\nvar_dump($transformer-&gt;fitted());\n</code></pre> <pre><code>bool(true)\n</code></pre> <p>To apply a Stateful transformer to a dataset object, pass the transformer instance to the <code>apply()</code> method like you normally would. The transformer will automatically be fitted with the dataset before transforming the samples.</p> <pre><code>use Rubix\\ML\\Transformers\\OneHotEncoder;\n\n$dataset-&gt;apply(new OneHotEncoder());\n</code></pre>"},{"location":"transformers/api.html#elastic","title":"Elastic","text":"<p>Some transformers are able to adapt to new training data. The <code>update()</code> method provided by the Elastic interface can be used to modify the fitting of the transformer with new data even after being previously fitted. Updating is the transformer equivalent to partially training an Online learner.</p>"},{"location":"transformers/api.html#update-a-fitting","title":"Update a Fitting","text":"<pre><code>public update(Dataset $dataset) : void\n</code></pre> <pre><code>use Rubix\\ML\\Transformers\\ZScaleStandardizer;\n\n$transformer = new ZScaleStandardizer();\n\n$folds = $dataset-&gt;fold(3);\n\n$transformer-&gt;fit($folds[0]);\n\n$transformer-&gt;update($folds[1]);\n\n$transformer-&gt;update($folds[2]);\n</code></pre>"},{"location":"transformers/api.html#reversible","title":"Reversible","text":"<p>Transformers that implement the Reversible interface can reverse the transformation applied to a dataset. To apply the reverse transform to a dataset call the <code>reverseApply()</code> method on the dataset object and pass it the reversible transformer.</p> <pre><code>$transformer = new ZScaleStandardizer();\n\n$dataset-&gt;apply($transformer);\n\n// Do something\n\n$dataset-&gt;reverseApply($transformer);\n</code></pre>"},{"location":"transformers/bm25-transformer.html","title":"BM25 Transformer","text":"<p>[source]</p>"},{"location":"transformers/bm25-transformer.html#bm25-transformer","title":"BM25 Transformer","text":"<p>BM25 is a sublinear term weighting scheme that takes term frequency (TF), document frequency (DF), and document length into account. It is similar to TF-IDF but with variable sublinearity and the addition of document length normalization.</p> <p>Note: BM25 Transformer assumes that its inputs are token frequency vectors such as those created by Word Count Vectorizer.</p> <p>Interfaces: Transformer, Stateful, Elastic</p> <p>Data Type Compatibility: Continuous only</p>"},{"location":"transformers/bm25-transformer.html#parameters","title":"Parameters","text":"# Param Default Type Description 1 dampening 1.2 float The term frequency (TF) dampening factor i.e. the <code>K1</code> parameter in the formula. Lower values will cause the TF to saturate quicker. 2 normalization 0.75 float The importance of document length in normalizing the term frequency i.e. the <code>b</code> parameter in the formula."},{"location":"transformers/bm25-transformer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\BM25Transformer;\n\n$transformer = new BM25Transformer(1.2, 0.75);\n</code></pre>"},{"location":"transformers/bm25-transformer.html#additional-methods","title":"Additional Methods","text":"<p>Return the document frequencies calculated during fitting: <pre><code>public dfs() : ?array\n</code></pre></p> <p>Return the average number of tokens per document: <pre><code>public averageDocumentLength() : ?float\n</code></pre></p>"},{"location":"transformers/bm25-transformer.html#references","title":"References","text":"<ul> <li>S. Robertson et al. (2009). The Probabilistic Relevance Framework: BM25 and Beyond.</li> <li>K. Sparck Jones et al. (2000). A probabilistic model of information retrieval: development and comparative experiments.</li> </ul>"},{"location":"transformers/boolean-converter.html","title":"Boolean Converter","text":"<p>[source]</p>"},{"location":"transformers/boolean-converter.html#boolean-converter","title":"Boolean Converter","text":"<p>This transformer is used to convert boolean values to a compatible continuous or categorical datatype. Strings should be used when the boolean should be treated as a categorical value. Ints or floats when the boolean should be treated as a continuous value.</p> <p>Interfaces: Transformer</p> <p>Data Type Compatibility: Categorical, Continuous</p>"},{"location":"transformers/boolean-converter.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 trueValue 'true' string, int, float The value to convert <code>true</code> to. 2 falseValue 'false' string, int, float The value to convert <code>false</code> to."},{"location":"transformers/boolean-converter.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\BooleanConverter;\n\n$transformer = new BooleanConverter('true', 'false);\n\n$transformer = new BooleanConverter('tall', 'not tall');\n\n$transformer = new BooleanConverter(1, 0);\n</code></pre>"},{"location":"transformers/boolean-converter.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/gaussian-random-projector.html","title":"Gaussian Random Projector","text":"<p>[source]</p>"},{"location":"transformers/gaussian-random-projector.html#gaussian-random-projector","title":"Gaussian Random Projector","text":"<p>Random Projection is a dimensionality reduction technique based on the Johnson-Lindenstrauss lemma. It uses random matrices to project feature vectors onto a target number of dimensions. The Gaussian Random Projector utilizes a random matrix sampled from a smooth Gaussian distribution which projects samples onto a spherically random hyperplane through the origin.</p> <p>Interfaces: Transformer, Stateful, Persistable</p> <p>Data Type Compatibility: Continuous only</p>"},{"location":"transformers/gaussian-random-projector.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 dimensions int The number of target dimensions to project onto."},{"location":"transformers/gaussian-random-projector.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\GaussianRandomProjector;\n\n$transformer = new GaussianRandomProjector(100);\n</code></pre>"},{"location":"transformers/gaussian-random-projector.html#additional-methods","title":"Additional Methods","text":"<p>Estimate the minimum dimensionality needed to satisfy a max distortion constraint with n samples using the Johnson-Lindenstrauss lemma: <pre><code>public static minDimensions(int $n, float $maxDistortion = 0.5) : int\n</code></pre></p> <pre><code>use Rubix\\ML\\Transformers\\GaussianRandomProjector;\n\n$dimensions = GaussianRandomProjector::minDimensions(5000, 0.2);\n</code></pre>"},{"location":"transformers/hot-deck-imputer.html","title":"Hot Deck Imputer","text":"<p>[source]</p>"},{"location":"transformers/hot-deck-imputer.html#hot-deck-imputer","title":"Hot Deck Imputer","text":"<p>A hot deck is a set of complete donor samples that may be referenced when imputing a value for a missing feature value. Hot Deck Imputer first finds the k most similar donors to a sample that contains a missing value and then chooses a value at random from those donors.</p> <p>Note</p> <p>Requires a NaN safe distance kernel such as Safe Euclidean for continuous features.</p> <p>Interfaces: Transformer, Stateful, Persistable</p> <p>Data Type Compatibility: Depends on distance kernel</p>"},{"location":"transformers/hot-deck-imputer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 k 5 int The number of nearest neighbor donors to consider when imputing a value. 2 weighted false bool Should we use distances as weights when selecting a donor sample? 3 categoricalPlaceholder '?' string The categorical placeholder denoting the category that contains missing values. 4 tree BallTree Spatial The spatial tree used to run nearest neighbor searches."},{"location":"transformers/hot-deck-imputer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\HotDeckImputer;\nuse Rubix\\ML\\Graph\\Trees\\BallTree;\nuse Rubix\\ML\\Kernels\\Distance\\Gower;\n\n$transformer = new HotDeckImputer(20, false, '?', new BallTree(50, new Gower(1.0)));\n</code></pre>"},{"location":"transformers/hot-deck-imputer.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/hot-deck-imputer.html#references","title":"References","text":"<ol> <li> <p>C. Hasler et al. (2015). Balanced k-Nearest Neighbor Imputation.\u00a0\u21a9</p> </li> </ol>"},{"location":"transformers/image-resizer.html","title":"Image Resizer","text":"<p>[source]</p>"},{"location":"transformers/image-resizer.html#image-resizer","title":"Image Resizer","text":"<p>Image Resizer fits (scales and crops) images to a user-specified width and height that preserves aspect ratio.</p> <p>Note</p> <p>The GD extension is required to use this transformer.</p> <p>Interfaces: Transformer</p> <p>Data Type Compatibility: Image</p>"},{"location":"transformers/image-resizer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 width 32 int The width of the resized image. 2 heights 32 int The height of the resized image."},{"location":"transformers/image-resizer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\ImageResizer;\n\n$transformer = new ImageResizer(28, 28);\n</code></pre>"},{"location":"transformers/image-resizer.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/image-rotator.html","title":"Image Rotator","text":"<p>[source]</p>"},{"location":"transformers/image-rotator.html#image-rotator","title":"Image Rotator","text":"<p>Image Rotator permutes an image feature by rotating it and adding optional randomized jitter. The image is then cropped to fit the original width and height maintaining the dimensionality. Permutations such as these are useful for training computer vision models that are robust to </p> <p>Note</p> <p>The GD extension is required to use this transformer.</p> <p>Interfaces: Transformer</p> <p>Data Type Compatibility: Image</p>"},{"location":"transformers/image-rotator.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 offset float The angle of the rotation in degrees. 2 jitter 0.0 float The amount of random jitter to apply to the rotation."},{"location":"transformers/image-rotator.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\ImageRotator;\n\n$transformer = new ImageRotator(-90.0); // Rotate 90 degrees clockwise.\n\n$transformer = new ImageRotator(0.0, 0.5); // Add random jitter about the origin.\n</code></pre>"},{"location":"transformers/image-rotator.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/image-vectorizer.html","title":"Image Vectorizer","text":"<p>[source]</p>"},{"location":"transformers/image-vectorizer.html#image-vectorizer","title":"Image Vectorizer","text":"<p>Image Vectorizer takes images of the same size and converts them into flat feature vectors of raw color channel intensities. Intensities range from 0 to 255 and can either be read from 1 channel (grayscale) or 3 channels (RGB color) per pixel.</p> <p>Note</p> <p>Note that the GD extension is required to use this transformer.</p> <p>Interfaces: Transformer, Stateful</p> <p>Data Type Compatibility: Image</p>"},{"location":"transformers/image-vectorizer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 grayscale false bool Should we encode the image in grayscale instead of color?"},{"location":"transformers/image-vectorizer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\ImageVectorizer;\n\n$transformer = new ImageVectorizer(true);\n</code></pre>"},{"location":"transformers/image-vectorizer.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/interval-discretizer.html","title":"Interval Discretizer","text":"<p>[source]</p>"},{"location":"transformers/interval-discretizer.html#interval-discretizer","title":"Interval Discretizer","text":"<p>Assigns continuous features to ordered categories using variable width per-feature histograms with a fixed user-specified number of bins.</p> <p>Interfaces: Transformer, Stateful, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"transformers/interval-discretizer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 bins 5 int The number of bins per histogram. 2 equiWidth false bool Should the bins be equal width?"},{"location":"transformers/interval-discretizer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\IntervalDiscretizer;\n\n$transformer = new IntervalDiscretizer(8, false);\n</code></pre>"},{"location":"transformers/interval-discretizer.html#additional-methods","title":"Additional Methods","text":"<p>Return the bin intervals of the fitted data: <pre><code>public intervals() : array\n</code></pre></p>"},{"location":"transformers/knn-imputer.html","title":"KNN Imputer","text":"<p>[source]</p>"},{"location":"transformers/knn-imputer.html#knn-imputer","title":"KNN Imputer","text":"<p>An unsupervised imputer that replaces missing values in a dataset with the distance-weighted average of the samples' k nearest neighbors' values. The average for a continuous feature column is defined as the mean of the values of each donor. Similarly, average is defined as the most frequent value for categorical features.</p> <p>Note</p> <p>Requires a NaN safe distance kernel such as Safe Euclidean for continuous features.</p> <p>Interfaces: Transformer, Stateful, Persistable</p> <p>Data Type Compatibility: Depends on distance kernel</p>"},{"location":"transformers/knn-imputer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 k 5 int The number of nearest neighbor donors to consider when imputing a value. 2 weighted false bool Should we use distances as weights when selecting a donor sample? 3 categoricalPlaceholder '?' string The categorical placeholder denoting the category that contains missing values. 4 tree BallTree Spatial The spatial tree used to run nearest neighbor searches."},{"location":"transformers/knn-imputer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\KNNImputer;\nuse Rubix\\ML\\Graph\\Trees\\BallTee;\nuse Rubix\\ML\\Kernels\\Distance\\SafeEuclidean;\n\n$transformer = new KNNImputer(10, false, '?', new BallTree(30, new SafeEuclidean()));\n</code></pre>"},{"location":"transformers/knn-imputer.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/knn-imputer.html#references","title":"References","text":"<ol> <li> <p>O. Troyanskaya et al. (2001). Missing value estimation methods for DNA microarrays.\u00a0\u21a9</p> </li> </ol>"},{"location":"transformers/l1-normalizer.html","title":"L1 Normalizer","text":"<p>[source]</p>"},{"location":"transformers/l1-normalizer.html#l1-normalizer","title":"L1 Normalizer","text":"<p>Transform each sample (row) vector in the sample matrix such that each feature is divided by the L1 norm (or magnitude) of that vector.</p> <p>Interfaces: Transformer</p> <p>Data Type Compatibility: Continuous only</p>"},{"location":"transformers/l1-normalizer.html#parameters","title":"Parameters","text":"<p>This transformer does not have any parameters.</p>"},{"location":"transformers/l1-normalizer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\L1Normalizer;\n\n$transformer = new L1Normalizer();\n</code></pre>"},{"location":"transformers/l1-normalizer.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/l2-normalizer.html","title":"L2 Normalizer","text":"<p>[source]</p>"},{"location":"transformers/l2-normalizer.html#l2-normalizer","title":"L2 Normalizer","text":"<p>Transform each sample (row) vector in the sample matrix such that each feature is divided by the L2 norm (or magnitude) of that vector.</p> <p>Interfaces: Transformer</p> <p>Data Type Compatibility: Continuous only</p>"},{"location":"transformers/l2-normalizer.html#parameters","title":"Parameters","text":"<p>This transformer does not have any parameters.</p>"},{"location":"transformers/l2-normalizer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\L2Normalizer;\n\n$transformer = new L2Normalizer();\n</code></pre>"},{"location":"transformers/l2-normalizer.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/lambda-function.html","title":"Lambda Function","text":"<p>[source]</p>"},{"location":"transformers/lambda-function.html#lambda-function","title":"Lambda Function","text":"<p>Run a stateless lambda function over the samples in a dataset. The function receives three arguments - the sample to be transformed, its row offset in the dataset, and a user-defined outside context variable that can be used to hold state.</p> <p>Note: If the transformation results in a change in dimensionality, the change must be consistent for each sample.</p> <p>Interfaces: Transformer</p> <p>Compatibility Depends on callback function</p>"},{"location":"transformers/lambda-function.html#parameters","title":"Parameters","text":"# Param Default Type Description 1 callback callable The function to call over the samples in the dataset. 2 context null mixed The outside context that gets injected into the callback function on each call."},{"location":"transformers/lambda-function.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\LambdaFunction;\n\n$callback = function (&amp;$sample, $offset, $context) {\n    $sample[] = log1p($sample[3]);\n};\n\n$transformer = new LambdaFunction($callback, 'example context');\n</code></pre>"},{"location":"transformers/lambda-function.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/linear-discriminant-analysis.html","title":"Linear Discriminant Analysis","text":"<p>[source]</p>"},{"location":"transformers/linear-discriminant-analysis.html#linear-discriminant-analysis","title":"Linear Discriminant Analysis","text":"<p>Linear Discriminant Analysis (LDA) is a supervised dimensionality reduction technique that selects the most informative features using information in the class labels. More formally, LDA finds a linear combination of features that characterizes or best discriminates two or more classes.</p> <p>Interfaces: Transformer, Stateful, Persistable</p> <p>Data Type Compatibility: Continuous only</p>"},{"location":"transformers/linear-discriminant-analysis.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 dimensions int The target number of dimensions to project onto."},{"location":"transformers/linear-discriminant-analysis.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\LinearDiscriminantAnalysis;\n\n$transformer = new LinearDiscriminantAnalysis(20);\n</code></pre>"},{"location":"transformers/linear-discriminant-analysis.html#additional-methods","title":"Additional Methods","text":"<p>Return the proportion of information lost due to the transformation: <pre><code>public lossiness() : ?float\n</code></pre></p>"},{"location":"transformers/max-absolute-scaler.html","title":"Max Absolute Scaler","text":"<p>[source]</p>"},{"location":"transformers/max-absolute-scaler.html#max-absolute-scaler","title":"Max Absolute Scaler","text":"<p>Scale the sample matrix by the maximum absolute value of each feature column independently such that the feature value is between -1 and 1.</p> <p>Interfaces: Transformer, Stateful, Elastic, Reversible, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"transformers/max-absolute-scaler.html#parameters","title":"Parameters","text":"<p>This transformer does not have any parameters.</p>"},{"location":"transformers/max-absolute-scaler.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\MaxAbsoluteScaler;\n\n$transformer = new MaxAbsoluteScaler();\n</code></pre>"},{"location":"transformers/max-absolute-scaler.html#additional-methods","title":"Additional Methods","text":"<p>Return the maximum absolute values for each feature column: <pre><code>public maxabs() : array\n</code></pre></p>"},{"location":"transformers/min-max-normalizer.html","title":"Min Max Normalizer","text":"<p>[source]</p>"},{"location":"transformers/min-max-normalizer.html#min-max-normalizer","title":"Min Max Normalizer","text":"<p>The Min Max Normalizer scales the input features to a value between a user-specified range (default 0 to 1).</p> <p>Interfaces: Transformer, Stateful, Elastic, Reversible, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"transformers/min-max-normalizer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 min 0.0 float The minimum value of the transformed features. 2 max 1.0 float The maximum value of the transformed features."},{"location":"transformers/min-max-normalizer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\MinMaxNormalizer;\n\n$transformer = new MinMaxNormalizer(-5.0, 5.0);\n</code></pre>"},{"location":"transformers/min-max-normalizer.html#additional-methods","title":"Additional Methods","text":"<p>Return the minimum values for each fitted feature column: <pre><code>public minimums() : ?array\n</code></pre></p> <p>Return the maximum values for each fitted feature column: <pre><code>public maximums() : ?array\n</code></pre></p>"},{"location":"transformers/missing-data-imputer.html","title":"Missing Data Imputer","text":"<p>[source]</p>"},{"location":"transformers/missing-data-imputer.html#missing-data-imputer","title":"Missing Data Imputer","text":"<p>Missing Data Imputer replaces missing continuous (denoted by <code>NaN</code>) or categorical values (denoted by special placeholder category such as <code>'?'</code>) with a guess based on user-defined Strategy.</p> <p>Interfaces: Transformer, Stateful, Persistable</p> <p>Data Type Compatibility: Categorical and Continuous</p>"},{"location":"transformers/missing-data-imputer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 continuous Mean Strategy The guessing strategy to employ for continuous feature columns. 2 categorical K Most Frequent Strategy The guessing strategy to employ for categorical feature columns. 3 categoricalPlaceholder '?' string The special placeholder category that denotes missing values."},{"location":"transformers/missing-data-imputer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\MissingDataImputer;\nuse Rubix\\ML\\Strategies\\Percentile;\nuse Rubix\\ML\\Strategies\\Prior;\n\n$transformer = new MissingDataImputer(new Percentile(0.55), new Prior(), '?');\n</code></pre>"},{"location":"transformers/missing-data-imputer.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/multibyte-text-normalizer.html","title":"Multibyte Text Normalizer","text":"<p>[source]</p>"},{"location":"transformers/multibyte-text-normalizer.html#multibyte-text-normalizer","title":"Multibyte Text Normalizer","text":"<p>This transformer converts the characters in all multibyte strings to the same case. Multibyte strings contain characters such as accents (\u00e9, \u00e8, \u00e0), emojis (\ud83d\ude00, \ud83d\ude09) or characters of non roman alphabets such as Chinese and Cyrillic.</p> <p>Note</p> <p>\u26a0\ufe0f We recommend you install the mbstring extension for best performance.</p> <p>Interfaces: Transformer</p> <p>Data Type Compatibility: Categorical</p>"},{"location":"transformers/multibyte-text-normalizer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 uppercase false bool Should the text be converted to uppercase?"},{"location":"transformers/multibyte-text-normalizer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\MultibyteTextNormalizer;\n\n$transformer = new MultibyteTextNormalizer(false);\n</code></pre>"},{"location":"transformers/multibyte-text-normalizer.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/numeric-string-converter.html","title":"Numeric String Converter","text":"<p>[source]</p>"},{"location":"transformers/numeric-string-converter.html#numeric-string-converter","title":"Numeric String Converter","text":"<p>Convert all numeric strings to their equivalent integer and floating point types. Useful for when extracting from a source that only recognizes data as string types such as CSV.</p> <p>Note</p> <p>The string representations of the PHP constants <code>NAN</code> and <code>INF</code> are the string literals 'NAN' and 'INF' respectively. </p> <p>Interfaces: Transformer, Reversible</p> <p>Data Type Compatibility: Categorical</p>"},{"location":"transformers/numeric-string-converter.html#parameters","title":"Parameters","text":"<p>This transformer does not have any parameters.</p>"},{"location":"transformers/numeric-string-converter.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\NumericStringConverter;\n\n$transformer = new NumericStringConverter();\n</code></pre>"},{"location":"transformers/numeric-string-converter.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/one-hot-encoder.html","title":"One Hot Encoder","text":"<p>[source]</p>"},{"location":"transformers/one-hot-encoder.html#one-hot-encoder","title":"One Hot Encoder","text":"<p>The One Hot Encoder takes a categorical feature column and produces an n-dimensional continuous representation where n is equal to the number of unique categories present in that column. A <code>0</code> in any location indicates that the category represented by that column is not present in the sample, whereas a <code>1</code> indicates that a category is present.</p> <p>Interfaces: Transformer, Stateful, Persistable</p> <p>Data Type Compatibility: Categorical</p>"},{"location":"transformers/one-hot-encoder.html#parameters","title":"Parameters","text":"<p>This transformer does not have any parameters.</p>"},{"location":"transformers/one-hot-encoder.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\OneHotEncoder;\n\n$transformer = new OneHotEncoder();\n</code></pre>"},{"location":"transformers/one-hot-encoder.html#additional-methods","title":"Additional Methods","text":"<p>Return the categories computed during fitting indexed by feature column: <pre><code>public categories() : ?array\n</code></pre></p>"},{"location":"transformers/polynomial-expander.html","title":"Polynomial Expander","text":"<p>[source]</p>"},{"location":"transformers/polynomial-expander.html#polynomial-expander","title":"Polynomial Expander","text":"<p>This transformer will generate polynomials up to and including the specified degree of each continuous feature. Polynomial expansion is sometimes used to fit data that is non-linear using a linear estimator such as Ridge, Logistic Regression, or Softmax Classifier.</p> <p>Interfaces: Transformer</p> <p>Data Type Compatibility: Continuous only</p>"},{"location":"transformers/polynomial-expander.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 degree 2 int The degree of the polynomials to generate for each feature."},{"location":"transformers/polynomial-expander.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\PolynomialExpander;\n\n$transformer = new PolynomialExpander(3);\n</code></pre>"},{"location":"transformers/polynomial-expander.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/principal-component-analysis.html","title":"Principal Component Analysis","text":"<p>[source]</p>"},{"location":"transformers/principal-component-analysis.html#principal-component-analysis","title":"Principal Component Analysis","text":"<p>Principal Component Analysis (PCA) is a dimensionality reduction technique that aims to transform the feature space by the k principal components that explain the most variance. PCA is used to compress high-dimensional samples down to lower dimensions such that they would retain as much information as possible.</p> <p>Interfaces: Transformer, Stateful, Persistable</p> <p>Data Type Compatibility: Continuous only</p>"},{"location":"transformers/principal-component-analysis.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 dimensions int The target number of dimensions to project onto."},{"location":"transformers/principal-component-analysis.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\PrincipalComponentAnalysis;\n\n$transformer = new PrincipalComponentAnalysis(15);\n</code></pre>"},{"location":"transformers/principal-component-analysis.html#additional-methods","title":"Additional Methods","text":"<p>Return the proportion of information lost due to the transformation: <pre><code>public lossiness() : ?float\n</code></pre></p>"},{"location":"transformers/principal-component-analysis.html#references","title":"References","text":"<ol> <li> <p>H. Abdi et al. (2010). Principal Component Analysis.\u00a0\u21a9</p> </li> </ol>"},{"location":"transformers/regex-filter.html","title":"Regex Filter","text":"<p>[source]</p>"},{"location":"transformers/regex-filter.html#regex-filter","title":"Regex Filter","text":"<p>Filters the text features of a dataset by matching and removing patterns from a list of regular expressions.</p> <p>Note</p> <p>Patterns are filtered in the same sequence as they are given in the constructor.</p> <p>Interfaces: Transformer</p> <p>Data Type Compatibility: Categorical</p>"},{"location":"transformers/regex-filter.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 patterns array A list of regular expression patterns used to filter the text columns of the dataset."},{"location":"transformers/regex-filter.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\RegexFilter;\n\n$transformer = new RegexFilter([\n    RegexFilter::URL,\n    RegexFilter::MENTION,\n    '/(?&lt;me&gt;.+)/',\n    RegexFilter::EXTRA_CHARACTERS,\n]);\n</code></pre>"},{"location":"transformers/regex-filter.html#predefined-regex-patterns","title":"Predefined Regex Patterns","text":"Class Constant Description EMAIL A pattern to match any email address. URL An alias for the default (Gruber 1) URL matching pattern. GRUBER_1 The original Gruber URL matching pattern. GRUBER_2 The improved Gruber URL matching pattern. EXTRA_CHARACTERS Matches consecutively repeated non word or number characters such as punctuation and special characters. EXTRA_WORDS Matches consecutively repeated words. EXTRA_WHITESPACE Matches consecutively repeated whitespace characters. EMOJIS A pattern to match unicode emojis. MENTION A pattern that matches Twitter-style mentions (@example). HASHTAG Matches Twitter-style hashtags (#example)."},{"location":"transformers/regex-filter.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/regex-filter.html#references","title":"References:","text":"<ol> <li> <p>J. Gruber. (2009). A Liberal, Accurate Regex Pattern for Matching URLs.\u00a0\u21a9</p> </li> <li> <p>J. Gruber. (2010). An Improved Liberal, Accurate Regex Pattern for Matching URLs.\u00a0\u21a9</p> </li> </ol>"},{"location":"transformers/robust-standardizer.html","title":"Robust Standardizer","text":"<p>[source]</p>"},{"location":"transformers/robust-standardizer.html#robust-standardizer","title":"Robust Standardizer","text":"<p>This standardizer transforms continuous features by centering them around the median and scaling by the median absolute deviation (MAD) referred to as a robust  or modified Z-Score. The use of robust statistics make this standardizer more immune to outliers than Z Scale Standardizer.</p> \\[ {\\displaystyle z^\\prime = {x - \\operatorname {median}(X) \\over MAD }} \\] <p>Interfaces: Transformer, Stateful, Reversible, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"transformers/robust-standardizer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 center true bool Should we center the data at 0?"},{"location":"transformers/robust-standardizer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\RobustStandardizer;\n\n$transformer = new RobustStandardizer(true);\n</code></pre>"},{"location":"transformers/robust-standardizer.html#additional-methods","title":"Additional Methods","text":"<p>Return the medians calculated by fitting the training set: <pre><code>public medians() : array\n</code></pre></p> <p>Return the median absolute deviations calculated during fitting: <pre><code>public mads() : array\n</code></pre></p>"},{"location":"transformers/sparse-random-projector.html","title":"Sparse Random Projector","text":"<p>[source]</p>"},{"location":"transformers/sparse-random-projector.html#sparse-random-projector","title":"Sparse Random Projector","text":"<p>A database-friendly random projector that samples its random projection matrix from a sparse probabilistic approximation of the Gaussian distribution. The term database-friendly refers to the fact that the number of non-zero operations required to transform the input matrix is reduced by the sparsity factor.</p> <p>Interfaces: Transformer, Stateful, Persistable</p> <p>Data Type Compatibility: Continuous only</p>"},{"location":"transformers/sparse-random-projector.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 dimensions int The number of target dimensions to project onto. 2 sparsity 0.66 float The proportion of zero to non-zero elements in the random projection matrix. If null, sparsity factor will be chosen automatically."},{"location":"transformers/sparse-random-projector.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\SparseRandomProjector;\n\n$transformer = new SparseRandomProjector(30, null);\n</code></pre>"},{"location":"transformers/sparse-random-projector.html#additional-methods","title":"Additional Methods","text":"<p>Estimate the minimum dimensionality needed to satisfy a max distortion constraint with n samples using the Johnson-Lindenstrauss lemma: <pre><code>public static minDimensions(int $n, float $maxDistortion = 0.5) : int\n</code></pre></p> <pre><code>use Rubix\\ML\\Transformers\\SparseRandomProjector;\n\n$dimensions = SparseRandomProjector::minDimensions(10000, 0.5);\n</code></pre>"},{"location":"transformers/sparse-random-projector.html#references","title":"References","text":"<ol> <li> <p>D. Achlioptas. (2003). Database-friendly random projections: Johnson-Lindenstrauss with binary coins.\u00a0\u21a9</p> </li> <li> <p>P. Li at al. (2006). Very Sparse Random Projections.\u00a0\u21a9</p> </li> </ol>"},{"location":"transformers/stop-word-filter.html","title":"Stop Word Filter","text":"<p>[source]</p>"},{"location":"transformers/stop-word-filter.html#stop-word-filter","title":"Stop Word Filter","text":"<p>Removes user-specified words from any categorical feature columns including blobs of text.</p> <p>Interfaces: Transformer</p> <p>Data Type Compatibility: Categorical</p>"},{"location":"transformers/stop-word-filter.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 stopWords array A list of stop words to filter out of each text feature."},{"location":"transformers/stop-word-filter.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\StopWordFilter;\n\n$transformer = new StopWordFilter(['i', 'me', 'my', ...]);\n</code></pre>"},{"location":"transformers/stop-word-filter.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/t-sne.html","title":"t-SNE","text":"<p>[source]</p>"},{"location":"transformers/t-sne.html#t-sne","title":"t-SNE","text":"<p>T-distributed Stochastic Neighbor Embedding is a two-stage non-linear manifold learning algorithm based on Batch Gradient Descent that seeks to maintain the distances between samples in low-dimensional space. During the first stage (early stage) the distances are exaggerated to encourage more pronounced clusters. Since the t-SNE cost function (KL Divergence) has a rough gradient, momentum is employed to help escape bad local minima.</p> <p>Note</p> <p>T-SNE is implemented using the exact method which scales quadratically in the number of samples. Therefore, it is recommended to subsample datasets larger than a few thousand samples.</p> <p>Interfaces: Transformer, Verbose</p> <p>Data Type Compatibility: Depends on distance kernel</p>"},{"location":"transformers/t-sne.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 dimensions 2 int The number of dimensions of the target embedding. 2 rate 100.0 float The learning rate that controls the global step size. 3 perplexity 30 int The number of effective nearest neighbors to refer to when computing the variance of the distribution over that sample. 4 exaggeration 12.0 float The factor to exaggerate the distances between samples during the early stage of embedding. 5 epochs 1000 int The maximum number of times to iterate over the embedding. 6 minGradient 1e-7 float The minimum norm of the gradient necessary to continue embedding. 7 window 10 int The number of epochs without improvement in the training loss to wait before considering an early stop. 8 kernel Euclidean Distance The distance kernel to use when measuring distances between samples."},{"location":"transformers/t-sne.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\TSNE;\nuse Rubix\\ML\\Kernels\\Distance\\Manhattan;\n\n$transformer = new TSNE(3, 10.0, 30, 12.0, 500, 1e-6, 10, new Manhattan());\n</code></pre>"},{"location":"transformers/t-sne.html#additional-methods","title":"Additional Methods","text":"<p>Return an iterable progress table with the steps from the last training session: <pre><code>public steps() : iterable\n</code></pre></p> <pre><code>use Rubix\\ML\\Extractors\\CSV;\n\n$extractor = new CSV('progress.csv', true);\n\n$extractor-&gt;export($transformer-&gt;steps());\n</code></pre> <p>Return the magnitudes of the gradient at each epoch from the last embedding: <pre><code>public losses() : float[]|null\n</code></pre></p>"},{"location":"transformers/t-sne.html#references","title":"References","text":"<ol> <li> <p>L. van der Maaten et al. (2008). Visualizing Data using t-SNE.\u00a0\u21a9</p> </li> <li> <p>L. van der Maaten. (2009). Learning a Parametric Embedding by Preserving Local Structure.\u00a0\u21a9</p> </li> </ol>"},{"location":"transformers/text-normalizer.html","title":"Text Normalizer","text":"<p>[source]</p>"},{"location":"transformers/text-normalizer.html#text-normalizer","title":"Text Normalizer","text":"<p>Converts all the characters in a blob of text to the same case.</p> <p>Interfaces: Transformer</p> <p>Data Type Compatibility: Categorical</p> <p>Note</p> <p>This transformer does not handle multibyte strings. For multibyte support, see MultibyteTextNormalizer.</p>"},{"location":"transformers/text-normalizer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 uppercase false bool Should the text be converted to uppercase?"},{"location":"transformers/text-normalizer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\TextNormalizer;\n\n$transformer = new TextNormalizer(false);\n</code></pre>"},{"location":"transformers/text-normalizer.html#additional-methods","title":"Additional Methods","text":"<p>This transformer does not have any additional methods.</p>"},{"location":"transformers/tf-idf-transformer.html","title":"TF-IDF Transformer","text":"<p>[source]</p>"},{"location":"transformers/tf-idf-transformer.html#tf-idf-transformer","title":"TF-IDF Transformer","text":"<p>Term Frequency - Inverse Document Frequency is a measurement of how important a word is to a document. The TF-IDF value increases with the number of times a word appears in a document (TF) and is offset by the frequency of the word in the corpus (IDF).</p> <p>Note</p> <p>TF-IDF Transformer assumes that its inputs are token frequency vectors such as those created by Word Count Vectorizer.</p> <p>Interfaces: Transformer, Stateful, Elastic, Reversible, Persistable</p> <p>Data Type Compatibility: Continuous only</p>"},{"location":"transformers/tf-idf-transformer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 smoothing 1.0 float The amount of additive (Laplace) smoothing to add to the IDFs. 2 dampening false bool Should we apply a sub-linear function to dampen the effect of recurring tokens?"},{"location":"transformers/tf-idf-transformer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\TfIdfTransformer;\n\n$transformer = new TfIdfTransformer(2.0, true);\n</code></pre>"},{"location":"transformers/tf-idf-transformer.html#additional-methods","title":"Additional Methods","text":"<p>Return the document frequencies calculated during fitting: <pre><code>public dfs() : ?array\n</code></pre></p>"},{"location":"transformers/tf-idf-transformer.html#references","title":"References","text":"<ol> <li> <p>S. Robertson. (2003). Understanding Inverse Document Frequency: On theoretical arguments for IDF.\u00a0\u21a9</p> </li> <li> <p>C. D. Manning et al. (2009). An Introduction to Information Retrieval.\u00a0\u21a9</p> </li> </ol>"},{"location":"transformers/token-hashing-vectorizer.html","title":"Token Hashing Vectorizer","text":"<p>[source]</p>"},{"location":"transformers/token-hashing-vectorizer.html#token-hashing-vectorizer","title":"Token Hashing Vectorizer","text":"<p>Token Hashing Vectorizer builds token count vectors on the fly by employing a hashing trick. It is a stateless transformer that uses a hashing algorithm to assign token occurrences to a bucket in a vector of user-specified dimensionality. The advantage of hashing over storing a fixed vocabulary is that there is no memory footprint however there is a chance that certain tokens will collide with other tokens especially in lower-dimensional vector spaces.</p> <p>Note</p> <p>The default hashing function is CRC32 and is a good mix between speed and output space utilization. MurmurHash has even greater utilization but at the cost of some speed and it is only available on PHP 8.1 and above. FNV1 is comparable to CRC32 but with slightly more overhead.</p> <p>Interfaces: Transformer</p> <p>Data Type Compatibility: Categorical</p>"},{"location":"transformers/token-hashing-vectorizer.html#parameters","title":"Parameters","text":"# Param Default Type Description 1 dimensions int The dimensionality of the vector space. 2 tokenizer Word Tokenizer The tokenizer used to extract tokens from blobs of text. 3 hashFn callable 'crc32' The hash function that accepts a string token and returns an integer."},{"location":"transformers/token-hashing-vectorizer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\TokenHashingVectorizer;\nuse Rubix\\ML\\Tokenizers\\Word();\n\n$transformer = new TokenHashingVectorizer(10000, new Word(), TokenHashingVectorizer::MURMUR3);\n</code></pre>"},{"location":"transformers/token-hashing-vectorizer.html#additional-constants","title":"Additional Constants","text":"<p>The CRC32 callback function. <pre><code>public const CRC32 callable(string):int\n</code></pre></p> <p>The MurmurHash3 callback function. <pre><code>public const MURMUR3 callable(string):int\n</code></pre></p> <p>The FNV1 callback function. <pre><code>public const FNV1 callable(string):int\n</code></pre></p>"},{"location":"transformers/token-hashing-vectorizer.html#additional-methods","title":"Additional Methods","text":"<p>The MurmurHash3 hashing function: <pre><code>public static murmur3(string $input) : int\n</code></pre></p> <p>Note</p> <p>MurmurHash3 is only available on PHP 8.1 or above.</p> <p>The FNV1a 32-bit hashing function: <pre><code>public static fnv1a32(string $input) : int\n</code></pre></p>"},{"location":"transformers/truncated-svd.html","title":"Truncated SVD","text":"<p>[source]</p>"},{"location":"transformers/truncated-svd.html#truncated-svd","title":"Truncated SVD","text":"<p>Truncated Singular Value Decomposition (SVD) is a matrix factorization and dimensionality reduction technique that generalizes eigendecomposition to general matrices. When applied to datasets of document term frequency vectors, the technique is called Latent Semantic Analysis (LSA) and computes a statistical model of relationships between words.</p> <p>Note</p> <p>Note that the Tensor extension is required to use this transformer.</p> <p>Interfaces: Transformer, Stateful, Persistable</p> <p>Data Type Compatibility: Continuous only</p>"},{"location":"transformers/truncated-svd.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 dimensions int The target number of dimensions to project onto."},{"location":"transformers/truncated-svd.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\TruncatedSVD;\n\n$transformer = new TruncatedSVD(100);\n</code></pre>"},{"location":"transformers/truncated-svd.html#additional-methods","title":"Additional Methods","text":"<p>Return the proportion of information lost due to the transformation: <pre><code>public lossiness() : ?float\n</code></pre></p>"},{"location":"transformers/truncated-svd.html#references","title":"References","text":"<ol> <li> <p>S. Deerwater et al. (1990). Indexing by Latent Semantic Analysis.\u00a0\u21a9</p> </li> </ol>"},{"location":"transformers/word-count-vectorizer.html","title":"Word Count Vectorizer","text":"<p>[source]</p>"},{"location":"transformers/word-count-vectorizer.html#word-count-vectorizer","title":"Word Count Vectorizer","text":"<p>The Word Count Vectorizer builds a vocabulary from the training samples and transforms text blobs into fixed length sparse feature vectors. Each feature column represents a word or token from the vocabulary and the value denotes the number of times that word appears in a given document.</p> <p>Interfaces: Transformer, Stateful, Persistable</p> <p>Data Type Compatibility: Categorical</p>"},{"location":"transformers/word-count-vectorizer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 maxVocabularySize PHP_INT_MAX int The maximum number of unique tokens to embed into each document vector. 2 minDocumentCount 1 float The minimum number of documents a word must appear in to be added to the vocabulary. 3 maxDocumentRatio 0.8 float The maximum ratio of documents a word can appear in to be added to the vocabulary. 4 tokenizer Word Tokenizer The tokenizer used to extract features from blobs of text."},{"location":"transformers/word-count-vectorizer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\WordCountVectorizer;\nuse Rubix\\ML\\Tokenizers\\NGram;\n\n$transformer = new WordCountVectorizer(10000, 5, 0.5, new NGram(1, 2));\n</code></pre>"},{"location":"transformers/word-count-vectorizer.html#additional-methods","title":"Additional Methods","text":"<p>Return an array of words that comprise each of the vocabularies: <pre><code>public vocabularies() : array\n</code></pre></p>"},{"location":"transformers/z-scale-standardizer.html","title":"Z Scale Standardizer","text":"<p>[source]</p>"},{"location":"transformers/z-scale-standardizer.html#z-scale-standardizer","title":"Z Scale Standardizer","text":"<p>A method of centering and scaling a dataset such that it has 0 mean and unit variance, also known as a Z-Score. Although Z-Scores are technically unbounded, in practice they mostly fall between -3 and 3 - that is, they are no more than 3 standard deviations away from the mean.</p> \\[ {\\displaystyle z = {x - \\mu \\over \\sigma }} \\] <p>Interfaces: Transformer, Stateful, Elastic, Reversible, Persistable</p> <p>Data Type Compatibility: Continuous</p>"},{"location":"transformers/z-scale-standardizer.html#parameters","title":"Parameters","text":"# Name Default Type Description 1 center true bool Should we center the data at 0?"},{"location":"transformers/z-scale-standardizer.html#example","title":"Example","text":"<pre><code>use Rubix\\ML\\Transformers\\ZScaleStandardizer;\n\n$transformer = new ZScaleStandardizer(true);\n</code></pre>"},{"location":"transformers/z-scale-standardizer.html#additional-methods","title":"Additional Methods","text":"<p>Return the means calculated by fitting the training set: <pre><code>public means() : array\n</code></pre></p> <p>Return the variances calculated during fitting: <pre><code>public variances() : array\n</code></pre></p>"},{"location":"transformers/z-scale-standardizer.html#references","title":"References","text":"<ol> <li> <p>T. F. Chan et al. (1979). Updating Formulae and a Pairwise Algorithm for Computing Sample Variances.\u00a0\u21a9</p> </li> </ol>"}]}